[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Lineares I - Seminário",
    "section": "",
    "text": "Introdução\nEste material faz parte da disciplina LCE5861 - Modelos Lineares I, ministrada pelo prof. Dr. César Gonçalves de Lima na ESALQ/USP, como parte do Programa de Pós-Graduação em Estatística e Experimentação Agronômica.\nSerão apresentados exercícios e exemplos de aulas resolvidos com o software SAS, os quais foram convertidos para a linguagem de programação R. Parte desses exercícios e exemplos são oriundos de Rencher e Schaalje (2008)1.\nPara acessar o script em R, clique aqui. Para acessar os scripts em SAS, clique aqui."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Modelos Lineares I - Seminário",
    "section": "",
    "text": "Rencher, A. C.; Schaalje, G. B. (2008). Linear models in Statistics, 2ª ed. New York: Wiley.↩︎"
  },
  {
    "objectID": "exemplo1.html#vetores-e-matriz",
    "href": "exemplo1.html#vetores-e-matriz",
    "title": "\n1  Exemplo\n",
    "section": "\n1.1 Vetores e Matriz",
    "text": "1.1 Vetores e Matriz\n\n\nR\nSAS\n\n\n\nPara criar vetores no R, definimos um nome para cada vetor (y1, y2, y3 e y4), em seguida, utilizamos o operador &lt;- e a função c() para atribuir valores a cada um dos objetos (vetores). Dentro de c(), declaramos seus valores, separados por vírgula.\n\ny1 &lt;- c(72,60,56,41,32,30,39,42,37,33,32,63,54,47,91,56,79,81,78,46,39,32,60,35,39,50,43,48)\n\ny2 &lt;- c(66,53,57,29,32,35,39,43,40,29,30,45,46,51,79,68,65,80,55,38,35,30,50,37,36,34,37,54)\n\ny3 &lt;- c(76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37,34,30,67,48,39,37,39,57)\n\ny4 &lt;- c(77,63,58,38,36,26,27,25,25,36,28,63,52,43,75,50,61,58,60,38,37,32,54,39,31,40,50,43)\n\nCom a função cbind() juntamos os vetores y1, y2, y3 e y4, salvando-os em um objeto chamado Y. Utilizando a função colnames(), alteramos os nomes dos vetores (\"North\", \"East\", \"South\", \"West\").\n\nY &lt;- cbind(y1, y2, y3, y4)\ncolnames(Y) &lt;- c(\"North\", \"East\", \"South\", \"West\")\nY\n\n      North East South West\n [1,]    72   66    76   77\n [2,]    60   53    66   63\n [3,]    56   57    64   58\n [4,]    41   29    36   38\n [5,]    32   32    35   36\n [6,]    30   35    34   26\n [7,]    39   39    31   27\n [8,]    42   43    31   25\n [9,]    37   40    31   25\n[10,]    33   29    27   36\n[11,]    32   30    34   28\n[12,]    63   45    74   63\n[13,]    54   46    60   52\n[14,]    47   51    52   43\n[15,]    91   79   100   75\n[16,]    56   68    47   50\n[17,]    79   65    70   61\n[18,]    81   80    68   58\n[19,]    78   55    67   60\n[20,]    46   38    37   38\n[21,]    39   35    34   37\n[22,]    32   30    30   32\n[23,]    60   50    67   54\n[24,]    35   37    48   39\n[25,]    39   36    39   31\n[26,]    50   34    37   40\n[27,]    43   37    39   50\n[28,]    48   54    57   43\n\n\nCom a função class(), constatamos que o objeto Y é uma matriz (matrix).\n\nclass(Y)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n\nproc iml;\n\ny1 = {72,60,56,41,32,30,39,42,37,33,32,63,54,47, 91,56,79,81,78,46, 39,32,60,35,39,50,43,48};\n\ny2 = {66,53,57,29,32,35,39,43,40,29,30,45,46,51, 79,68,65,80,55,38, 35,30,50,37,36,34,37,54};\n\ny3 = {76,66,64,36,35,34,31,31,31,27,34,74,60,52,100,47,70,68,67,37, 34,30,67,48,39,37,39,57};\n\ny4 = {77,63,58,38,36,26,27,25,25,36,28,63,52,43, 75,50,61,58,60,38, 37,32,54,39,31,40,50,43};\n\n\nY = y1||y2||y3||y4;\ncreate Cork var {North East South West};\nappend from Y;\nClose Cork;"
  },
  {
    "objectID": "exemplo1.html#matriz-de-variâncias-e-covariâncias",
    "href": "exemplo1.html#matriz-de-variâncias-e-covariâncias",
    "title": "\n1  Exemplo\n",
    "section": "\n1.2 Matriz de Variâncias e Covariâncias",
    "text": "1.2 Matriz de Variâncias e Covariâncias\nPara obtermos a matriz de variâncias e covariâncias, precisamos dos seguintes objetos:\n\nn: número de observações \\(n\\);\nIn: matriz identidade \\(\\mathbf{I}\\);\njn: vetor coluna de 1’s \\(\\mathbf{j}\\);\nJnn: matriz de 1’s \\(\\mathbf{J}\\).\n\n\n\nR\nSAS\n\n\n\nAs funções nrow() e ncol() nos retornam o número de linhas e colunas de um objeto, respectivamente.\n\nn &lt;- nrow(Y)\np &lt;- ncol(Y)\n\nn; p\n\n[1] 28\n\n\n[1] 4\n\n\nNo caso da matriz Y, apresenta dimensão \\(28 \\times 4\\).\nA função diag() cria uma matriz identidade. Basta inserir dentro da função a dimensão da matriz.\n\nIn &lt;- diag(n)\nIn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    1    0    0    0    0    0    0    0    0     0     0     0     0\n [2,]    0    1    0    0    0    0    0    0    0     0     0     0     0\n [3,]    0    0    1    0    0    0    0    0    0     0     0     0     0\n [4,]    0    0    0    1    0    0    0    0    0     0     0     0     0\n [5,]    0    0    0    0    1    0    0    0    0     0     0     0     0\n [6,]    0    0    0    0    0    1    0    0    0     0     0     0     0\n [7,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n [8,]    0    0    0    0    0    0    0    1    0     0     0     0     0\n [9,]    0    0    0    0    0    0    0    0    1     0     0     0     0\n[10,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[11,]    0    0    0    0    0    0    0    0    0     0     1     0     0\n[12,]    0    0    0    0    0    0    0    0    0     0     0     1     0\n[13,]    0    0    0    0    0    0    0    0    0     0     0     0     1\n[14,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[15,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[16,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[17,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[18,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[19,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[20,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[21,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[22,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[23,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[24,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[25,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[26,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[27,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[28,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n [1,]     0     0     0     0     0     0     0     0     0     0     0     0\n [2,]     0     0     0     0     0     0     0     0     0     0     0     0\n [3,]     0     0     0     0     0     0     0     0     0     0     0     0\n [4,]     0     0     0     0     0     0     0     0     0     0     0     0\n [5,]     0     0     0     0     0     0     0     0     0     0     0     0\n [6,]     0     0     0     0     0     0     0     0     0     0     0     0\n [7,]     0     0     0     0     0     0     0     0     0     0     0     0\n [8,]     0     0     0     0     0     0     0     0     0     0     0     0\n [9,]     0     0     0     0     0     0     0     0     0     0     0     0\n[10,]     0     0     0     0     0     0     0     0     0     0     0     0\n[11,]     0     0     0     0     0     0     0     0     0     0     0     0\n[12,]     0     0     0     0     0     0     0     0     0     0     0     0\n[13,]     0     0     0     0     0     0     0     0     0     0     0     0\n[14,]     1     0     0     0     0     0     0     0     0     0     0     0\n[15,]     0     1     0     0     0     0     0     0     0     0     0     0\n[16,]     0     0     1     0     0     0     0     0     0     0     0     0\n[17,]     0     0     0     1     0     0     0     0     0     0     0     0\n[18,]     0     0     0     0     1     0     0     0     0     0     0     0\n[19,]     0     0     0     0     0     1     0     0     0     0     0     0\n[20,]     0     0     0     0     0     0     1     0     0     0     0     0\n[21,]     0     0     0     0     0     0     0     1     0     0     0     0\n[22,]     0     0     0     0     0     0     0     0     1     0     0     0\n[23,]     0     0     0     0     0     0     0     0     0     1     0     0\n[24,]     0     0     0     0     0     0     0     0     0     0     1     0\n[25,]     0     0     0     0     0     0     0     0     0     0     0     1\n[26,]     0     0     0     0     0     0     0     0     0     0     0     0\n[27,]     0     0     0     0     0     0     0     0     0     0     0     0\n[28,]     0     0     0     0     0     0     0     0     0     0     0     0\n      [,26] [,27] [,28]\n [1,]     0     0     0\n [2,]     0     0     0\n [3,]     0     0     0\n [4,]     0     0     0\n [5,]     0     0     0\n [6,]     0     0     0\n [7,]     0     0     0\n [8,]     0     0     0\n [9,]     0     0     0\n[10,]     0     0     0\n[11,]     0     0     0\n[12,]     0     0     0\n[13,]     0     0     0\n[14,]     0     0     0\n[15,]     0     0     0\n[16,]     0     0     0\n[17,]     0     0     0\n[18,]     0     0     0\n[19,]     0     0     0\n[20,]     0     0     0\n[21,]     0     0     0\n[22,]     0     0     0\n[23,]     0     0     0\n[24,]     0     0     0\n[25,]     0     0     0\n[26,]     1     0     0\n[27,]     0     1     0\n[28,]     0     0     1\n\n\n\\[\n\\mathbf{I}_{(28)} =\n  \\begin{bmatrix}\n   1 & 0 & 0 & \\dots & 0 \\\\\n   0 & 1 & 0 & \\dots & 0 \\\\\n   0 & 0 & 1 &\\dots & 0 \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   0 & 0 & 0 & \\dots & 1\n   \\end{bmatrix}\n\\]\nCom a função matrix(), podemos criar qualquer tipo de matriz, vetor ou escalar. Para isso, utilizamos três argumentos dentro da função:\n\ndata =: os elementos que compõem a matriz;\nnrow =: número de linhas da matriz;\nncol =: número de colunas da matriz.\n\n\njn &lt;- matrix(data = 1, nrow = n, ncol = 1)\njn\n\n      [,1]\n [1,]    1\n [2,]    1\n [3,]    1\n [4,]    1\n [5,]    1\n [6,]    1\n [7,]    1\n [8,]    1\n [9,]    1\n[10,]    1\n[11,]    1\n[12,]    1\n[13,]    1\n[14,]    1\n[15,]    1\n[16,]    1\n[17,]    1\n[18,]    1\n[19,]    1\n[20,]    1\n[21,]    1\n[22,]    1\n[23,]    1\n[24,]    1\n[25,]    1\n[26,]    1\n[27,]    1\n[28,]    1\n\nJnn &lt;- matrix(data = 1, nrow = n, ncol = n)\nJnn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [2,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [3,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [4,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [5,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [6,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [7,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [8,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [9,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[10,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[11,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[12,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[13,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[14,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[17,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[18,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[19,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[20,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[21,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[22,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[23,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[24,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[25,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[26,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[27,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[28,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n [1,]     1     1     1     1     1     1     1     1     1     1     1     1\n [2,]     1     1     1     1     1     1     1     1     1     1     1     1\n [3,]     1     1     1     1     1     1     1     1     1     1     1     1\n [4,]     1     1     1     1     1     1     1     1     1     1     1     1\n [5,]     1     1     1     1     1     1     1     1     1     1     1     1\n [6,]     1     1     1     1     1     1     1     1     1     1     1     1\n [7,]     1     1     1     1     1     1     1     1     1     1     1     1\n [8,]     1     1     1     1     1     1     1     1     1     1     1     1\n [9,]     1     1     1     1     1     1     1     1     1     1     1     1\n[10,]     1     1     1     1     1     1     1     1     1     1     1     1\n[11,]     1     1     1     1     1     1     1     1     1     1     1     1\n[12,]     1     1     1     1     1     1     1     1     1     1     1     1\n[13,]     1     1     1     1     1     1     1     1     1     1     1     1\n[14,]     1     1     1     1     1     1     1     1     1     1     1     1\n[15,]     1     1     1     1     1     1     1     1     1     1     1     1\n[16,]     1     1     1     1     1     1     1     1     1     1     1     1\n[17,]     1     1     1     1     1     1     1     1     1     1     1     1\n[18,]     1     1     1     1     1     1     1     1     1     1     1     1\n[19,]     1     1     1     1     1     1     1     1     1     1     1     1\n[20,]     1     1     1     1     1     1     1     1     1     1     1     1\n[21,]     1     1     1     1     1     1     1     1     1     1     1     1\n[22,]     1     1     1     1     1     1     1     1     1     1     1     1\n[23,]     1     1     1     1     1     1     1     1     1     1     1     1\n[24,]     1     1     1     1     1     1     1     1     1     1     1     1\n[25,]     1     1     1     1     1     1     1     1     1     1     1     1\n[26,]     1     1     1     1     1     1     1     1     1     1     1     1\n[27,]     1     1     1     1     1     1     1     1     1     1     1     1\n[28,]     1     1     1     1     1     1     1     1     1     1     1     1\n      [,26] [,27] [,28]\n [1,]     1     1     1\n [2,]     1     1     1\n [3,]     1     1     1\n [4,]     1     1     1\n [5,]     1     1     1\n [6,]     1     1     1\n [7,]     1     1     1\n [8,]     1     1     1\n [9,]     1     1     1\n[10,]     1     1     1\n[11,]     1     1     1\n[12,]     1     1     1\n[13,]     1     1     1\n[14,]     1     1     1\n[15,]     1     1     1\n[16,]     1     1     1\n[17,]     1     1     1\n[18,]     1     1     1\n[19,]     1     1     1\n[20,]     1     1     1\n[21,]     1     1     1\n[22,]     1     1     1\n[23,]     1     1     1\n[24,]     1     1     1\n[25,]     1     1     1\n[26,]     1     1     1\n[27,]     1     1     1\n[28,]     1     1     1\n\n\nNos casos anteriores, criamos o vetor coluna de 1’s jn de dimensão \\(28 \\times 1\\) e a matriz de 1’s Jnn de dimensão \\(28 \\times 28\\).\n\\[\nj_n = \\mathbf{j}_{(28 \\times 1)} = [1,1,...,1]'\n\\]\n\\[\nJnn = \\mathbf{J}_{(28 \\times 28)} =\n  \\begin{bmatrix}\n   1 & 1 & 1 & \\dots & 1 \\\\\n   1 & 1 & 1 & \\dots & 1 \\\\\n   1 & 1 & 1 &\\dots & 1 \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   1 & 1 & 1 & \\dots & 1\n   \\end{bmatrix}\n\\]\nAlém disso, note que a matriz \\(\\mathbf{J}\\) (Jnn) pode ser obtida por \\(\\mathbf{J} = \\mathbf{j} \\space \\mathbf{j}'\\), ou seja, a multiplicação do vetor coluna \\(\\mathbf{j}\\) (jn) pela sua transposta.\nNo R, utilizamos a função t() para realizar a transposição de uma matriz ou vetor.\n\nJnn &lt;- jn %*% t(jn)\nJnn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [2,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [3,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [4,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [5,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [6,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [7,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [8,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [9,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[10,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[11,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[12,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[13,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[14,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[17,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[18,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[19,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[20,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[21,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[22,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[23,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[24,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[25,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[26,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[27,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[28,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\n [1,]     1     1     1     1     1     1     1     1     1     1     1     1\n [2,]     1     1     1     1     1     1     1     1     1     1     1     1\n [3,]     1     1     1     1     1     1     1     1     1     1     1     1\n [4,]     1     1     1     1     1     1     1     1     1     1     1     1\n [5,]     1     1     1     1     1     1     1     1     1     1     1     1\n [6,]     1     1     1     1     1     1     1     1     1     1     1     1\n [7,]     1     1     1     1     1     1     1     1     1     1     1     1\n [8,]     1     1     1     1     1     1     1     1     1     1     1     1\n [9,]     1     1     1     1     1     1     1     1     1     1     1     1\n[10,]     1     1     1     1     1     1     1     1     1     1     1     1\n[11,]     1     1     1     1     1     1     1     1     1     1     1     1\n[12,]     1     1     1     1     1     1     1     1     1     1     1     1\n[13,]     1     1     1     1     1     1     1     1     1     1     1     1\n[14,]     1     1     1     1     1     1     1     1     1     1     1     1\n[15,]     1     1     1     1     1     1     1     1     1     1     1     1\n[16,]     1     1     1     1     1     1     1     1     1     1     1     1\n[17,]     1     1     1     1     1     1     1     1     1     1     1     1\n[18,]     1     1     1     1     1     1     1     1     1     1     1     1\n[19,]     1     1     1     1     1     1     1     1     1     1     1     1\n[20,]     1     1     1     1     1     1     1     1     1     1     1     1\n[21,]     1     1     1     1     1     1     1     1     1     1     1     1\n[22,]     1     1     1     1     1     1     1     1     1     1     1     1\n[23,]     1     1     1     1     1     1     1     1     1     1     1     1\n[24,]     1     1     1     1     1     1     1     1     1     1     1     1\n[25,]     1     1     1     1     1     1     1     1     1     1     1     1\n[26,]     1     1     1     1     1     1     1     1     1     1     1     1\n[27,]     1     1     1     1     1     1     1     1     1     1     1     1\n[28,]     1     1     1     1     1     1     1     1     1     1     1     1\n      [,26] [,27] [,28]\n [1,]     1     1     1\n [2,]     1     1     1\n [3,]     1     1     1\n [4,]     1     1     1\n [5,]     1     1     1\n [6,]     1     1     1\n [7,]     1     1     1\n [8,]     1     1     1\n [9,]     1     1     1\n[10,]     1     1     1\n[11,]     1     1     1\n[12,]     1     1     1\n[13,]     1     1     1\n[14,]     1     1     1\n[15,]     1     1     1\n[16,]     1     1     1\n[17,]     1     1     1\n[18,]     1     1     1\n[19,]     1     1     1\n[20,]     1     1     1\n[21,]     1     1     1\n[22,]     1     1     1\n[23,]     1     1     1\n[24,]     1     1     1\n[25,]     1     1     1\n[26,]     1     1     1\n[27,]     1     1     1\n[28,]     1     1     1\n\n\n\\[\n\\mathbf{J} = \\mathbf{j} \\space \\mathbf{j}' =\n\\begin{bmatrix}\n1 \\\\ 1 \\\\ \\vdots \\\\1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 & \\dots & 1\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n   1 & 1 & \\dots & 1 \\\\\n   1 & 1 & \\dots & 1 \\\\\n   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   1 & 1 & \\dots & 1\n   \\end{bmatrix}\n\\]\nA seguir, calcularemos a matriz de variâncias e covariâncias amostrais, dada pela equação:\n\\[\\mathbf{\\Sigma} = \\frac{1}{n-1} \\mathbf{Y}'(\\mathbf{I} - \\frac{1}{n}J) \\mathbf{Y}\\]\n\nSigma &lt;- (1 / (n - 1)) * t(Y) %*% (In - (1/n) * Jnn) %*% Y\nSigma\n\n         North     East    South     West\nNorth 290.4061 223.7526 288.4378 226.2712\nEast  223.7526 219.9299 229.0595 171.3743\nSouth 288.4378 229.0595 350.0040 259.5410\nWest  226.2712 171.3743 259.5410 226.0040\n\n\nA função t() realiza a transposição de uma matriz ou vetor. Já o operador %*% realiza a multiplicação entre duas matrizes ou vetores conformes.\nNo R, temos a função cov() que realiza o cálculo da matriz de variâncias e covariâncias. Para isso, basta declarar dentro da função a matriz desejada.\n\ncov(Y)\n\n         North     East    South     West\nNorth 290.4061 223.7526 288.4378 226.2712\nEast  223.7526 219.9299 229.0595 171.3743\nSouth 288.4378 229.0595 350.0040 259.5410\nWest  226.2712 171.3743 259.5410 226.0040\n\n\n\\[\n\\mathbf{\\Sigma} =\n  \\begin{bmatrix}\n   \\sigma^2_1 & \\sigma_{12} & \\sigma_{13} & \\dots & \\sigma_{1p} \\\\\n   \\sigma_{21} & \\sigma^2_2 & \\sigma_{23} & \\dots & \\sigma_{2p} \\\\\n   \\sigma_{31} & \\sigma_{32} & \\sigma^2_3 &\\dots & \\sigma_{p3} \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   \\sigma_{p1} & \\sigma_{p2} & \\sigma_{p3} & \\dots & \\sigma^2_p\n   \\end{bmatrix}\n\\]\nOs elementos da diagonal principal são as variâncias e os demais elementos, as covariâncias. Note que a matriz de variâncias e covariâncias é simétrica.\n\n1.3 Matriz de Correlações\nPara calcular a matriz de correlação (\\(\\mathbf{\\rho}_{ij}\\)), utilizamos a seguinte expressão:\n\\[\\mathbf{\\rho}_{ij} = \\mathbf{D}_{\\sigma}^{-1} \\mathbf{\\Sigma}  \\mathbf{D}_{\\sigma}^{-1}\\]\nem que \\(\\mathbf{D}_{\\sigma}\\) é uma matriz diagonal com a raiz quadrada das variâncias, ou seja, a raiz quadrada da diagonal da matriz de variâncias e covariâncias (\\(\\mathbf{\\Sigma}\\)).\n\nD &lt;- sqrt(diag(Sigma))\nD\n\n   North     East    South     West \n17.04131 14.83003 18.70839 15.03343 \n\ncorr &lt;- solve(diag(D)) %*% Sigma %*% solve(diag(D))\ncorr\n\n          [,1]      [,2]      [,3]      [,4]\n[1,] 1.0000000 0.8853667 0.9047173 0.8832188\n[2,] 0.8853667 1.0000000 0.8256001 0.7686801\n[3,] 0.9047173 0.8256001 1.0000000 0.9228082\n[4,] 0.8832188 0.7686801 0.9228082 1.0000000\n\n\nA função sqrt() realiza a operação raiz quadrada. Já a solve(), calcula a inversa de uma matriz.\nNo R, temos a função cor() que realiza o cálculo da matriz de correlação. Novamente, basta declarar a matriz dentro da função.\n\ncor(Y)\n\n          North      East     South      West\nNorth 1.0000000 0.8853667 0.9047173 0.8832188\nEast  0.8853667 1.0000000 0.8256001 0.7686801\nSouth 0.9047173 0.8256001 1.0000000 0.9228082\nWest  0.8832188 0.7686801 0.9228082 1.0000000\n\n\n\\[\n\\mathbf{\\rho} =\n  \\begin{bmatrix}\n   1 & \\rho_{12} & \\rho_{13} & \\dots & \\rho_{1p} \\\\\n   \\rho_{21} & 1 & \\rho_{23} & \\dots & \\rho_{2p} \\\\\n   \\rho_{31} & \\rho_{32} & 1 &\\dots & \\rho_{p3} \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   \\rho_{p1} & \\rho_{p2} & \\rho_{p3} & \\dots & 1\n   \\end{bmatrix}\n\\]\nPor fim, a partir da matriz de correlação (\\(\\mathbf{\\rho}_{ij}\\)), podemos retornar para a matriz de variâncias e covariâncias (\\(\\mathbf{\\Sigma}\\)) a partir da seguinte equação:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{D}_{\\sigma} \\mathbf{\\rho}_{ij} \\mathbf{D}_{\\sigma}\n\\]\n\nVerifica &lt;- diag(D) %*% corr %*% diag(D)\nVerifica\n\n         [,1]     [,2]     [,3]     [,4]\n[1,] 290.4061 223.7526 288.4378 226.2712\n[2,] 223.7526 219.9299 229.0595 171.3743\n[3,] 288.4378 229.0595 350.0040 259.5410\n[4,] 226.2712 171.3743 259.5410 226.0040\n\n\n\n\n\n\np = ncol(Y);\nn = nrow(Y);\nIn = I(n);\njn = j(n,1,1);\nJnn = J(n,n,1);\nSigma = (1/(n-1))*t(Y)*(In-(1/n)*Jnn)*Y;\nD = sqrt(diag(Sigma));\ncorr = inv(D)*Sigma*inv(D);\nVerifica = D*corr*D;\ntitle 'Matriz de variâncias e covariâncias amostrais utilizando proc iml';\nprint ,,Sigma[format=8.4],, 'Matriz de correlações:' ,, corr[format=8.5],, Verifica[format=8.4];"
  },
  {
    "objectID": "exemplo1.html#sec-corr",
    "href": "exemplo1.html#sec-corr",
    "title": "\n1  Exemplo\n",
    "section": "\n1.3 Matriz de Correlações",
    "text": "1.3 Matriz de Correlações\nPara calcular a matriz de correlação (\\(\\mathbf{\\rho}_{ij}\\)), utilizamos a seguinte expressão:\n\\[\\mathbf{\\rho}_{ij} = \\mathbf{D}_{\\sigma}^{-1} \\mathbf{\\Sigma}  \\mathbf{D}_{\\sigma}^{-1}\\]\nem que \\(\\mathbf{D}_{\\sigma}\\) é uma matriz diagonal com a raiz quadrada das variâncias, ou seja, a raiz quadrada da diagonal da matriz de variâncias e covariâncias (\\(\\mathbf{\\Sigma}\\)).\n\nD &lt;- sqrt(diag(Sigma))\nD\n\n   North     East    South     West \n17.04131 14.83003 18.70839 15.03343 \n\ncorr &lt;- solve(diag(D)) %*% Sigma %*% solve(diag(D))\ncorr\n\n          [,1]      [,2]      [,3]      [,4]\n[1,] 1.0000000 0.8853667 0.9047173 0.8832188\n[2,] 0.8853667 1.0000000 0.8256001 0.7686801\n[3,] 0.9047173 0.8256001 1.0000000 0.9228082\n[4,] 0.8832188 0.7686801 0.9228082 1.0000000\n\n\nA função sqrt() realiza a operação raiz quadrada. Já a solve(), calcula a inversa de uma matriz.\nNo R, temos a função cor() que realiza o cálculo da matriz de correlação. Novamente, basta declarar a matriz dentro da função.\n\ncor(Y)\n\n          North      East     South      West\nNorth 1.0000000 0.8853667 0.9047173 0.8832188\nEast  0.8853667 1.0000000 0.8256001 0.7686801\nSouth 0.9047173 0.8256001 1.0000000 0.9228082\nWest  0.8832188 0.7686801 0.9228082 1.0000000\n\n\n\\[\n\\mathbf{\\rho} =\n  \\begin{bmatrix}\n   1 & \\rho_{12} & \\rho_{13} & \\dots & \\rho_{1p} \\\\\n   \\rho_{21} & 1 & \\rho_{23} & \\dots & \\rho_{2p} \\\\\n   \\rho_{31} & \\rho_{32} & 1 &\\dots & \\rho_{p3} \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   \\rho_{p1} & \\rho_{p2} & \\rho_{p3} & \\dots & 1\n   \\end{bmatrix}\n\\]\nPor fim, a partir da matriz de correlação (\\(\\mathbf{\\rho}_{ij}\\)), podemos retornar para a matriz de variâncias e covariâncias (\\(\\mathbf{\\Sigma}\\)) a partir da seguinte equação:\n\\[\n\\mathbf{\\Sigma} = \\mathbf{D}_{\\sigma} \\mathbf{\\rho}_{ij} \\mathbf{D}_{\\sigma}\n\\]\n\nVerifica &lt;- diag(D) %*% corr %*% diag(D)\nVerifica\n\n         [,1]     [,2]     [,3]     [,4]\n[1,] 290.4061 223.7526 288.4378 226.2712\n[2,] 223.7526 219.9299 229.0595 171.3743\n[3,] 288.4378 229.0595 350.0040 259.5410\n[4,] 226.2712 171.3743 259.5410 226.0040"
  },
  {
    "objectID": "exemplo1.html#distância-de-mahalanobis-distância-padronizada",
    "href": "exemplo1.html#distância-de-mahalanobis-distância-padronizada",
    "title": "\n1  Exemplo\n",
    "section": "\n1.4 Distância de Mahalanobis (Distância padronizada)",
    "text": "1.4 Distância de Mahalanobis (Distância padronizada)\n\n\nR\nSAS\n\n\n\nPrimeiramente, calcularemos o vetor de médias (\\(\\mathbf{\\mu}\\)) da matriz \\(\\mathbf{Y}\\).\n\\[\\mathbf{\\mu} = \\frac{1}n \\mathbf{j}' \\mathbf{Y}\\]\n\nmi &lt;- (1/n) * t(jn) %*% Y\nmi\n\n        North     East    South     West\n[1,] 50.53571 46.17857 49.67857 45.17857\n\n\nCom o vetor de médias (\\(\\mathbf{\\mu}\\)), calcularemos a distância de Mahalanobis (\\(DM\\)), dada pela seguinte expressão:\n\\[\nDM = (\\mathbf{y} - \\mathbf{\\mu})' \\mathbf{\\Sigma} (\\mathbf{y} - \\mathbf{\\mu})\n\\]\n\nDM2 &lt;- rep(0, n)\nfor (i in 1:n) {\n  yi &lt;- Y[i,]\n  DM &lt;- as.numeric((yi - mi) %*% solve(Sigma) %*% t(yi - mi))\n  DM2[i] &lt;- DM\n}\nDM2\n\n [1]  7.619457  2.509622  2.714316  2.521244  1.744186  3.069220  2.212975\n [8]  4.114008  2.800802  3.315503  2.099698  5.891095  1.062421  1.691059\n[15]  8.996903 10.409160  4.075630  8.576964  7.124169  1.434898  1.086315\n[22]  1.372791  2.110873  3.913168  1.647245  4.667263  5.412107  3.806908\n\n\nNo R, podemos utilizar a função mahalanobis() para calcular a distância de Mahalanobis. Como argumentos, temos:\n\nx =: matriz utilizada para o cálculo;\ncenter =: o vetor de médias (\\(\\mu\\));\ncov =: a matriz de variâncias e covariâncias (\\(\\Sigma\\)).\n\n\nmahalanobis(x = Y, center = mi, cov = Sigma)\n\n [1]  7.619457  2.509622  2.714316  2.521244  1.744186  3.069220  2.212975\n [8]  4.114008  2.800802  3.315503  2.099698  5.891095  1.062421  1.691059\n[15]  8.996903 10.409160  4.075630  8.576964  7.124169  1.434898  1.086315\n[22]  1.372791  2.110873  3.913168  1.647245  4.667263  5.412107  3.806908\n\n\nOrdenando os valores do vetor da distância de Mahalanobis, temos:\n\nrank &lt;- rank(DM2)\ndata.frame(Y, DM2, rank)\n\n   North East South West       DM2 rank\n1     72   66    76   77  7.619457   25\n2     60   53    66   63  2.509622   11\n3     56   57    64   58  2.714316   13\n4     41   29    36   38  2.521244   12\n5     32   32    35   36  1.744186    7\n6     30   35    34   26  3.069220   15\n7     39   39    31   27  2.212975   10\n8     42   43    31   25  4.114008   20\n9     37   40    31   25  2.800802   14\n10    33   29    27   36  3.315503   16\n11    32   30    34   28  2.099698    8\n12    63   45    74   63  5.891095   23\n13    54   46    60   52  1.062421    1\n14    47   51    52   43  1.691059    6\n15    91   79   100   75  8.996903   27\n16    56   68    47   50 10.409160   28\n17    79   65    70   61  4.075630   19\n18    81   80    68   58  8.576964   26\n19    78   55    67   60  7.124169   24\n20    46   38    37   38  1.434898    4\n21    39   35    34   37  1.086315    2\n22    32   30    30   32  1.372791    3\n23    60   50    67   54  2.110873    9\n24    35   37    48   39  3.913168   18\n25    39   36    39   31  1.647245    5\n26    50   34    37   40  4.667263   21\n27    43   37    39   50  5.412107   22\n28    48   54    57   43  3.806908   17\n\n\nA observação 13 apresenta a menor distância, enquanto a 16, a maior distância de Mahalanobis.\n\n\n\nmi = (1/n)*t(jn)*y;\nprint 'Vetor de médias:' mi[format=5.2],,;\n\n\nDM2 = j(n,1,0);\ni=1;\ndo while (i&lt;=n);\nyi= Y[i,];\nDM = (yi-mi)*inv(Sigma)*t(yi-mi);\nDM2[i] = DM;\ni=i+1;\nend;\n\nrank = rank(DM2);\nprint\n\n\n'-----------------------------------------------------------------',\n'Distância de Mahalanobis de cada ponto (y) ao vetor de médias(mi)',\n'-----------------------------------------------------------------';\nprint ,,Y ' ' DM2[format=8.4] ' ' rank;\nquit;\n\nproc corr cov data=cork;\ntitle 'Matriz de variâncias e covariâncias utilizando proc corr';\nvar north east south west;\nrun;"
  },
  {
    "objectID": "exemplo2.1.html",
    "href": "exemplo2.1.html",
    "title": "\n2  Gráfico da Normal Bivariada\n",
    "section": "",
    "text": "Construiremos o gráfico da normal bivariada utilizando o pacote plotly. Este pacote permite confeccionar gráficos dinâmicos tridimensionais.\n\ninstall.packages(\"plotly\")\n\n\nlibrary(plotly)\n\n\n\nR\nSAS\n\n\n\nPara construir o gráfico da normal bivariada, primeiramente, definimos os valores dos vetores das variáveis y1 e y2.\n\ny1 &lt;- seq(from = -4, to = 4, by = 0.1)\ny1\n\n [1] -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6\n[16] -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1\n[31] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4\n[46]  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9\n[61]  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4\n[76]  3.5  3.6  3.7  3.8  3.9  4.0\n\ny2 &lt;- seq(from = -4, to = 4, by = 0.1)\ny2\n\n [1] -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6\n[16] -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1\n[31] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4\n[46]  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9\n[61]  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4\n[76]  3.5  3.6  3.7  3.8  3.9  4.0\n\n\nA função seq() cria uma sequência de valores e possui três argumentos:\n\nfrom =: valor em que a sequência começa;\nto =: valor em que a sequência termina;\nby =: de quanto em quanto a sequência é construída.\n\nEm seguida, calculamos a densidade bivariada, dada pela seguinte equação:\n\\[\n\\phi = \\frac{1}{2\\pi \\sqrt{1-r^2}} exp\\left\\{-\\frac{y_1^2 - 2r y_1 y_2 + y_2^2}{2(1-r^2)}\\right\\}\n\\]\nem que \\(r\\) é o coeficiente de correlação entre y1 e y2, variando de -1 a 1.\nO valor do coeficiente de correlação será salvo no objeto r. Já o objeto pi armazena o valor de \\(\\pi\\) com seis casas decimais.\nPara verificar as alterações gráficas, redefina o valor do objeto r (valor entre -1 e 1).\n\nr &lt;- -0.75  # Modificar este valor para verificar as alterações gráficas\npi\n\n[1] 3.141593\n\n\nPara realizar o cálculo da densidade bivariada no R, criamos uma função (function()) que executa a equação definida anteriormente para cada um dos valores de y1 e y2, gerando a matriz final, salva no objeto z.\n\nz &lt;- outer(y1, y2, function(y1,y2) { \n  phi &lt;- 1/(2 * pi * sqrt(1 - r^2)) * exp(-(y1^2 - 2 * r * y1 * y2 + y2^2) / (2 * (1 - r^2)))\n  return(phi)\n})\n\nCom isso, podemos criar o gráfico de densidade da normal bivariada.\n\nplot_ly(x = y1, y = y2, z = z, type = \"surface\") |&gt; \n  layout(title = paste(\"Densidade Normal Bivariada (r =\", r, \")\"))\n\n\n\n\n\nCom a função plot_ly(), definimos os objetos que compõem os eixos x, y e z, além do estilo do gráfico (type = \"surface\"). Em seguida, utilizando a função layout(), inserimos o título do gráfico.\nLink para aplicativo dinâmico: https://gustavojy.github.io/n-multi-graph-app/.\n\n\n\n%let r=-0.75; * Fixa o coeficiente de correlação entre y1 e y2;\ndata Normal;\n pi=3.1416;\n do y1=-4 to 4 by 0.1;\n  do y2=-4 to 4 by 0.1; \n  phi=exp(-(y1*y1-2*&r*y1*y2+y2*y2)/2/(1-&r*&r))/2/pi/sqrt(1-&r*&r);\n  output;\n  end;\n end;\nrun;\n\ngoptions reset=all border;\nproc g3d data=Normal;\n title 'Densidade Normal Bivariada (r =' &r ')';\n plot y1*y2=phi / rotate=-20;\nrun;"
  },
  {
    "objectID": "exemplo2.2.html#exemplo-1",
    "href": "exemplo2.2.html#exemplo-1",
    "title": "\n3  Propriedades\n",
    "section": "\n3.1 Exemplo 1",
    "text": "3.1 Exemplo 1\nPara os exemplos dos Teoremas 1, 2 e 3 considere um vetor aleatório \\(\\mathbf{y} \\sim N_3(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\), em que:\n\\[\n\\mathbf{\\mu} =\n  \\begin{bmatrix}\n  3 \\\\ 1 \\\\ 2\n  \\end{bmatrix}\n\\space e \\space\n\\mathbf{\\Sigma} =\n  \\begin{bmatrix}\n   4 & 0 & 2 \\\\\n   0 & 1 & -1 \\\\\n   2 & -1 & 3 \\\\\n   \\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\n\nmi &lt;- c(3, 1, 2)\nmi\n\n[1] 3 1 2\n\nSigma &lt;- matrix(c(4, 0, 2, 0, 1, -1, 2, -1, 3), nrow = 3, ncol = 3)\nSigma\n\n     [,1] [,2] [,3]\n[1,]    4    0    2\n[2,]    0    1   -1\n[3,]    2   -1    3\n\n\n\n\n\noptions nocenter ps=1000;\nproc iml;\nreset fuzz;\n\ny = {'y1','y2','y3'};\nmi = {3,1,2};\nSigma = {4 0 2, 0 1 -1, 2 -1 3};\n\n\n\n\n\n3.1.1 Teorema 1.1\nSeja \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) e \\(\\mathbf{a}\\) um vetor \\(p \\times 1\\) de constantes. Então, a variável aleatória \\(z\\) é dada por:\n\\[z = \\mathbf{a'}\\mathbf{y} \\sim N(\\mathbf{a' \\mu}, \\mathbf{a'\\Sigma a})\\]\nComo exemplo, considere:\n\\[\nz = y_1 - 2y_2 + y_3\n\\text{ ,   em que }\n\\mathbf{a} =\n  \\begin{bmatrix}\n    1 \\\\ -2 \\\\ 1\n  \\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\n\na &lt;- c(1, -2, 1)\na\n\n[1]  1 -2  1\n\nE_z &lt;- t(a) %*% mi\nE_z\n\n     [,1]\n[1,]    3\n\nvar_z &lt;- t(a) %*% Sigma %*% a\nvar_z\n\n     [,1]\n[1,]   19\n\n\nCom isso, a variável aleatória \\(z\\) fica:\n\\[z = \\mathbf{a'y} \\sim N(\\mathbf{a' \\mu} = 3, \\mathbf{a'\\Sigma a} = 19)\\]\n\n\n\na = {1, -2, 1};\nmi_z = t(a) * mi;\nvar_z = t(a)*Sigma*a;\nprint y mi Sigma,,,,, 'item (i)   ',,'z = y1-2y2+y3   ' mi_z var_z;\n\n\n\n\n\n3.1.2 Teorema 1.2\nSeja \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) e \\(\\mathbf{A}\\) uma matriz \\(k \\times p\\) de constantes e posto \\(k \\le p\\). Então, o vetor aleatório \\(\\mathbf{z}\\) é dado por:\n\\[\\mathbf{z} = \\mathbf{A} \\mathbf{y} \\sim N_k(\\mathbf{A \\mu}, \\mathbf{A\\Sigma A'})\\]\nAgora, considere as seguintes combinações lineares \\(z_1\\) e \\(z_2\\):\n\\[\n\\begin{aligned}\nz_1 = y_1 - y_2 + y_3 \\space \\text{ e } \\space z_2 = 3y_1 + y_2 - 2y_3 \\\\\n\\mathbf{z} =\n\\begin{bmatrix}\n\\mathbf{z_1} \\\\ \\mathbf{z_2}\n\\end{bmatrix}\n\\text{ = }\n\\begin{bmatrix}\n1 & -1 & 1 \\\\\n3 & 1 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ y_3\n\\end{bmatrix}\n\\text{ = }\n\\mathbf{Ay}\n\\end{aligned}\n\\]\n\n\nR\nSAS\n\n\n\n\nA &lt;- matrix(c(1, -1, 1, 3, 1, -2), nrow = 2, ncol = 3, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1   -1    1\n[2,]    3    1   -2\n\nE_z1z2 &lt;- A %*% mi\nE_z1z2\n\n     [,1]\n[1,]    4\n[2,]    6\n\ncov_z1z2 &lt;- A %*% Sigma %*% t(A)\ncov_z1z2\n\n     [,1] [,2]\n[1,]   14    4\n[2,]    4   29\n\n\nPelo teorema, o vetor aleatório fica:\n\\[\n\\mathbf{z} = \\mathbf{Ay} \\sim N_2\n\\left(\n  \\mathbf{A \\mu} =\n    \\begin{bmatrix}\n      4 \\\\ 6\n    \\end{bmatrix},\n  \\mathbf{A\\Sigma A'} =\n    \\begin{bmatrix}\n      14 & 4 \\\\\n      4 & 29\n    \\end{bmatrix}\n\\right)\n\\]\n\n\n\nZZ = {'z1 = y1-y2+y3','z2 = 3y1+y2-2y3'}; \nA = {1 -1  1, 3  1 -2};\nmi_ZZ = A*mi;\nSigma_ZZ = A*Sigma*t(A);\nprint 'item (ii)',,  ZZ '     ' mi_ZZ '     ' Sigma_ZZ;\n\n\n\n\n\n3.1.3 Teorema 2\nSe \\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu, \\Sigma})\\), então qualquer subvetor \\(r \\times 1\\) de \\(\\mathbf{y}\\) tem uma distribuição normal \\(r\\)-variada com médias, variâncias e covariâncias iguais às da distribuição normal \\(p\\)-variada original.\nComo exemplo, considere: \\(y_1 \\sim N(3,4)\\), \\(y_2 \\sim N(1,1)\\) e \\(y_3 \\sim N(2, 3)\\).\n\n\nR\nSAS\n\n\n\nDessa forma, o vetor \\(\\begin{bmatrix} y_1 \\\\ y_2\\end{bmatrix}\\):\n\nA12 &lt;- matrix(c(1, 0, 0, 0, 1, 0), nrow = 2, ncol = 3, byrow = TRUE)\nA12\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n\nmi_12 &lt;- A12 %*% mi\nmi_12\n\n     [,1]\n[1,]    3\n[2,]    1\n\nSigma_12 &lt;- A12 %*% Sigma %*% t(A12)\nSigma_12\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    0    1\n\n\n\\[\n\\begin{bmatrix}\n  y_1 \\\\ y_2\n\\end{bmatrix}\n  \\sim N_2\\left(\n    \\begin{bmatrix}\n      3 \\\\ 1\n    \\end{bmatrix},\n    \\begin{bmatrix}\n      4 & 0 \\\\\n      0 & 1\n    \\end{bmatrix}\n  \\right) \\\\\n\\]\nEnquanto isso, o vetor \\(\\begin{bmatrix} y_1 \\\\ y_3\\end{bmatrix}\\):\n\nA13 &lt;- matrix(c(1, 0, 0, 0, 0, 1), nrow = 2, ncol = 3, byrow = TRUE)\nA13\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    0    1\n\nmi_13 &lt;- A13 %*% mi\nmi_13\n\n     [,1]\n[1,]    3\n[2,]    2\n\nSigma_13 &lt;- A13 %*% Sigma %*% t(A13)\nSigma_13\n\n     [,1] [,2]\n[1,]    4    2\n[2,]    2    3\n\n\n\\[\n\\begin{bmatrix}\n  y_1 \\\\ y_3\n\\end{bmatrix}\n  \\sim N_2\\left(\n    \\begin{bmatrix}\n      3 \\\\ 2\n    \\end{bmatrix},\n    \\begin{bmatrix}\n      4 & 2 \\\\\n      2 & 3\n    \\end{bmatrix}\n  \\right)\n\\]\n\n\n\nA12 = {1 0 0, 0 1 0};\nmi_12 = A12*mi;\nSigma_12 = A12*Sigma*t(A12);\nprint mi_12 Sigma_12;\n\nA13 = {1 0 0, 0 0 1};\nmi_13 = A13*mi;\nSigma_13 = A13*Sigma*t(A13);\nprint mi_13 Sigma_13;\n\n\n\n\n\n3.1.4 Teorema 3\nSe o vetor particionado \\(\\mathbf{v} = \\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{x} \\end{bmatrix} \\sim N_{p+q}(\\mathbf{\\mu, \\Sigma})\\) então os subvetores aleatórios \\(\\mathbf{y}\\) e \\(\\mathbf{x}\\) são independentes se \\(\\mathbf{\\Sigma}_{xy} = 0\\).\n\n\nR\nSAS\n\n\n\n\na1 &lt;- c(1, 0, 0)\na1\n\n[1] 1 0 0\n\nb2 &lt;- c(0, 1, 0)\nb2\n\n[1] 0 1 0\n\nSigma\n\n     [,1] [,2] [,3]\n[1,]    4    0    2\n[2,]    0    1   -1\n[3,]    2   -1    3\n\ncov_12 &lt;- t(a1) %*% Sigma %*% b2\ncov_12\n\n     [,1]\n[1,]    0\n\n\n\n\n\na1 = {1 0 0};\nb2 = {0 1 0};\ncov_12 = a1*Sigma*t(b2);\nprint cov_12;"
  },
  {
    "objectID": "exemplo2.2.html#exemplo-2",
    "href": "exemplo2.2.html#exemplo-2",
    "title": "\n3  Propriedades\n",
    "section": "\n3.2 Exemplo 2",
    "text": "3.2 Exemplo 2\n\n3.2.1 Teorema 4\nSe \\(\\mathbf{y}\\) e \\(\\mathbf{x}\\) têm distribuição conjunta normal multivariada com \\(\\mathbf{\\Sigma}_{yx} \\ne 0\\) então a distribuição condicional de \\(\\mathbf{y}\\) dado \\(\\mathbf{x}\\), \\(f(\\mathbf{y} \\mid \\mathbf{x})\\), é normal multivariada com vetor de médias e matriz de covariâncias dados por:\n\\[\n\\begin{aligned}\nE(\\mathbf{y} \\mid \\mathbf{x}) = \\mathbf{\\mu}_y + \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1}(\\mathbf{x} - \\mathbf{\\mu}_x) \\\\\ncov(\\mathbf{y} \\mid \\mathbf{x}) = \\mathbf{\\Sigma}_{yy} - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{xy}\n\\end{aligned}\n\\]\nPara ilustrar o Teorema 4, considere o vetor aleatório \\(\\mathbf{v} \\sim N_4(\\mathbf{\\mu, \\Sigma})\\) em que:\n\\[\n\\begin{aligned}\n\\mathbf{\\mu} =\n\\begin{bmatrix}\n2 \\\\ 5 \\\\ -2 \\\\ 1\n\\end{bmatrix}\n\\text{ e }\n\\mathbf{\\Sigma} =\n\\begin{bmatrix}\n9 & 0 & 3 & 3 \\\\\n0 & 1 & -1 & 2 \\\\\n3 & -1 & 6 & -3 \\\\\n3 & 2 & -3 & 7 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\n\nR\nSAS\n\n\n\n\nmi &lt;- c(2, 5, -2, 1)\nmi\n\n[1]  2  5 -2  1\n\nSigma &lt;- matrix(\n  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), \n  nrow = 4, byrow = TRUE\n)\nSigma\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    0    3    3\n[2,]    0    1   -1    2\n[3,]    3   -1    6   -3\n[4,]    3    2   -3    7\n\n\n\n\n\noptions nocenter ps=1000;\nproc iml;\nreset fuzz;\n\nmi = {2,5,-2,1};\nSigma = {9 0 3 3, 0 1 -1 2, 3 -1 6 -3, 3 2 -3 7};\nprint v mi Sigma;\n\n\n\n\nSe \\(\\mathbf{v} = \\begin{bmatrix} y_1, y_2, x_1, x_2 \\end{bmatrix} '\\) é um vetor particionado dessa forma, então:\n\\[\n\\mathbf{\\mu_y} =\n\\begin{bmatrix}\n2 \\\\ 5\n\\end{bmatrix}\n,\n\\mathbf{\\mu_x} =\n\\begin{bmatrix}\n-2 \\\\ 1\n\\end{bmatrix} \\\\\n\\]\n\\[\n\\mathbf{\\Sigma_{yy}} =\n\\begin{bmatrix}\n9 & 0 \\\\ 0 & 1\n\\end{bmatrix}\n,\n\\mathbf{\\Sigma_{xx}}\n\\begin{bmatrix}\n6 & -3 \\\\ -3 & 7\n\\end{bmatrix}\n,\n\\mathbf{\\Sigma_{yx}}\n\\begin{bmatrix}\n3 & 3 \\\\ -1 & 2\n\\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\n\nAy &lt;- matrix(c(1, 0, 0, 0, 0, 1, 0, 0), nrow = 2, byrow = TRUE)\nAy\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n\nmi_y &lt;- Ay %*% mi\nmi_y\n\n     [,1]\n[1,]    2\n[2,]    5\n\nSigma_yy &lt;- Ay %*% Sigma %*% t(Ay)\nSigma_yy\n\n     [,1] [,2]\n[1,]    9    0\n[2,]    0    1\n\nAx &lt;- matrix(c(0, 0, 1, 0, 0, 0, 0, 1), nrow = 2, byrow = TRUE)\nAx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    1    0\n[2,]    0    0    0    1\n\nmi_x &lt;- Ax %*% mi\nmi_x\n\n     [,1]\n[1,]   -2\n[2,]    1\n\nSigma_xx &lt;- Ax %*%Sigma %*% t(Ax)\nSigma_xx\n\n     [,1] [,2]\n[1,]    6   -3\n[2,]   -3    7\n\nSigma_yx &lt;- Ay %*% Sigma %*% t(Ax)\nSigma_yx\n\n     [,1] [,2]\n[1,]    3    3\n[2,]   -1    2\n\n\n\n\n\nAy = {1 0 0 0, 0 1 0 0};\nmi_y = Ay*mi;\nSigma_yy = Ay*Sigma*t(Ay);\n\nAx = {0 0 1 0, 0 0 0 1};\nmi_x = Ax*mi;\nSigma_xx = Ax*Sigma*t(Ax);\n\nSigma_yx = Ay*Sigma*t(Ax);\nprint mi_y Sigma_yy,, mi_x Sigma_xx,, Sigma_yx;\n\n\n\n\nPara calcular \\(cov(\\mathbf{y} \\mid \\mathbf{x}) = \\mathbf{\\Sigma}_{yy} - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{xy}\\) procedemos da seguinte maneira:\n\n\nR\nSAS\n\n\n\n\ncov_ydx &lt;- Sigma_yy - Sigma_yx %*% solve(Sigma_xx) %*% t(Sigma_yx)\ncov_ydx\n\n           [,1]       [,2]\n[1,]  3.8181818 -0.7272727\n[2,] -0.7272727  0.4242424\n\n\n\\[\ncov(\\mathbf{y} \\mid \\mathbf{x}) =\n\\begin{bmatrix}\n3,82 & -0,73 \\\\ -0,73 & 0,42\n\\end{bmatrix}\n\\]\n\n\n\ncov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);\n\nprint Sigma_yy[format=6.0],, cov_ydx[format=6.2];\n\n\n\n\nNote que a variância de \\(y_1\\) e de \\(y_2\\) são maiores do que as variâncias condicionais:\n\nSigma_yy\n\n     [,1] [,2]\n[1,]    9    0\n[2,]    0    1\n\ncov_ydx\n\n           [,1]       [,2]\n[1,]  3.8181818 -0.7272727\n[2,] -0.7272727  0.4242424\n\n\n\\[\n\\begin{aligned}\nvar(y_1) = 9 \\text{ e } var(y_1 \\mid x_1, x_2) = 3,82 \\\\\nvar(y_2) = 1 \\text{ e } var(y_2 \\mid x_1, x_2) = 0,42\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "exemplo2.2.html#exemplo-3",
    "href": "exemplo2.2.html#exemplo-3",
    "title": "\n3  Propriedades\n",
    "section": "\n3.3 Exemplo 3",
    "text": "3.3 Exemplo 3\n\n3.3.1 Teorema 4 (Corolário)\nConsidere:\n\\[\n\\mathbf{v} =\n\\begin{bmatrix}\ny, & x_1, & \\dots, & x_q\n\\end{bmatrix}\n' =\n\\begin{bmatrix}\ny \\\\ \\mathbf{x'}\n\\end{bmatrix}\n\\text{ com }\n\\mathbf{\\mu} =\n\\begin{bmatrix}\n\\mu_y \\\\ \\mathbf{\\mu_x}\n\\end{bmatrix}\n\\text{ e }\n\\mathbf{\\Sigma} =\n\\begin{bmatrix}\n\\sigma_y^2 & \\mathbf{\\sigma}_{yx}' \\\\\n\\mathbf{\\sigma}_{yx} & \\mathbf{\\Sigma}_{xx}\n\\end{bmatrix}\n\\text{, }\n\\]\nentão \\(y \\mid \\mathbf{x}\\) tem distribuição normal univariada com:\n\\[\n\\begin{aligned}\nE(y \\mid \\mathbf{x}) = \\mu_y + \\mathbf{\\sigma}_{yx}' \\mathbf{\\Sigma}_{xx}^{-1} (\\mathbf{x} - \\mathbf{\\mu}_x) \\\\\nvar(y \\mid \\mathbf{x}) = \\sigma_{y}^2 - \\mathbf{\\sigma}_{yx}' \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\sigma}_{yx}\n\\end{aligned}\n\\]\nComo exemplo, seguimos com o vetor \\(\\mathbf{v}\\) utilizado no Teorema 4- 3.2.1.\n\\[\n\\mathbf{v} \\sim N_4(\\mathbf{\\mu}, \\mathbf{\\Sigma})\n\\text{ , }\n\\mathbf{\\mu} =\n\\begin{bmatrix}\n2 \\\\ 5 \\\\ -2 \\\\ 1\n\\end{bmatrix}\n\\text{ e }\n\\mathbf{\\Sigma} =\n\\begin{bmatrix}\n9 & 0 & 3 & 3 \\\\\n0 & 1 & -1 & 2 \\\\\n3 & -1 & 6 & -3 \\\\\n3 & 2 & -3 & 7 \\\\\n\\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\n\nmi &lt;- c(2, 5, -2, 1)\nmi\n\n[1]  2  5 -2  1\n\nSigma &lt;- matrix(\n  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), \n  nrow = 4, byrow = TRUE\n  )\nSigma\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    0    3    3\n[2,]    0    1   -1    2\n[3,]    3   -1    6   -3\n[4,]    3    2   -3    7\n\n\n\n\n\noptions nocenter ps=1000;\nproc iml;\nreset fuzz;\n\nmi = {2,5,-2,1};\nSigma = {9 0 3 3, 0 1 -1 2, 3 -1 6 -3, 3 2 -3 7};\nprint v mi Sigma;\n\n\n\n\nSe \\(\\mathbf{v} = \\begin{bmatrix} y, & x_1, & x_2, & x_3 \\end{bmatrix}'\\) então:\n\\[\\mu_y = 2 \\text{ e } var(y) = 9 \\\\\\]\n\\[\n\\mathbf{\\mu}_x =\n\\begin{bmatrix}\n5 \\\\ -2 \\\\ 1\n\\end{bmatrix}\n\\text{ , }\n\\mathbf{\\Sigma}_{xx} =\n\\begin{bmatrix}\n1 & -1 & 2 \\\\\n-1 & 6 & -3 \\\\\n2 & -3 & 7\n\\end{bmatrix}\n\\text{ e }\n\\mathbf{\\sigma}_{yx} =\n\\begin{bmatrix}\n0 \\\\ 3 \\\\ 3\n\\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\n\nAy &lt;- matrix(c(1, 0, 0, 0), nrow = 1)\nAy\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n\nmi_y &lt;- Ay %*% mi\nmi_y\n\n     [,1]\n[1,]    2\n\nSigma_yy &lt;- Ay %*% Sigma %*% t(Ay)\nSigma_yy\n\n     [,1]\n[1,]    9\n\nAx &lt;- matrix(\n  c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1), \n  nrow = 3, ncol = 4, byrow = TRUE\n)\nAx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1    0    0\n[2,]    0    0    1    0\n[3,]    0    0    0    1\n\nmi_x &lt;- Ax %*% mi\nmi_x\n\n     [,1]\n[1,]    5\n[2,]   -2\n[3,]    1\n\nSigma_xx &lt;- Ax %*% Sigma %*% t(Ax)\nSigma_xx\n\n     [,1] [,2] [,3]\n[1,]    1   -1    2\n[2,]   -1    6   -3\n[3,]    2   -3    7\n\nsigma_yx &lt;- Ay %*% Sigma %*% t(Ax)\nsigma_yx\n\n     [,1] [,2] [,3]\n[1,]    0    3    3\n\n\n\n\n\nAy = {1 0 0 0};\nmi_y = Ay*mi;\nSigma_yy = Ay*Sigma*t(Ay);\n\nAx = {0 1 0 0, 0 0 1 0, 0 0 0 1};\nmi_x = Ax*mi;\nSigma_xx = Ax*Sigma*t(Ax);\n\nSigma_yx = Ay*Sigma*t(Ax);\nprint mi_y Sigma_yy,, mi_x Sigma_xx,, Sigma_yx;\n\n\n\n\nPelo Corolário, temos:\n\\[\nvar(y \\mid x_1, x_2, x_3) = \\sigma_{y}^2 - \\mathbf{\\sigma}_{yx}' \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\sigma}_{yx}\n\\]\n\n\nR\nSAS\n\n\n\n\ncov_ydx &lt;- Sigma_yy - sigma_yx %*% solve(Sigma_xx) %*% t(sigma_yx)\ncov_ydx\n\n         [,1]\n[1,] 2.571429\n\n\nPortanto, \\(var(y \\mid x_1, x_2, x_3) = 2.571\\).\nNote que a variância de \\(y\\) é maior do que a variância condicional:\n\nSigma_yy\n\n     [,1]\n[1,]    9\n\ncov_ydx\n\n         [,1]\n[1,] 2.571429\n\n\n\\[\n\\begin{aligned}\nvar(y) &= 9 \\\\\nvar(y \\mid x_1, x_2, x_3) &= 2.571\n\\end{aligned}\n\\]\n\n\n\ncov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);\n\nprint Sigma_yy[format=6.0],, cov_ydx[format=6.2];"
  },
  {
    "objectID": "exemplo2.3.html",
    "href": "exemplo2.3.html",
    "title": "\n4  Correlação Parcial\n",
    "section": "",
    "text": "Considere um vetor \\(\\mathbf{v}\\), formado por um subconjunto de \\(y's\\) que inclui \\(y_1\\) e \\(y_2\\), sendo denotado por \\(\\mathbf{y}\\). O outro subconjunto de \\(y's\\) que inclui \\(y_3\\) e \\(y_4\\) é denotado por \\(\\mathbf{x}\\).\n\\[\n\\mathbf{v} =\n\\begin{bmatrix}\n\\mathbf{y} \\\\ \\mathbf{x}\n\\end{bmatrix}\n\\text{, em que }\n\\mathbf{y} =\n\\begin{bmatrix}\ny_1, ..., y_2\n\\end{bmatrix}\n\\text{'  e  }\n\\mathbf{x} =\n\\begin{bmatrix}\ny_3, ..., y_4\n\\end{bmatrix}\n\\text{'}\n\\]\nVamos comparar o coeficiente de correlação parcial entre \\(y_1\\) e \\(y_2\\) (\\(\\rho_{12}\\)) com o coeficiente de correlação parcial condicional de \\(y_1\\) e \\(y_2\\) dado \\(y_3\\) e \\(y_4\\) (\\(\\rho_{12.34}\\)).\nPara isso, utilizaremos a seguinte matriz de variâncias e covariâncias:\n\\[\n\\mathbf{\\Sigma} =\n  \\begin{bmatrix}\n    9 & 0 & 3 & 3 \\\\\n    0 & 1 & -1 & 2 \\\\\n    3 & -1 & 6 & -3 \\\\\n    3 & 2 & -3 & 7 \\\\\n   \\end{bmatrix}\n\\text{.}\n\\]\nPrimeiramente, calcularemos a correlação linear entre as variáveis \\(y_1\\) e \\(y_2\\), ou seja, \\(\\rho_{12}\\) (vide Seção 1.3).\n\n\nR\nSAS\n\n\n\nCom a matriz de variâncias e covariâncias salva no objeto Sigma, calculamos a raiz quadrada da diagonal de Sigma, ou seja, a raiz quadrada das variâncias (D).\n\nSigma &lt;- matrix(\n  c(9, 0, 3, 3, 0, 1, -1, 2, 3, -1, 6, -3, 3, 2, -3, 7), \n  nrow = 4, byrow = TRUE\n)\nSigma\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    0    3    3\n[2,]    0    1   -1    2\n[3,]    3   -1    6   -3\n[4,]    3    2   -3    7\n\nD &lt;- sqrt(diag(Sigma))\nD\n\n[1] 3.000000 1.000000 2.449490 2.645751\n\n\nCom a raiz quadrada das variâncias (D), calculamos os coeficientes de correlação por meio da seguinte expressão:\n\\[\\mathbf{\\rho} = \\mathbf{D}_{\\sigma}^{-1} \\mathbf{\\Sigma} \\mathbf{D}_{\\sigma}^{-1}\\]\n\nRo &lt;- solve(diag(D)) %*% Sigma %*% solve(diag(D))\nRo\n\n          [,1]       [,2]       [,3]       [,4]\n[1,] 1.0000000  0.0000000  0.4082483  0.3779645\n[2,] 0.0000000  1.0000000 -0.4082483  0.7559289\n[3,] 0.4082483 -0.4082483  1.0000000 -0.4629100\n[4,] 0.3779645  0.7559289 -0.4629100  1.0000000\n\n\nAssim, verificamos que as variáveis \\(y_1\\) e \\(y_2\\) são não correlacionadas (\\(\\rho_{12} = \\rho_{21} = 0\\)).\n\n\n\noptions nocenter ps=1000;\nproc iml;\nreset fuzz;\n\nSigma = {9  0  3  3, \n         0  1 -1  2, \n         3 -1  6 -3, \n         3  2 -3  7};\nD = sqrt(diag(Sigma));\nRo = inv(D)*Sigma*inv(D);\nprint Sigma '     ' Ro[format=6.2];\n\n\n\n\nPara calcular o coeficiente de correlação parcial entre \\(y_1\\) e \\(y_2\\) dada a ocorrência de \\(y_3\\) e \\(y_4\\) (\\(\\rho_{12.34}\\)), particionaremos \\(\\Sigma\\) da seguinte maneira:\n\\[\n\\mathbf{\\Sigma}_{yy} =\n  \\begin{bmatrix}\n    9 & 0 \\\\\n    0 & 1\n  \\end{bmatrix}\n\\text{  ,  }\n\\mathbf{\\Sigma}_{xx} =\n  \\begin{bmatrix}\n    6 & -3 \\\\\n    -3 & 7\n  \\end{bmatrix}\n,\\mathbf{\\Sigma}_{yx} =\n  \\begin{bmatrix}\n    3 & 3 \\\\\n    -1 & 2\n  \\end{bmatrix}\n\\]\n\n\nR\nSAS\n\n\n\nPara particionar \\(\\mathbf{\\Sigma}\\), utilizaremos o Teorema 2 3.1.3.\nA primeira partição (\\(\\mathbf{\\Sigma}_{yy}\\)) fica:\n\nAy &lt;- matrix(c(1, 0, 0, 0, 0, 1, 0, 0), nrow = 2, byrow = TRUE)\nAy\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n\nSigma_yy &lt;- Ay %*% Sigma %*% t(Ay)\nSigma_yy\n\n     [,1] [,2]\n[1,]    9    0\n[2,]    0    1\n\n\nNote que esta partição corresponde ao \\(\\rho_{12} = 0\\):\n\nDyy &lt;- sqrt(diag(Sigma_yy))\nDyy\n\n[1] 3 1\n\nRo_yy &lt;- solve(diag(Dyy)) %*% Sigma_yy %*% solve(diag(Dyy))\nRo_yy\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nPara a partição \\(\\mathbf{\\Sigma}_{xx}\\) temos:\n\nAx &lt;- matrix(c(0, 0, 1, 0, 0, 0, 0, 1), nrow = 2, byrow = TRUE)\nAx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    1    0\n[2,]    0    0    0    1\n\nSigma_xx &lt;- Ax %*% Sigma %*% t(Ax)\nSigma_xx\n\n     [,1] [,2]\n[1,]    6   -3\n[2,]   -3    7\n\n\nPor fim, a partição \\(\\mathbf{\\Sigma}_{yx}\\):\n\nSigma_yx &lt;- Ay %*% Sigma %*% t(Ax)\nSigma_yx\n\n     [,1] [,2]\n[1,]    3    3\n[2,]   -1    2\n\n\nUsando o Teorema 4 3.2.1, obtemos a matriz de covariâncias condicionais de \\(y_1\\) e \\(y_2\\) dado \\(y_3\\) e \\(y_4\\):\n\\[cov(y \\mid x) = \\mathbf{\\Sigma}_{yy} - \\mathbf{\\Sigma}_{yx} \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{xy}\\]\n\ncov_ydx &lt;- Sigma_yy - Sigma_yx %*% solve(Sigma_xx) %*% t(Sigma_yx)\ncov_ydx\n\n           [,1]       [,2]\n[1,]  3.8181818 -0.7272727\n[2,] -0.7272727  0.4242424\n\n\nNote que \\(\\mathbf{\\Sigma}\\) é uma matriz simétrica, logo \\(\\mathbf{\\Sigma}_{yx} = \\mathbf{\\Sigma}'_{xy} \\iff \\mathbf{\\Sigma}_{xy} = \\mathbf{\\Sigma}'_{yx}\\).\nAssim, podemos calcular o coeficiente de correlação parcial \\(\\rho_{12.34}\\):\n\nD &lt;- sqrt(diag(cov_ydx))\nD\n\n[1] 1.9540168 0.6513389\n\nRo_ydx &lt;- solve(diag(D)) %*% cov_ydx %*% solve(diag(D))\nRo_ydx\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.5714286\n[2,] -0.5714286  1.0000000\n\n\nPortanto, conhecendo os valores de \\(y_3\\) e \\(y_4\\), a correlação parcial entre \\(y_1\\) e \\(y_2\\) é negativa (\\(\\rho_{12.34} = -0,571\\)) e diferente da correlação linear simples (\\(\\rho_{12} = 0\\)).\n\nRo_yy\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nRo_ydx\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.5714286\n[2,] -0.5714286  1.0000000\n\n\n\n\n\nAy = {1 0 0 0, 0 1 0 0};\nSigma_yy = Ay*Sigma*t(Ay);\nDyy = sqrt(diag(Sigma_yy));\nRo_yy = inv(Dyy)*Sigma_yy*inv(Dyy);\n\nAx = {0 0 1 0, 0 0 0 1};\nSigma_xx = Ax*Sigma*t(Ax);\n\nSigma_yx = Ay*Sigma*t(Ax);\n\ncov_ydx = Sigma_yy-Sigma_yx*inv(Sigma_xx)*t(Sigma_yx);\nD = sqrt(diag(cov_ydx));\nRo_ydx = inv(D)*cov_ydx*inv(D);\n\nprint Ro_yy[format=8.2] '    ' Ro_ydx[format=8.3];"
  },
  {
    "objectID": "exemplo3.1.html#análise-exploratória",
    "href": "exemplo3.1.html#análise-exploratória",
    "title": "\n5  Estimação e Teste de Hipóteses\n",
    "section": "\n5.1 Análise exploratória",
    "text": "5.1 Análise exploratória\nPara visualizar a dispersão dos dados, construiremos um gráfico de pontos (gráfico de dispersão) entre as notas dos alunos na prova (y) e nas tarefas de casa (x1). Para isso, utilizaremos os recursos do pacote ggplot2.\n\nlibrary(ggplot2)\n\n\nnotas &lt;- data.frame(y, x1)\nnotas\n\n    y x1\n1  95 96\n2  80 77\n3   0  0\n4   0  0\n5  79 78\n6  77 64\n7  72 89\n8  66 47\n9  98 90\n10 90 93\n11  0 18\n12 95 86\n13 35  0\n14 50 30\n15 72 59\n16 55 77\n17 75 74\n18 66 67\n\nggplot(data = notas, aes(x = x1, y = y)) +\n  geom_point()\n\n\n\n\nPrimeiramente, criamos um data frame com as notas dos alunos. Em seguida, com a função ggplot(), atribuímos a variável nota nas tarefas (x1) ao eixo das abscissas e a nota na prova (y) ao eixo das ordenadas. Com a geom_point(), adicionamos a camada gráfica referente à geometria de pontos, ou seja, a geometria do gráfico de dispersão. Note que as funções ggplot() e geom_point() são ligadas pelo operador +."
  },
  {
    "objectID": "exemplo3.1.html#sec-estima-beta",
    "href": "exemplo3.1.html#sec-estima-beta",
    "title": "\n5  Estimação e Teste de Hipóteses\n",
    "section": "\n5.2 Estimação dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\)\n",
    "text": "5.2 Estimação dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\)\n\nPara estimar os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\), utilizaremos o Método dos Mínimos Quadrados Ordinários (MQO). O método consiste em achar os estimadores \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) que minimizem a soma de quadrados dos desvios \\(\\sum^n_{i=1} (y_i - \\hat{y}_i)^2\\).\nPrimeiramente, criaremos os seguintes objetos:\n\nn: número de observações \\(n\\);\njn: vetor coluna de 1’s \\(\\mathbf{j}\\);\nJnn: matriz de 1’s \\(\\mathbf{J}\\);\nIn: matriz identidade \\(\\mathbf{I}\\).\n\n\n\nR\nSAS\n\n\n\n\nn &lt;- length(y)\nn\n\n[1] 18\n\njn &lt;- matrix(data = 1, nrow = n, ncol = 1)\njn\n\n      [,1]\n [1,]    1\n [2,]    1\n [3,]    1\n [4,]    1\n [5,]    1\n [6,]    1\n [7,]    1\n [8,]    1\n [9,]    1\n[10,]    1\n[11,]    1\n[12,]    1\n[13,]    1\n[14,]    1\n[15,]    1\n[16,]    1\n[17,]    1\n[18,]    1\n\nJnn &lt;- jn %*% t(jn)\nJnn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [2,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [3,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [4,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [5,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [6,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [7,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [8,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n [9,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[10,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[11,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[12,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[13,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[14,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[15,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[16,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[17,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n[18,]    1    1    1    1    1    1    1    1    1     1     1     1     1\n      [,14] [,15] [,16] [,17] [,18]\n [1,]     1     1     1     1     1\n [2,]     1     1     1     1     1\n [3,]     1     1     1     1     1\n [4,]     1     1     1     1     1\n [5,]     1     1     1     1     1\n [6,]     1     1     1     1     1\n [7,]     1     1     1     1     1\n [8,]     1     1     1     1     1\n [9,]     1     1     1     1     1\n[10,]     1     1     1     1     1\n[11,]     1     1     1     1     1\n[12,]     1     1     1     1     1\n[13,]     1     1     1     1     1\n[14,]     1     1     1     1     1\n[15,]     1     1     1     1     1\n[16,]     1     1     1     1     1\n[17,]     1     1     1     1     1\n[18,]     1     1     1     1     1\n\nIn &lt;- diag(n)\nIn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n [1,]    1    0    0    0    0    0    0    0    0     0     0     0     0\n [2,]    0    1    0    0    0    0    0    0    0     0     0     0     0\n [3,]    0    0    1    0    0    0    0    0    0     0     0     0     0\n [4,]    0    0    0    1    0    0    0    0    0     0     0     0     0\n [5,]    0    0    0    0    1    0    0    0    0     0     0     0     0\n [6,]    0    0    0    0    0    1    0    0    0     0     0     0     0\n [7,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n [8,]    0    0    0    0    0    0    0    1    0     0     0     0     0\n [9,]    0    0    0    0    0    0    0    0    1     0     0     0     0\n[10,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n[11,]    0    0    0    0    0    0    0    0    0     0     1     0     0\n[12,]    0    0    0    0    0    0    0    0    0     0     0     1     0\n[13,]    0    0    0    0    0    0    0    0    0     0     0     0     1\n[14,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[15,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[16,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[17,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n[18,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n      [,14] [,15] [,16] [,17] [,18]\n [1,]     0     0     0     0     0\n [2,]     0     0     0     0     0\n [3,]     0     0     0     0     0\n [4,]     0     0     0     0     0\n [5,]     0     0     0     0     0\n [6,]     0     0     0     0     0\n [7,]     0     0     0     0     0\n [8,]     0     0     0     0     0\n [9,]     0     0     0     0     0\n[10,]     0     0     0     0     0\n[11,]     0     0     0     0     0\n[12,]     0     0     0     0     0\n[13,]     0     0     0     0     0\n[14,]     1     0     0     0     0\n[15,]     0     1     0     0     0\n[16,]     0     0     1     0     0\n[17,]     0     0     0     1     0\n[18,]     0     0     0     0     1\n\n\nAs operações matriciais para obter \\(\\hat\\beta_1\\) e \\(\\hat\\beta_0\\) são:\n\\[\n\\begin{align}\n\\hat\\beta_1 &= \\frac{\\left[\\mathbf{x}' \\left(\\mathbf{I} - \\frac{1}n \\mathbf{J} \\right) \\mathbf{y}\\right]} {\\left[\\mathbf{x}' \\left(\\mathbf{I} - \\frac{1}n \\mathbf{J} \\right) \\mathbf{x}\\right]} \\\\\n\\hat\\beta_0 &= \\frac{1}n \\mathbf{j}' (\\mathbf{y} - \\beta_1 \\mathbf{x})\n\\end{align}\n\\]\n\nBeta1 &lt;- as.numeric(t(x1) %*% (In - (1 / n) * Jnn) %*% y / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1))\nBeta1\n\n[1] 0.8726465\n\nBeta0 &lt;- as.numeric((1 / n) * t(jn) %*% (y - Beta1 * x1))\nBeta0\n\n[1] 10.72691\n\n\nO R possui a função lm(), que nos retorna as estimativas dos parâmetros de um modelo linear. Para isso, dentro da função, colocamos os valores de y e x1 ligados pelo operador ~, que representa uma fórmula.\n\nlm(y ~ x1)\n\n\nCall:\nlm(formula = y ~ x1)\n\nCoefficients:\n(Intercept)           x1  \n    10.7269       0.8726  \n\n\nCom isso, a equação predita fica:\n\\[\\hat{y}_i = 10.7269 + 0.8726 x_i\\]\nGraficamente, temos:\n\nggplot(data = notas, aes(x = x1, y = y)) +\n  geom_point() + \n  geom_abline(intercept = 10.7269, slope = 0.8726, color = \"red\")\n\n\n\n\nCom a função geom_abline(), declaramos o \\(\\hat\\beta_0 = 10.7269\\) no argumento intercept = e \\(\\hat\\beta_1 = 0.8726\\) no argumento slope =.\nTambém podemos utilizar a função geom_smooth() para construir a reta de regressão. Essa função calcula, automaticamente, os valores do \\(\\hat\\beta_0\\) e \\(\\hat\\beta_1\\) para construir a reta.\n\nggplot(data = notas, aes(x = x1, y = y)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\nn = nrow(y);\njn = j(n,1,1);\nJnn = j(n,n,1);\nIn = I(n);\nX = jn||x1;\ny_barra = (1/n)*Jnn*y;\nTot = y-y_barra;\nSQTotal = t(Tot)*(Tot);\n* pág.135;\nBeta1 = t(x1)*(In-(1/n)*Jnn)*y/(t(x1)*(In-(1/n)*Jnn)*x1);\nBeta0 = (1/n)*t(jn)*(y - Beta1*x1);\nprint 'Estimativas dos parâmetros da reta:' Beta0[format=8.4] Beta1[format=8.4],,,;"
  },
  {
    "objectID": "exemplo3.1.html#sec-estvar",
    "href": "exemplo3.1.html#sec-estvar",
    "title": "\n5  Estimação e Teste de Hipóteses\n",
    "section": "\n5.3 Estimação da variância (\\(\\sigma^2\\))",
    "text": "5.3 Estimação da variância (\\(\\sigma^2\\))\nPara estimar a variância \\(\\sigma^2\\), utilizamos a seguinte expressão:\n\\[\ns^2 =\n\\frac{(\\mathbf{y} - \\mathbf{\\hat{y}})' \\space (\\mathbf{y} - \\mathbf{\\hat{y}})}{n-2} =  \\frac{SQRes}{n-2}\n\\]\nEm que \\(SQRes\\) é a soma de quadrados dos resíduos e \\(\\hat{y}\\), os valores estimados de \\(\\mathbf{y}\\).\n\n\nR\nSAS\n\n\n\n\ny_hat &lt;- Beta0 + Beta1 * x1\ny_hat\n\n [1] 94.50098 77.92069 10.72691 10.72691 78.79334 66.57629 88.39245 51.74130\n [9] 89.26510 91.88304 26.43455 85.77451 10.72691 36.90631 62.21306 77.92069\n[17] 75.30275 69.19423\n\nRes &lt;- y - y_hat\nRes\n\n [1]   0.4990229   2.0793072 -10.7269091 -10.7269091   0.2066607  10.4237122\n [7] -16.3924513  14.2587034   8.7349022  -1.8830375 -26.4345469   9.2254883\n[13]  24.2730909  13.0936946   9.7869449 -22.9206928  -0.3027532  -3.1942274\n\nSQRes &lt;- t(Res) %*% Res\nSQRes\n\n         [,1]\n[1,] 3071.229\n\ns2 &lt;- as.numeric(SQRes / (n - 2))\ns2\n\n[1] 191.9518\n\n\nPara calcular o resíduo padronizado, dividimos o resíduo pelo desvio padrão da variância estimada (\\(\\sqrt{s^2}\\)).\n\nres_pad &lt;- Res / sqrt(s2)\nres_pad\n\n [1]  0.03601839  0.15007989 -0.77424509 -0.77424509  0.01491632  0.75236099\n [7] -1.18317166  1.02916236  0.63046633 -0.13591357 -1.90798839  0.66587578\n[13]  1.75197917  0.94507454  0.70640050 -1.65436600 -0.02185207 -0.23055242\n\n\n\n\n\n\nValores observados(`y`)\nValores estimados(`y_hat`)\nResíduo(`Res`)\nResíduo Padronizado(`res_pad`)\n\n\n\n95\n94.5010\n0.4990\n0.0360\n\n\n80\n77.9207\n2.0793\n0.1501\n\n\n0\n10.7269\n-10.7269\n-0.7742\n\n\n0\n10.7269\n-10.7269\n-0.7742\n\n\n79\n78.7933\n0.2067\n0.0149\n\n\n77\n66.5763\n10.4237\n0.7524\n\n\n72\n88.3925\n-16.3925\n-1.1832\n\n\n66\n51.7413\n14.2587\n1.0292\n\n\n98\n89.2651\n8.7349\n0.6305\n\n\n90\n91.8830\n-1.8830\n-0.1359\n\n\n0\n26.4345\n-26.4345\n-1.9080\n\n\n95\n85.7745\n9.2255\n0.6659\n\n\n35\n10.7269\n24.2731\n1.7520\n\n\n50\n36.9063\n13.0937\n0.9451\n\n\n72\n62.2131\n9.7869\n0.7064\n\n\n55\n77.9207\n-22.9207\n-1.6544\n\n\n75\n75.3028\n-0.3028\n-0.0219\n\n\n66\n69.1942\n-3.1942\n-0.2306\n\n\n\n\n\nVariância dos dados originais e desvio padrão estimado:\n\nvar_y &lt;- (t(y) %*% (In - (1/n) * Jnn) %*% y) / (n - 1)\nvar_y\n\n         [,1]\n[1,] 1055.546\n\ns &lt;- sqrt(s2)\ns\n\n[1] 13.85467\n\n\n\n\n\n\nVariância dos dados originais (`var_y`)\nVariância de y|x (`s2`)\nDesvio padrão de y|x (`s`)\n\n\n1055.546\n191.9518\n13.8547\n\n\n\n\n\n\n\nk = 1;\n\ny_hat = Beta0 + Beta1*x1;\nReg = y_hat-y_barra;\nSQReg = t(Reg)*Reg;\n\nRes = y-y_hat;\nSQRes = t(Res)*Res;\ns2 = SQRes/(n-k-1);\n* s2 = (t(y)*y - t(Beta)*t(X)*y)/(n-k-1);\nres_pad = res/sqrt(s2);\n\n*pág. 141;\nprint 'Valores observados(y) e estimados(y_hat), residuo(res) e residuo padronizado(res_pad):',\n      '--------------------------------------------------------------------------------------'; \nprint y '   ' y_hat [format=8.4] '   ' res [format=8.4]  '   ' res_pad [format=8.4],,,;\n\n\nvar_y = (t(y)*(In - (1/n)*Jnn)*y)/(n-1); * Calcula a variância amostral de y;\ns = sqrt(s2);\n\nprint 'Variância dos dados originais:' var_y [format=10.4],,\n      'Variância de y|x:             ' s2[format=10.4],,\n      'Desvio padrão de y|x :        ' s[format=10.4] ,,,;"
  },
  {
    "objectID": "exemplo3.1.html#teste-de-hipóteses-e-intervalo-de-confiança-para-beta_1",
    "href": "exemplo3.1.html#teste-de-hipóteses-e-intervalo-de-confiança-para-beta_1",
    "title": "\n5  Estimação e Teste de Hipóteses\n",
    "section": "\n5.4 Teste de Hipóteses e Intervalo de Confiança para \\(\\beta_1\\)\n",
    "text": "5.4 Teste de Hipóteses e Intervalo de Confiança para \\(\\beta_1\\)\n\nRealizaremos um teste de hipótese \\(H_0: \\beta_1 = 0\\), com a suposição que \\(y_i \\sim N(\\beta_0+ \\beta_1x_i, \\space \\sigma^2)\\) ou \\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\nDas propriedades de \\(\\hat\\beta_1\\) e \\(s^2\\), a estatística t é dada por:\n\\[\nt = \\frac{\\hat\\beta_1}{\\sqrt{\\frac{s^2}{\\sum^n_{i=1}(x_i-\\bar{x})^2}}} =  \\frac{\\hat\\beta_1}{\\text{Erro padrão}}\n\\]\n\n\nR\nSAS\n\n\n\n\nx_barra &lt;- t(jn) %*% (x1 / n)\nx_barra\n\n         [,1]\n[1,] 58.05556\n\nvar_Beta0 &lt;- s2 * ((1 / n) + (x_barra^2 / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1)))\nvar_Beta0\n\n         [,1]\n[1,] 43.78905\n\nstderr_Beta0 &lt;- sqrt(var_Beta0)\nstderr_Beta0\n\n         [,1]\n[1,] 6.617329\n\nvar_Beta1 &lt;- s2 / (t(x1) %*% (In - (1 / n) * Jnn) %*% x1)\nvar_Beta1\n\n            [,1]\n[1,] 0.009828088\n\nstderr_Beta1 &lt;- sqrt(var_Beta1)\nstderr_Beta1\n\n           [,1]\n[1,] 0.09913671\n\n\nComo valores da estatística t, temos:\n\nt0 &lt;- Beta0 / stderr_Beta0\nt0\n\n         [,1]\n[1,] 1.621033\n\nt1 &lt;- Beta1 / stderr_Beta1\nt1\n\n         [,1]\n[1,] 8.802456\n\n\nPara construir os limites de intervalo de confiança:\n\\[\n\\hat\\beta \\pm t_{\\alpha/2, n-2} \\sqrt{\\frac{s^2}{\\sum^n_{i=1}(x_i-\\bar{x})^2}}\n\\]\nonde \\(t_{(\\alpha/2, n-2)}\\) é o percentil de ordem \\(100(1-\\alpha)\\%\\) da distribuição \\(t\\) com \\(n-2\\) graus de liberdade e \\(\\alpha\\) é o nível de significância do teste.\n\nttab &lt;- qt(0.975, n - 2) # quantil t com probabilidade 0.975 e gl = n-2\nttab\n\n[1] 2.119905\n\nliminf0 &lt;- Beta0 - ttab * stderr_Beta0\nliminf0\n\n          [,1]\n[1,] -3.301202\n\nlimsup0 &lt;- Beta0 + ttab * stderr_Beta0\nlimsup0\n\n         [,1]\n[1,] 24.75502\n\nliminf1 &lt;- Beta1 - ttab * stderr_Beta1\nliminf1\n\n          [,1]\n[1,] 0.6624861\n\nlimsup1 &lt;- Beta1 + ttab * stderr_Beta1\nlimsup1\n\n         [,1]\n[1,] 1.082807\n\n\nDado que o \\(t\\) calculado para \\(\\beta_1\\) é maior que o limite superior, temos evidências para rejeitar \\(H_0: \\beta_1 = 0\\), com nível de significância de 5%.\n\n\n\n\nParâmetro\nEstimado\nVariância\nErro Padrão\nt\nIC 95%\n\n\n\nBo\n10.7269\n43.7890\n6.6173\n1.6210\n[-3.301 , 24.755]\n\n\nB1\n0.8726\n0.0098\n0.0991\n8.8025\n[0.662 , 1.083]\n\n\n\n\n\n\n\n\nx_barra = t(jn)*x1/n;\nvar_Beta0 = s2*(1/n + x_barra**2/(t(x1)*(In-(1/n)*Jnn)*x1));\nstderr_Beta0 = sqrt(var_Beta0);\n\nvar_Beta1 = s2/(t(x1)*(In-(1/n)*Jnn)*x1);\nstderr_Beta1 = sqrt(var_Beta1);\n\n\nttab = tinv(0.975,n-2);\nliminf0 = Beta0-ttab*stderr_Beta0; limsup0 = Beta0+ttab*stderr_Beta0;\nliminf1 = Beta1-ttab*stderr_Beta1; limsup1 = Beta1+ttab*stderr_Beta1;\n*pág.146;\nprint Beta0[format=10.4] var_Beta0[format=10.4] stderr_Beta0[format=10.4]\n      '     I.C.(Beta0,95%) = ' liminf0[format=10.4] limsup0[format=10.4] ,,, \n      Beta1[format=10.4] var_Beta1[format=10.4] stderr_Beta1[format=10.4] \n      '     I.C.(Beta1,95%) = ' liminf1[format=10.4] limsup1[format=10.4] ,,,;"
  },
  {
    "objectID": "exemplo3.1.html#coeficiente-de-determinação-r2",
    "href": "exemplo3.1.html#coeficiente-de-determinação-r2",
    "title": "\n5  Estimação e Teste de Hipóteses\n",
    "section": "\n5.5 Coeficiente de Determinação (\\(R^2\\))",
    "text": "5.5 Coeficiente de Determinação (\\(R^2\\))\nO coeficiente de determinação (\\(R^2\\)) indica a proporção da variação em \\(y\\) que é explicada pelo modelo ou que é devida à regressão em \\(x\\). É definido como:\n\\[R^2 = \\frac{SQReg}{SQTotal} = 1 - \\frac{SQRes}{SQTotal}\\]\nem que:\n\n\\(SQReg\\) é a soma de quadrados da regressão;\n\\(SQRes\\) é a soma de quadrados dos resíduos;\n\\(SQTotal\\) é a soma de quadrados total (\\(SQTotal = SQReg + SQRes\\)).\n\n\n\nR\nSAS\n\n\n\nPara calcular \\(SQReg\\):\n\ny_barra &lt;- (1 / n) * Jnn %*% y\ny_barra\n\n          [,1]\n [1,] 61.38889\n [2,] 61.38889\n [3,] 61.38889\n [4,] 61.38889\n [5,] 61.38889\n [6,] 61.38889\n [7,] 61.38889\n [8,] 61.38889\n [9,] 61.38889\n[10,] 61.38889\n[11,] 61.38889\n[12,] 61.38889\n[13,] 61.38889\n[14,] 61.38889\n[15,] 61.38889\n[16,] 61.38889\n[17,] 61.38889\n[18,] 61.38889\n\nReg &lt;- y_hat - y_barra\nReg\n\n             [,1]\n [1,]  33.1120882\n [2,]  16.5318039\n [3,] -50.6619797\n [4,] -50.6619797\n [5,]  17.4044505\n [6,]   5.1873989\n [7,]  27.0035624\n [8,]  -9.6475923\n [9,]  27.8762089\n[10,]  30.4941486\n[11,] -34.9543420\n[12,]  24.3856228\n[13,] -50.6619797\n[14,] -24.4825835\n[15,]   0.8241662\n[16,]  16.5318039\n[17,]  13.9138643\n[18,]   7.8053385\n\nSQReg &lt;- t(Reg) %*% Reg\nSQReg\n\n         [,1]\n[1,] 14873.05\n\n\nJá \\(SQTotal\\):\n\nTot &lt;- y - y_barra\nTot\n\n            [,1]\n [1,]  33.611111\n [2,]  18.611111\n [3,] -61.388889\n [4,] -61.388889\n [5,]  17.611111\n [6,]  15.611111\n [7,]  10.611111\n [8,]   4.611111\n [9,]  36.611111\n[10,]  28.611111\n[11,] -61.388889\n[12,]  33.611111\n[13,] -26.388889\n[14,] -11.388889\n[15,]  10.611111\n[16,]  -6.388889\n[17,]  13.611111\n[18,]   4.611111\n\nSQTotal &lt;- t(Tot) %*% Tot\nSQTotal\n\n         [,1]\n[1,] 17944.28\n\nSQTotal &lt;- SQReg + SQRes\nSQTotal\n\n         [,1]\n[1,] 17944.28\n\n\nAssim, o coeficiente de determinação é dado por:\n\nR2 &lt;- SQReg / SQTotal\nR2\n\n          [,1]\n[1,] 0.8288463\n\n\nO coeficiente de correlação (\\(r\\)) é dado pela raiz quadrada do coeficiente de determinação (\\(R^2\\)).\n\nr &lt;- sqrt(R2)\nr\n\n        [,1]\n[1,] 0.91041\n\n\n\n\n\n\nCoeficiente de determinação (R2)\nCoeficiente de correlação (r)\n\n\n0.8288\n0.9104\n\n\n\n\nA estatística t para testar \\(H_0: \\beta_1 = 0\\) também pode ser expressa em termos de \\(r\\):\n\\[\nt = \\frac{r \\space \\sqrt{n - 2}}{\\sqrt{1-r^2}}\n\\]\n\ntcalc1 &lt;- r * sqrt(n - 2) / (sqrt(1 - r^2))\ntcalc1\n\n         [,1]\n[1,] 8.802456\n\ntcalc2 &lt;- Beta1 / stderr_Beta1\ntcalc2\n\n         [,1]\n[1,] 8.802456\n\n\nCom a estatística t, calculamos o p-valor da seguinte maneira:\n\np_valor &lt;- 2 * (1 - pt(abs(tcalc1), n - 2))\np_valor\n\n             [,1]\n[1,] 1.570688e-07\n\n\nOnde a função pt() retorna a função de distribuição da estatística t.\nDado que o p-valor é menor que \\(\\alpha = 0,05\\), rejeitamos \\(H_0: \\beta_1 = 0\\).\nPor fim, podemos ter uma visão geral dos resultados da análise de regressão com a função summary(), a partir do modelo criado com a função lm().\n\nmodelo &lt;- lm(y ~ x1)\n\nsummary(modelo)\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.4345  -8.8437   0.3528   9.6466  24.2731 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.72691    6.61733   1.621    0.125    \nx1           0.87265    0.09914   8.802 1.57e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.85 on 16 degrees of freedom\nMultiple R-squared:  0.8288,    Adjusted R-squared:  0.8181 \nF-statistic: 77.48 on 1 and 16 DF,  p-value: 1.571e-07\n\n\n\nplot(modelo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2 = SQReg/SQTotal;  * Coeficiente de determinação - R2';\n\ncorr = sqrt(R2);\n\nprint '     SQTotal  =     SQReg   +    SQRes',,\n       SQTotal[format=12.4] SQReg[format=12.4] SQRes[format=12.4],,,\n      'Coeficiente de determinação (R2): ' R2[format=10.4],,,\n      'Coeficiente de correlação (r):    ' corr[format=10.4],,,;\n\ntcalc1 = Beta1/stderr_Beta1; * Para testar H0: Beta1 = 0;\ntcalc2 = corr*sqrt(n-2)/(sqrt(1-corr**2));\np_valor = 2*(1-cdf('t',abs(tcalc1),n-2));\n\nprint 'H0: Beta1 = 0   ' tcalc1[format=10.4] tcalc2[format=10.4] p_valor[format=10.4];\n\nquit;"
  },
  {
    "objectID": "exemplo4.1.html#estimação-dos-parâmetros-beta",
    "href": "exemplo4.1.html#estimação-dos-parâmetros-beta",
    "title": "6  Estimação",
    "section": "\n6.1 Estimação dos parâmetros \\(\\beta\\)\n",
    "text": "6.1 Estimação dos parâmetros \\(\\beta\\)\n\nPara estimar os parâmetros \\(\\beta\\), utilizamos o Método dos Mínimos Quadrados Ordinários (MQO), com base em uma amostra de \\(n\\) observações \\((y_i, x_{i1},x_{i2},\\dots,x_{ik})\\) para \\(i = 1,2, \\dots , n\\).\nDessa forma, precisamos criar os seguintes objetos:\n\nn: número de observações \\(n\\);\njn: vetor coluna de 1’s \\(\\mathbf{j}\\);\nJnn: matriz de 1’s \\(\\mathbf{J}\\);\nIn: matriz identidade \\(\\mathbf{I}\\).\n\n\n\nR\nSAS\n\n\n\n\nn &lt;- length(y)\nn\n\n[1] 12\n\njn &lt;- matrix(data = 1, nrow = n, ncol = 1)\njn\n\n      [,1]\n [1,]    1\n [2,]    1\n [3,]    1\n [4,]    1\n [5,]    1\n [6,]    1\n [7,]    1\n [8,]    1\n [9,]    1\n[10,]    1\n[11,]    1\n[12,]    1\n\nJnn &lt;- jn %*% t(jn)\nJnn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    1    1    1    1    1    1    1    1    1     1     1     1\n [2,]    1    1    1    1    1    1    1    1    1     1     1     1\n [3,]    1    1    1    1    1    1    1    1    1     1     1     1\n [4,]    1    1    1    1    1    1    1    1    1     1     1     1\n [5,]    1    1    1    1    1    1    1    1    1     1     1     1\n [6,]    1    1    1    1    1    1    1    1    1     1     1     1\n [7,]    1    1    1    1    1    1    1    1    1     1     1     1\n [8,]    1    1    1    1    1    1    1    1    1     1     1     1\n [9,]    1    1    1    1    1    1    1    1    1     1     1     1\n[10,]    1    1    1    1    1    1    1    1    1     1     1     1\n[11,]    1    1    1    1    1    1    1    1    1     1     1     1\n[12,]    1    1    1    1    1    1    1    1    1     1     1     1\n\nIn &lt;- diag(n)\nIn\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    1    0    0    0    0    0    0    0    0     0     0     0\n [2,]    0    1    0    0    0    0    0    0    0     0     0     0\n [3,]    0    0    1    0    0    0    0    0    0     0     0     0\n [4,]    0    0    0    1    0    0    0    0    0     0     0     0\n [5,]    0    0    0    0    1    0    0    0    0     0     0     0\n [6,]    0    0    0    0    0    1    0    0    0     0     0     0\n [7,]    0    0    0    0    0    0    1    0    0     0     0     0\n [8,]    0    0    0    0    0    0    0    1    0     0     0     0\n [9,]    0    0    0    0    0    0    0    0    1     0     0     0\n[10,]    0    0    0    0    0    0    0    0    0     1     0     0\n[11,]    0    0    0    0    0    0    0    0    0     0     1     0\n[12,]    0    0    0    0    0    0    0    0    0     0     0     1\n\n\nVale a ressalva de que o vetor de \\(\\beta_0\\) corresponde ao vetor coluna de 1’s \\(\\mathbf{j}\\) (jn):\n\nx0 &lt;- jn\ncolnames(x0) &lt;- (\"x0\")\n\nx0\n\n      x0\n [1,]  1\n [2,]  1\n [3,]  1\n [4,]  1\n [5,]  1\n [6,]  1\n [7,]  1\n [8,]  1\n [9,]  1\n[10,]  1\n[11,]  1\n[12,]  1\n\n\nEm seguida, criaremos três objetos, cada qual respectivo às possíveis combinações entre as variáveis preditoras. Calcularemos as equações de predição de \\(y\\) sobre:\n\n\n\\(x_1\\) sozinho;\n\n\nX01 &lt;- cbind(x0, x1)\nX01\n\n      x0 x1\n [1,]  1  0\n [2,]  1  2\n [3,]  1  2\n [4,]  1  2\n [5,]  1  4\n [6,]  1  4\n [7,]  1  4\n [8,]  1  6\n [9,]  1  6\n[10,]  1  6\n[11,]  1  8\n[12,]  1  8\n\n\n\n\n\\(x_2\\) sozinho;\n\n\nX02 &lt;- cbind(x0, x2)\nX02\n\n      x0 x2\n [1,]  1  2\n [2,]  1  6\n [3,]  1  7\n [4,]  1  5\n [5,]  1  9\n [6,]  1  8\n [7,]  1  7\n [8,]  1 10\n [9,]  1 11\n[10,]  1  9\n[11,]  1 15\n[12,]  1 13\n\n\n\n\n\\(x_1\\) e \\(x_2\\) em conjunto.\n\n\nX012 &lt;- cbind(x0, x1, x2)\nX012\n\n      x0 x1 x2\n [1,]  1  0  2\n [2,]  1  2  6\n [3,]  1  2  7\n [4,]  1  2  5\n [5,]  1  4  9\n [6,]  1  4  8\n [7,]  1  4  7\n [8,]  1  6 10\n [9,]  1  6 11\n[10,]  1  6  9\n[11,]  1  8 15\n[12,]  1  8 13\n\n\nPara cada caso anterior, unimos os vetores das variáveis preditoras, formando a matriz \\(\\mathbf{X}\\) (X01, X02, X012) do modelo matricial \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\).\nPara obter os etimadores dos \\(\\beta 's\\) de cada um dos três modelos, utilizamos o MQO que minimiza a soma de quadrados dos desvios (\\(\\sum^n_{i=1} (y_i - \\hat{y}_i)^2\\)), onde\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + \\hat{\\beta}_2x_{i2} + \\dots \\hat{\\beta}_kx_{ik}\n\\]\né um estimador de \\(E(y_i)\\).\nAssim, o vetor \\(\\hat {\\boldsymbol{\\beta}} = [\\hat \\beta_0,\\hat \\beta_1,\\dots,\\hat \\beta_k]'\\) que minimiza a soma de quadrados dos desvios é dado por:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X'X})^{-1} \\mathbf{X}' \\mathbf{y}\n\\]\n\nBeta01 &lt;- solve(t(X01) %*% X01) %*% t(X01) %*% y\nBeta01\n\n       [,1]\nx0 1.858491\nx1 1.301887\n\n\n\nBeta02 &lt;- solve(t(X02) %*% X02) %*% t(X02) %*% y\nBeta02\n\n        [,1]\nx0 0.8613139\nx2 0.7810219\n\n\n\nBeta012 &lt;- solve(t(X012) %*% X012) %*% t(X012) %*% y\nBeta012\n\n        [,1]\nx0  5.375394\nx1  3.011830\nx2 -1.285489\n\n\nDessa forma, obtemos as seguintes equações de predição de \\(y\\):\n\\[\n\\begin{align}\n\\hat{y} &= 1,86 + 1,30 x_1 \\\\\n\\hat{y} &= 0,86 + 0,78 x_2\\\\\n\\hat{y} &= 5,38 + 3,01 x_1 - 1,29 x_2\n\\end{align}\n\\]\nComo citado anteriormente, os coeficientes de \\(x_1\\) e \\(x_2\\) diferem de acordo com o modelo proposto.\nComo propriedades dos estimadores de MQO \\(\\hat {\\boldsymbol{\\beta}}\\):\n\nSe \\(E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta}\\) então \\(\\hat{\\boldsymbol{\\beta}}\\) é um estimador não viesado de \\(\\boldsymbol{\\beta}\\);\nSe \\(cov(\\mathbf{y}) = \\sigma^2\\mathbf{I}\\), então \\(cov(\\hat {\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X'X})^{-1}\\).\n\n\n\n\nn = nrow(y);\nIn = I(n);\nJnn = J(n,n,1);\nx0 = j(n,1,1); *cria um vetor nx1 de uns;\n\nx01 = x0||x1;\nx02 = x0||x2;\nx012 = x0||x1||x2;\n\n* Estima vetor Beta nos três modelos;\nBeta01 = inv(t(X01)*X01)*t(X01)*y;\nBeta02 = inv(t(X02)*X02)*t(X02)*y;\nBeta012 = inv(t(X012)*X012)*t(X012)*y;\nprint '  Modelo: y = b0 + b1*x1 + e          =&gt;     Beta = ' Beta01[format=10.4],,,\n      '  Modelo: y = b0 + b2*x2 + e          =&gt;     Beta = ' Beta02[format=10.4],,,\n      '  Modelo: y = b0 + b1*x1 + b2*x2 + e  =&gt;     Beta = ' Beta012[format=10.4],,,,;\nprint Beta01[format=10.4] Beta02[format=10.4] Beta012[format=10.4];"
  },
  {
    "objectID": "exemplo4.1.html#estimação-da-variância-sigma2",
    "href": "exemplo4.1.html#estimação-da-variância-sigma2",
    "title": "6  Estimação",
    "section": "\n6.2 Estimação da variância (\\(\\sigma^2\\))",
    "text": "6.2 Estimação da variância (\\(\\sigma^2\\))\nEstimaremos a variância \\(\\sigma^2\\) do modelo completo por uma média das variâncias das amostras:\n\\[\ns^2 = \\frac{\\mathbf{y'y} - \\hat {\\boldsymbol{\\beta}'}' \\mathbf{X'} \\mathbf{y}}{n-k-1} = \\frac{\\text{SQResíduo}}{n-k-1}\n\\]\nonde \\(n\\) é o tamanho amostral e \\(k\\), o número de variáveis \\(x\\).\n\n\nR\nSAS\n\n\n\nNo objeto k, definimos o número de variáveis \\(x\\) do modelo completo, ou seja, duas variáveis (\\(x_1\\) e \\(x_2\\)). Além disso, anteriormente, já definimos ao objeto n o número de observações (12 observações).\n\nk &lt;- 2\nk\n\n[1] 2\n\nn\n\n[1] 12\n\n\nDessa forma, com a expressão apresentada anteriormente, obtemos a variância estimada \\(s^2\\).\n\ns2 &lt;- as.numeric((t(y) %*% y) - (t(Beta012) %*% t(X012) %*% y)) / (n - k - 1)\ns2\n\n[1] 2.828777\n\n\n\n\n\n\n\n\nNota\n\n\n\nA variância estimada \\(s^2\\) também poderia ser calculada da seguinte maneira, como realizado na Seção 5.3 para o caso da regressão linear simples:\n\nk &lt;- 2\n\ny_hat &lt;- X012 %*% Beta012\ncolnames(y_hat) &lt;- (\"y_hat\")\n\nres &lt;- y - y_hat\n\nSQRes &lt;- t(res) %*% res\n\ns2 &lt;- as.numeric(SQRes / (n - k - 1))\n\n\n\nPara verificar se \\(s^2\\) é um estimador não viesado de \\(cov(\\hat{\\boldsymbol{\\beta}})\\):\n\\[\\widehat{cov(\\hat{\\boldsymbol{\\beta}})} = s^2(\\mathbf{X'X})^{-1}\\]\n\ncov_Beta &lt;- s2 * solve(t(X012) %*% X012)\ncov_Beta\n\n           x0         x1         x2\nx0  2.7573880  0.6871161 -0.6469600\nx1  0.6871161  0.4584492 -0.3145564\nx2 -0.6469600 -0.3145564  0.2364750\n\n\nDessa forma, temos:\n\\[\n\\begin{align}\ns^2 &= 2,829 \\\\\n\\widehat{cov} &=\n\\begin{bmatrix}\n2.757 & 0,687 & -0,647 \\\\\n0,687 & 0,458 & -0,315 \\\\\n-0,647 & -0,315 & 0,236\n\\end{bmatrix}\n\\end{align}\n\\]\n\n\n\n*pág.168;\n* Ajusta modelo com x1 e x2 + vetor de estimativas da resposta + resíduos do modelo;\nX = x0||x1||x2;\nBeta = inv(t(X)*X)*t(X)*y;\ny_hat = X*Beta;\nNome = {'Beta0','Beta1','Beta2'};\nprint Nome Beta[format=10.4] '   ' X '   ' y[format=10.] '   ' y_hat[format=10.4],,;\n\n* Calcula estimativa de sigma2 no modelo inicial;\nres = y-y_hat;\nSQRes = t(res)*res;\ns2 = SQRes/(n-k-1);\ncov_Beta = s2*inv(t(X)*X);\n\nprint s2[format=10.3] cov_Beta[format=10.5];;"
  },
  {
    "objectID": "exemplo4.1.html#modelo-de-regressão-na-forma-centrada",
    "href": "exemplo4.1.html#modelo-de-regressão-na-forma-centrada",
    "title": "6  Estimação",
    "section": "\n6.3 Modelo de Regressão na Forma Centrada",
    "text": "6.3 Modelo de Regressão na Forma Centrada\nO modelo de regressão múltipla pode ser escrito, para cada \\(y_i\\), em termos das variáveis \\(x\\) centradas em suas respectivas médias:\n\\[\n\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots \\beta_kx_{ik} + \\epsilon_i \\\\\n&= \\alpha + \\beta_1(x_{i1} - \\bar{x}_1) + \\beta_2(x_{i2} - \\bar{x}_2) + \\dots + \\beta_k(x_{ik} - \\bar{x}_k) + \\epsilon_i\n\\end{align}\n\\]\nem que \\(\\alpha = \\beta_0 + \\beta_1\\bar{x}_1 + \\beta_2\\bar{x}_2 + \\dots + \\beta_k\\bar{x}_k\\), com \\(\\bar{x}_j = \\frac{(\\sum^n_{i=1} x_{ij})}n\\), \\(j = 1,2,\\dots,k\\).\nNa forma matricial, temos:\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\n\\mathbf{j} & \\mathbf{X}_c\n\\end{bmatrix}\n\\begin{bmatrix}\n\\alpha \\\\ \\boldsymbol{\\beta_1}\n\\end{bmatrix}\n+\n\\boldsymbol{\\epsilon}\n\\]\nonde \\(\\mathbf{j}\\) é o vetor coluna de 1’s, \\(\\boldsymbol{\\beta_1} = [\\boldsymbol{\\beta_1,\\beta_2,\\dots,\\beta_k}]'\\) e\n\\[\n\\mathbf{X}_c = \\left(\\mathbf{I} - \\frac{1}n \\mathbf{J}\\right) \\mathbf{X_1} =\n\\begin{bmatrix}\nx_{11} - \\bar{x}_1 & x_{12} - \\bar{x}_2 & \\dots & x_{1k} - \\bar{x}_k \\\\\nx_{21} - \\bar{x}_1 & x_{22} - \\bar{x}_2 & \\dots & x_{2k} - \\bar{x}_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\\\\nx_{n1} - \\bar{x}_1 & x_{n2} - \\bar{x}_2 & \\dots & x_{nk} - \\bar{x}_k \\\\\n\\end{bmatrix}\n\\]\nem que \\(\\mathbf{X_1} = \\begin{bmatrix}\\mathbf{x_1} & \\mathbf{x_2} & \\dots & \\mathbf{x_k}\\end{bmatrix}\\).\nOs estimadores de MQO de \\(\\alpha\\) e \\(\\boldsymbol{\\beta_1}\\) são dados por:\n\\[\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\ \\hat{\\boldsymbol{\\beta_1}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\bar{y} \\\\ (\\mathbf{X'_c X_c})^{-1} \\mathbf{X'_c} \\mathbf{y}\n\\end{bmatrix}\n\\]\nNa forma centrada, o vetor ajustado \\(\\hat{\\mathbf{y}}\\) fica:\n\\[\n\\hat{\\mathbf{y}} = \\hat{\\alpha}\\mathbf{j} + \\hat{\\beta_1} (\\mathbf{x_1} - \\bar{\\mathbf{x}}_1) + \\hat{\\beta_2} (\\mathbf{x_2} - \\bar{\\mathbf{x}}_2) + \\dots + \\hat{\\beta_k} (\\mathbf{x_k} - \\bar{\\mathbf{x}}_k)\n\\]\n\n\nR\nSAS\n\n\n\nObtendo a matriz \\(\\mathbf{X_1} = \\begin{bmatrix}\\mathbf{x_1} & \\mathbf{x_2} & \\dots & \\mathbf{x_k}\\end{bmatrix}\\):\n\nx1x2 &lt;- cbind(x1, x2)\nx1x2\n\n      x1 x2\n [1,]  0  2\n [2,]  2  6\n [3,]  2  7\n [4,]  2  5\n [5,]  4  9\n [6,]  4  8\n [7,]  4  7\n [8,]  6 10\n [9,]  6 11\n[10,]  6  9\n[11,]  8 15\n[12,]  8 13\n\n\nEm seguida, obtendo os valores dos \\(x\\) centrados a partir da expressão \\(\\mathbf{X_c} = \\left(\\mathbf{I} - \\frac{1}n \\mathbf{J}\\right) \\mathbf{X_1}\\):\n\nx1x2c &lt;- (In - (1 / n) * Jnn) %*% x1x2\nx1x2c\n\n              x1   x2\n [1,] -4.3333333 -6.5\n [2,] -2.3333333 -2.5\n [3,] -2.3333333 -1.5\n [4,] -2.3333333 -3.5\n [5,] -0.3333333  0.5\n [6,] -0.3333333 -0.5\n [7,] -0.3333333 -1.5\n [8,]  1.6666667  1.5\n [9,]  1.6666667  2.5\n[10,]  1.6666667  0.5\n[11,]  3.6666667  6.5\n[12,]  3.6666667  4.5\n\nXc &lt;- cbind(x0, x1x2c)\nXc\n\n      x0         x1   x2\n [1,]  1 -4.3333333 -6.5\n [2,]  1 -2.3333333 -2.5\n [3,]  1 -2.3333333 -1.5\n [4,]  1 -2.3333333 -3.5\n [5,]  1 -0.3333333  0.5\n [6,]  1 -0.3333333 -0.5\n [7,]  1 -0.3333333 -1.5\n [8,]  1  1.6666667  1.5\n [9,]  1  1.6666667  2.5\n[10,]  1  1.6666667  0.5\n[11,]  1  3.6666667  6.5\n[12,]  1  3.6666667  4.5\n\n\nCom a matriz \\(\\mathbf{X_c}\\), obtemos as estimativas dos coeficientes de regressão \\(\\beta\\) com os \\(x\\) centrados a partir da seguinte expressão:\n\\[\n\\hat{\\boldsymbol{\\beta_1}} = (\\mathbf{X'_c X_c})^{-1} \\mathbf{X'_c} \\mathbf{y}\n\\]\n\nBetac &lt;- solve(t(Xc) %*% Xc) %*% t(Xc) %*% y\nBetac\n\n        [,1]\nx0  7.500000\nx1  3.011830\nx2 -1.285489\n\n\nPara calcular a estimativa de \\(\\sigma^2\\) no modelo centrado, utilizamos a mesma expressão do modelo original:\n\\[\ns^2 = \\frac{\\mathbf{y'y} - \\hat {\\boldsymbol{\\beta_1}}' \\mathbf{X_c'} \\mathbf{y}}{n-k-1} = \\frac{\\text{SQResíduo}}{n-k-1}\n\\]\n\ns2c &lt;- as.numeric((t(y) %*% y) - (t(Betac) %*% t(Xc) %*% y)) / (n - k - 1)\ns2c\n\n[1] 2.828777\n\n\n\n\n\n\n\n\nNota\n\n\n\nMais uma vez, a variância estimada \\(s^2\\) também poderia ser calculada da seguinte maneira, como realizado na Seção 5.3 para o caso da regressão linear simples:\n\ny_hatc &lt;- Xc %*% Betac\ncolnames(y_hatc) &lt;- (\"y_hatc\")\n\nres_c &lt;- y - y_hatc\n\nSQRes_c &lt;- t(res_c) %*% res_c\n\ns2c &lt;- as.numeric(SQRes / (n - k - 1))\n\n\n\nNovamente, para verificar se \\(s^2\\) é um estimador não viesado de \\(cov(\\hat{\\boldsymbol{\\beta}})\\):\n\\[\\widehat{cov(\\hat{\\boldsymbol{\\beta}})} = s^2(\\mathbf{X'X})^{-1}\\]\n\ncov_Betac &lt;- s2c * solve(t(X012) %*% X012)\ncov_Betac\n\n           x0         x1         x2\nx0  2.7573880  0.6871161 -0.6469600\nx1  0.6871161  0.4584492 -0.3145564\nx2 -0.6469600 -0.3145564  0.2364750\n\n\nDessa forma, temos o mesmo resultado do modelo original:\n\\[\n\\begin{align}\ns^2 &= 2,829 \\\\\n\\widehat{cov} &=\n\\begin{bmatrix}\n2.757 & 0,687 & -0,647 \\\\\n0,687 & 0,458 & -0,315 \\\\\n-0,647 & -0,315 & 0,236\n\\end{bmatrix}\n\\end{align}\n\\]\n\n\n\nx1x2 = x1||x2;\nx1x2c = (In - (1/n)*Jnn)*x1x2;\nXc = x0||x1x2c; \nBetaC = inv(t(Xc)*Xc)*t(Xc)*y;\ny_hatc = Xc*Betac;\nXcLXc = t(Xc)*Xc;\n\n*pág. 175;\nk = 2;\nres = y - y_hat;\nSQRes = t(res)*res;\ns2c = (1/(n-k-1))*SQRes;\ncov_Betac = s2*inv(t(X)*X);\nNome = {'Beta0c','Beta1c','Beta2c'};\nprint 'Estimativas dos parâmetros com colunas x1 e x2 centradas nas médias';\nprint Nome Betac[format=10.4] '   ' Xc[format=10.4] '   ' y '   ' y_hatC[format=10.4],;\nprint s2c[format=10.5] cov_Betac[format=10.5];"
  },
  {
    "objectID": "exemplo4.1.html#coeficiente-de-determinação-na-regressão-com-x-fixos",
    "href": "exemplo4.1.html#coeficiente-de-determinação-na-regressão-com-x-fixos",
    "title": "6  Estimação",
    "section": "\n6.4 Coeficiente de Determinação na Regressão com x-fixos",
    "text": "6.4 Coeficiente de Determinação na Regressão com x-fixos\nPara calcular o coeficiente de determinação \\(R^2\\) com x-fixos:\n\\[\nR^2 = \\frac{\\mathbf{y'}\\left[\\mathbf{X(X'X)}^{-1}\\mathbf{X'} - \\frac{1}n \\mathbf{J}\\right]\\mathbf{y}}{\\mathbf{y'}[\\mathbf{I} - \\frac{1}n \\mathbf{J}]\\mathbf{y}}\n=\n\\frac{\\text{SQReg}}{\\text{SQTot}}\n\\]\nem que \\(SQReg\\) é a soma de quadrados da regressão com \\(k\\) graus de liberdade, associada somente ao efeito das variáveis regressoras \\(x\\); e \\(SQTot\\), a soma de quadrados total corrigida com \\(n - 1\\) graus de liberdade.\n\n\nR\nSAS\n\n\n\n\nSQReg &lt;- t(y) %*% (Xc %*% solve(t(Xc) %*% Xc) %*% t(Xc) - (1/n) * Jnn) %*% y\nSQReg\n\n        [,1]\n[1,] 139.541\n\nSQTot &lt;- t(y) %*% (In - (1/n) * Jnn) %*% y\nSQTot\n\n     [,1]\n[1,]  165\n\nR2 &lt;- SQReg / SQTot\nR2\n\n          [,1]\n[1,] 0.8457031\n\n\nA partir do coeficiente de determinação \\(R^2\\), podemos calcular o coeficiente de determinação ajustado (\\(R^2 \\text{ ajustado}\\)). Esta medida penaliza a inclusão de regressores ao modelo, visto que a inserção de inúmeras variáveis, mesmo que tenham pouco poder preditivo, aumentam o valor do \\(R^2\\). O modelo proposto é dado por:\n\\[\nR^2_{aj} = \\frac{(n - 1) R^2-k}{n - k - 1}\n\\]\nem que \\(n\\) é o número de observações e \\(k\\), o número de variáveis \\(x\\).\n\nR2aj &lt;- ((n - 1) * R2 - k) / (n - k - 1)\nR2aj\n\n          [,1]\n[1,] 0.8114149\n\n\nComparativamente, percebe-se que o \\(R^2_{aj}\\) é mais rigoroso em relação ao \\(R^2\\).\n\\[\n\\begin{align}\nR^2 &= 0,8457 \\\\\nR^2_{aj} &= 0,8114\n\\end{align}\n\\]\nTodavia, para esta situação, ambos indicam uma boa qualidade no ajuste do modelo.\n\n\n\n*pág.195;\n* Calcula coeficiente de determinação;\nSQreg = t(y)*(X*inv(t(X)*X)*t(X)-(1/n)*Jnn)*y;\nSQTot = t(y)*(In-(1/n)*Jnn)*y;\nR2 = SQReg/SQTot;\nR2aj = ((n-1)*R2-k)/(n-k-1);\nprint R2[format=10.4] R2aj[format=10.4];"
  },
  {
    "objectID": "exemplo4.1.html#resumo-dos-resultados",
    "href": "exemplo4.1.html#resumo-dos-resultados",
    "title": "6  Estimação",
    "section": "\n6.5 Resumo dos resultados",
    "text": "6.5 Resumo dos resultados\nNota-se que o modelo original e o na forma centrada obtiveram diferentes coeficientes estimados.\n\n\n\n\n\nBeta\nBeta Centrado\n\n\n\nx0\n5.375394\n7.500000\n\n\nx1\n3.011830\n3.011830\n\n\nx2\n-1.285489\n-1.285489\n\n\n\n\n\nApesar disso, os estimadores de \\(\\sigma^2\\) de ambos os modelos são os mesmos, bem como \\(cov(\\hat\\beta)\\) e os valores estimados \\(\\mathbf{\\hat y}\\).\n\n\n\n\ns2\ns2 centrado\n\n\n2.828777\n2.828777\n\n\n\n\n\n\nCov_Beta:\n\n\n        x0      x1      x2\nx0  2.7574  0.6871 -0.6470\nx1  0.6871  0.4584 -0.3146\nx2 -0.6470 -0.3146  0.2365\n\n\nCov_Beta Centrado:\n\n\n        x0      x1      x2\nx0  2.7574  0.6871 -0.6470\nx1  0.6871  0.4584 -0.3146\nx2 -0.6470 -0.3146  0.2365\n\n\n\n\n\n\ny_hat\ny_hat centrado\n\n\n\n2.804\n2.804\n\n\n3.686\n3.686\n\n\n2.401\n2.401\n\n\n4.972\n4.972\n\n\n5.853\n5.853\n\n\n7.139\n7.139\n\n\n8.424\n8.424\n\n\n10.591\n10.591\n\n\n9.306\n9.306\n\n\n11.877\n11.877\n\n\n10.188\n10.188\n\n\n12.759\n12.759"
  },
  {
    "objectID": "exemplo4.2.html#teste-de-regressão-global-geral",
    "href": "exemplo4.2.html#teste-de-regressão-global-geral",
    "title": "\n7  Teste de Hipótese - Exemplo 1\n",
    "section": "\n7.1 Teste de Regressão Global (Geral)",
    "text": "7.1 Teste de Regressão Global (Geral)\nComo hipóteses do teste de regressão global, assumiremos:\n\\[\n\\begin{align}\nH_0&: \\boldsymbol{\\beta_1} = 0 \\\\\nH_a&: \\text{Pelo menos um dos } \\beta's \\text{ não é nulo}\n\\end{align}\n\\]\nonde \\(\\boldsymbol{\\beta_1} = [\\beta_1, \\dots, \\beta_k]'\\).\nEm outras palavras:\n\\[\n\\begin{align}\nH_0&: \\text{Nenhum dos x's prediz y} \\\\\nH_a&: \\text{Pelo menos uma das variáveis x's é importante na predição de y}\n\\end{align}\n\\]\nO quadro da ANOVA para o teste F de \\(H_0: \\boldsymbol{\\beta_1} = 0\\) é:\n\n\n\n\nFV\ngl\nQM\nF\n\n\n\nDevida a Beta1\nk\nSQReg / k\nQMReg / QMRes\n\n\nResíduo\nn - k - 1\nSQRes / (n - k - 1)\n\n\n\nTotal\nn - 1\nSQTot / (n - 1)\n\n\n\n\n\n\nonde \\(n\\) são os números de observações e \\(k\\), o número de variáveis \\(x\\).\n\n\nR\nSAS\n\n\n\nA soma de quadrados e os graus de liberdade totais são obtidos da seguinte maneira:\n\nSQTot &lt;- t(y) %*% (In - (1 / n) * Jnn) %*% y\nSQTot\n\n     [,1]\n[1,]  165\n\ngl_tot &lt;- n - 1\ngl_tot\n\n[1] 11\n\n\nJá a soma de quadrados, quadrado médio e os graus de liberdade da regressão:\n\nBeta1 &lt;- solve(t(Xc) %*% Xc) %*% t(Xc) %*% y\nBeta1\n\n        [,1]\nx1  3.011830\nx2 -1.285489\n\nSQReg &lt;- t(Beta1) %*% t(Xc) %*% y\nSQReg\n\n        [,1]\n[1,] 139.541\n\ngl_reg &lt;- k\ngl_reg\n\n[1] 2\n\nQMReg &lt;- SQReg / gl_reg\nQMReg\n\n        [,1]\n[1,] 69.7705\n\n\nPara os resíduos, temos as seguintes soma de quadrados, quadrado médio e graus de liberdade:\n\nSQRes &lt;- SQTot - SQReg\nSQRes\n\n         [,1]\n[1,] 25.45899\n\ngl_res &lt;- n - k - 1\ngl_res\n\n[1] 9\n\nQMRes &lt;- SQRes / gl_res\nQMRes\n\n         [,1]\n[1,] 2.828777\n\n\nA estatística F e o p-valor:\n\nFcalc1 &lt;- QMReg / QMRes\nFcalc1\n\n         [,1]\n[1,] 24.66455\n\np_valor1 &lt;- 1 - pf(Fcalc1, k, n - k - 1)\np_valor1\n\n             [,1]\n[1,] 0.0002226422\n\n\nDessa forma, o quadro da ANOVA para o teste \\(H_0: \\boldsymbol{\\beta_1} = 0\\) fica:\n\n\n\nTeste de Regressão Global - H0: Beta1 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nRegressão\n2\n139.541\n69.771\n24.665\n4.26\n0.000223\n\n\nResíduo\n9\n25.459\n2.829\n\n\n\n\n\nTotal\n11\n165.000\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 2, 9)\\), há evidência para rejeitarmos \\(H_0: \\boldsymbol{\\beta_1} = 0\\) ao nível de 5% de significância, ou seja, pelo menos uma das variáveis regressoras, \\(x_1\\) ou \\(x_2\\), é importante para predizer \\(y\\). Da mesma maneira, o p-valor corrobora com o resultado, pois é menor que 0,05.\n\n\n\n* Cálculo das SQ´s para testar hipótese H0: Beta1 = 0;\nSQTotal = t(y)*(In - (1/n)*Jnn)*y;  * Calcula SQTotal;\ngl_total = n-1;\n\nBeta1 = inv(t(Xc)*Xc)*t(Xc)*y;      * Calcula Beta1;\nSQReg = t(Beta1)*t(Xc)*y;\ngl_reg = k;\nQMReg = SQReg/gl_reg;\n\nSQRes = SQTotal - SQReg;\ngl_res = n-k-1;\nQMRes = SQRes/gl_res;\n\nFcalc1 = QMReg/QMRes;\np_valor1 = 1-cdf('F',Fcalc1,k,n-k-1);\n\nprint 'Seção 8.1: TESTE DE REGRESSÃO GLOBAL - H0: Beta1 = 0',;\nprint 'Regressão' gl_reg   SQReg[format=10.4] QMReg[format=10.4] Fcalc1[format=10.3] p_valor1[format=10.6],,\n      'Resíduo  ' gl_res   SQRes[format=10.4] QMRes[format=10.4],,\n      'Total    ' gl_total SQTotal[format=10.4],,,,;\n\n\n\n\n\n7.1.1 Teste para um subconjunto de \\(\\beta's\\)\n\nNeste caso, estamos interessados em testar a hipótese de que um subconjunto das variáveis regressoras \\(x's\\) não é importante para predizer \\(y\\).\nComo hipóteses do teste de regressão, assumiremos:\n\\[\n\\begin{align}\nH_0&: \\boldsymbol{\\beta_2} = 0 \\\\\nH_a&: \\boldsymbol{\\beta_2} \\ne 0\n\\end{align}\n\\]\nO modelo completo, que inclui todas as variáveis, pode ser escrito como:\n\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n=\n\\begin{bmatrix}\n\\mathbf{X_1} & \\mathbf{X_2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{\\beta_1} \\\\ \\boldsymbol{\\beta_2}\n\\end{bmatrix}\n+\n\\boldsymbol{\\epsilon}\n=\n\\mathbf{X_1} \\boldsymbol{\\beta_1} + \\mathbf{X}_2 \\boldsymbol{\\beta_2} + \\boldsymbol{\\epsilon}\n\\]\nonde \\(\\boldsymbol{\\beta_2}\\) apresenta os parâmetros a serem testados em \\(H_0: \\boldsymbol{\\beta_2} = 0\\).\nJá o modelo reduzido pela hipótese \\(H_0: \\boldsymbol{\\beta_2} = 0\\) será:\n\\[\n\\mathbf{y} = \\mathbf{X_1} \\boldsymbol{\\beta_1}^* + \\boldsymbol{\\epsilon}^*\n\\]\nSeja \\(h\\) o número de parâmetros em \\(\\boldsymbol{\\beta_2}\\), temos:\n\n\\(\\mathbf{X_2}\\) uma matriz \\(n \\times h\\) e \\(\\boldsymbol{\\beta_2}\\) \\(h \\times 1\\);\n\\(\\mathbf{X_1}\\) uma matriz \\(n \\times (k+1-h)\\) e \\(\\boldsymbol{\\beta_1}\\) \\((k+1-h) \\times 1\\).\n\n\n\nR\nSAS\n\n\n\nNo caso do nosso exemplo, sob a hipótese \\(H_0: \\boldsymbol{\\beta_2} = 0\\), os vetores de parâmetros ficam:\n\\[\n\\boldsymbol{\\beta_1} =\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1\n\\end{bmatrix}\n\\text{ e }\n\\boldsymbol{\\beta_2} = \\beta_2\n\\]\n\nX01 &lt;- cbind(x0, x1)  # Variáveis importantes em X1Beta1\nX01\n\n      x0 x1\n [1,]  1  0\n [2,]  1  2\n [3,]  1  2\n [4,]  1  2\n [5,]  1  4\n [6,]  1  4\n [7,]  1  4\n [8,]  1  6\n [9,]  1  6\n[10,]  1  6\n[11,]  1  8\n[12,]  1  8\n\nX2 &lt;- as.matrix(x2)   # Variáveis desprezíveis em X2Beta2\nX2\n\n      [,1]\n [1,]    2\n [2,]    6\n [3,]    7\n [4,]    5\n [5,]    9\n [6,]    8\n [7,]    7\n [8,]   10\n [9,]   11\n[10,]    9\n[11,]   15\n[12,]   13\n\n\nPara desenvolver uma estatística para testar \\(H_0: \\boldsymbol{\\beta_2} = 0\\), precisamos escrever as somas de quadrados em termos de formas quadráticas de \\(\\mathbf{y}\\), dado por:\n\\[\nSQ(\\boldsymbol{\\beta_2} \\mid \\beta_1) = \\mathbf{y'} [\\mathbf{A_1-A_2}] \\mathbf{y}\n\\]\nonde \\(A_1 = \\mathbf{X(X'X)^{-1}X'}\\) e \\(A_2 = \\mathbf{X_1(X'_1X_1)^{-1}X'_1}\\).\n\nA1 &lt;- X %*% solve(t(X) %*% X) %*% t(X) \nA1\n\n             [,1]         [,2]        [,3]        [,4]         [,5]       [,6]\n [1,]  0.39432177  0.189274448  0.12776025  0.25078864  0.045741325 0.10725552\n [2,]  0.18927445  0.190851735  0.24132492  0.14037855  0.141955836 0.09148265\n [3,]  0.12776025  0.241324921  0.37539432  0.10725552  0.220820189 0.08675079\n [4,]  0.25078864  0.140378549  0.10725552  0.17350158  0.063091483 0.09621451\n [5,]  0.04574132  0.141955836  0.22082019  0.06309148  0.159305994 0.08044164\n [6,]  0.10725552  0.091482650  0.08675079  0.09621451  0.080441640 0.08517350\n [7,]  0.16876972  0.041009464 -0.04731861  0.12933754  0.001577287 0.08990536\n [8,]  0.02523659 -0.007886435 -0.06782334  0.05205047  0.018927445 0.07886435\n [9,] -0.03627760  0.042586751  0.06624606  0.01892744  0.097791798 0.07413249\n[10,]  0.08675079 -0.058359621 -0.20189274  0.08517350 -0.059936909 0.08359621\n[11,] -0.24132492  0.044164038  0.17981073 -0.09148265  0.194006309 0.05835962\n[12,] -0.11829653 -0.056782334 -0.08832808 -0.02523659  0.036277603 0.06782334\n              [,7]         [,8]        [,9]       [,10]       [,11]       [,12]\n [1,]  0.168769716  0.025236593 -0.03627760  0.08675079 -0.24132492 -0.11829653\n [2,]  0.041009464 -0.007886435  0.04258675 -0.05835962  0.04416404 -0.05678233\n [3,] -0.047318612 -0.067823344  0.06624606 -0.20189274  0.17981073 -0.08832808\n [4,]  0.129337539  0.052050473  0.01892744  0.08517350 -0.09148265 -0.02523659\n [5,]  0.001577287  0.018927445  0.09779180 -0.05993691  0.19400631  0.03627760\n [6,]  0.089905363  0.078864353  0.07413249  0.08359621  0.05835962  0.06782334\n [7,]  0.178233438  0.138801262  0.05047319  0.22712934 -0.07728707  0.09936909\n [8,]  0.138801262  0.165615142  0.10567823  0.22555205  0.07255521  0.19242902\n [9,]  0.050473186  0.105678233  0.12933754  0.08201893  0.20820189  0.16088328\n[10,]  0.227129338  0.225552050  0.08201893  0.36908517 -0.06309148  0.22397476\n[11,] -0.077287066  0.072555205  0.20820189 -0.06309148  0.49369085  0.22239748\n[12,]  0.099369085  0.192429022  0.16088328  0.22397476  0.22239748  0.28548896\n\nA2 &lt;- X01 %*% solve(t(X01) %*% X01) %*% t(X01)\nA2\n\n             [,1]        [,2]        [,3]        [,4]       [,5]       [,6]\n [1,]  0.34905660  0.22641509  0.22641509  0.22641509 0.10377358 0.10377358\n [2,]  0.22641509  0.16037736  0.16037736  0.16037736 0.09433962 0.09433962\n [3,]  0.22641509  0.16037736  0.16037736  0.16037736 0.09433962 0.09433962\n [4,]  0.22641509  0.16037736  0.16037736  0.16037736 0.09433962 0.09433962\n [5,]  0.10377358  0.09433962  0.09433962  0.09433962 0.08490566 0.08490566\n [6,]  0.10377358  0.09433962  0.09433962  0.09433962 0.08490566 0.08490566\n [7,]  0.10377358  0.09433962  0.09433962  0.09433962 0.08490566 0.08490566\n [8,] -0.01886792  0.02830189  0.02830189  0.02830189 0.07547170 0.07547170\n [9,] -0.01886792  0.02830189  0.02830189  0.02830189 0.07547170 0.07547170\n[10,] -0.01886792  0.02830189  0.02830189  0.02830189 0.07547170 0.07547170\n[11,] -0.14150943 -0.03773585 -0.03773585 -0.03773585 0.06603774 0.06603774\n[12,] -0.14150943 -0.03773585 -0.03773585 -0.03773585 0.06603774 0.06603774\n            [,7]        [,8]        [,9]       [,10]       [,11]       [,12]\n [1,] 0.10377358 -0.01886792 -0.01886792 -0.01886792 -0.14150943 -0.14150943\n [2,] 0.09433962  0.02830189  0.02830189  0.02830189 -0.03773585 -0.03773585\n [3,] 0.09433962  0.02830189  0.02830189  0.02830189 -0.03773585 -0.03773585\n [4,] 0.09433962  0.02830189  0.02830189  0.02830189 -0.03773585 -0.03773585\n [5,] 0.08490566  0.07547170  0.07547170  0.07547170  0.06603774  0.06603774\n [6,] 0.08490566  0.07547170  0.07547170  0.07547170  0.06603774  0.06603774\n [7,] 0.08490566  0.07547170  0.07547170  0.07547170  0.06603774  0.06603774\n [8,] 0.07547170  0.12264151  0.12264151  0.12264151  0.16981132  0.16981132\n [9,] 0.07547170  0.12264151  0.12264151  0.12264151  0.16981132  0.16981132\n[10,] 0.07547170  0.12264151  0.12264151  0.12264151  0.16981132  0.16981132\n[11,] 0.06603774  0.16981132  0.16981132  0.16981132  0.27358491  0.27358491\n[12,] 0.06603774  0.16981132  0.16981132  0.16981132  0.27358491  0.27358491\n\nSQB2_B1 &lt;- t(y) %*% (A1 - A2) %*% y\nSQB2_B1\n\n         [,1]\n[1,] 19.76742\n\n\nTendo a soma de quadrados \\(SQ(\\boldsymbol{\\beta_2} \\mid \\beta_1)\\), podemos calcular o seu quadrado médio (\\(QM(\\boldsymbol{\\beta_2} \\mid \\beta_1)\\)), bem como a estatística F e o p-valor.\n\nh &lt;- ncol(X2)         # Número de parâmetros x em Beta2 (g.l de Beta2|Beta1)\nh\n\n[1] 1\n\nQMB2_B1 &lt;- SQB2_B1 / h\nQMB2_B1\n\n         [,1]\n[1,] 19.76742\n\nFcalc2 &lt;- QMB2_B1 / QMRes\nFcalc2\n\n         [,1]\n[1,] 6.987976\n\np_valor2 &lt;- 1 - pf(Fcalc2, h, n - k - 1)\np_valor2\n\n           [,1]\n[1,] 0.02676076\n\n\nDessa forma, o quadro da ANOVA para o teste \\(H_0: \\boldsymbol{\\beta_2} = 0\\) fica:\n\n\n\nTeste de Regressão de Subconjunto - H0: Beta2 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nDevida a Beta2 ajust. Beta1\n1\n19.767\n19.767\n6.988\n5.12\n0.026761\n\n\nResíduo\n9\n25.459\n2.829\n\n\n\n\n\nTotal\n11\n165.000\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 1, 9)\\), há evidência para rejeitarmos \\(H_0: \\boldsymbol{\\beta_2} = 0\\) ao nível de 5% de significância, ou seja, os termos de segunda ordem (em \\(\\boldsymbol{\\beta_2}\\)) não são todos nulos e são importantes para a predição de \\(\\mathbf{y}\\).\n\n\n\n* SQ´s para testar hipótese H0: Beta2 = 0;\nX1 = X[,1:2];   * Variáveis importantes em X1Beta1;\nX2 = X[,3];     * Variáveis desprezíveis em X2Beta2; \nA1 = X*inv(t(X)*X)*t(X);\nA2 = X1*inv(t(X1)*X1)*t(X1);\nh = ncol(X2);\nSQB2_B1 = t(y)*(A1-A2)*y;\nQMB2_B1 = SQB2_B1/h;\n\nFcalc2 = QMB2_B1/QMRes;\np_valor2 = 1-cdf('F',Fcalc2,h,n-k-1);\n\nprint 'Seção 8.2: TESTE PARA SUBCONJUNTO DOS BETA´S - H0: Beta2 = 0',\n      '                (MODELO COMPLETO versus MODELO REDUZIDO)',;\nprint 'Beta2 | Beta1' h SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc2[format=10.4] p_valor2[format=10.4],\n      'Resíduo      ' gl_res SQRes[format=10.4] QMRes[format=10.4],,,,;"
  },
  {
    "objectID": "exemplo4.2.html#sec-hlg",
    "href": "exemplo4.2.html#sec-hlg",
    "title": "\n7  Teste de Hipótese - Exemplo 1\n",
    "section": "\n7.2 Hipótese Linear Geral",
    "text": "7.2 Hipótese Linear Geral\nNa hipótese linear geral, assumimos a hipótese \\(H_0: \\mathbf{C} \\boldsymbol{\\beta} = 0\\), onde \\(\\mathbf{C}\\) é uma matriz de coeficientes \\(q \\times (k+1)\\) e de posto \\(q \\le k+1\\).\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C} \\boldsymbol{\\beta} = 0 \\\\\nH_a &: \\mathbf{C} \\boldsymbol{\\beta} \\ne 0\n\\end{align}\n\\]\nDessa maneira, podemos expressar qualquer tipo de hipótese, de acordo com os coeficientes presentes na matriz \\(\\mathbf{C}\\).\nPara o exemplo até então utilizado, podemos formular a hipótese \\(H_0: \\boldsymbol{\\beta_1} = \\boldsymbol{\\beta_2} = 0\\) usando a hipótese linear geral.\n\\[\n\\begin{align}\nH_0 &: \\boldsymbol{\\beta_1} = \\boldsymbol{\\beta_2} = 0 \\\\\nH_0 &: \\mathbf{C} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &:\n\\begin{bmatrix}\n\\beta_1 \\\\ \\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC &lt;- matrix(data = c(0, 1, 0, 0, 0, 1), ncol = 3, byrow = TRUE)\nC\n\n     [,1] [,2] [,3]\n[1,]    0    1    0\n[2,]    0    0    1\n\nBeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nBeta\n\n        [,1]\nx0  5.375394\nx1  3.011830\nx2 -1.285489\n\nCBeta &lt;- C %*% Beta\nCBeta\n\n          [,1]\n[1,]  3.011830\n[2,] -1.285489\n\n\nA soma de quadrados da hipótese (\\(SQHip\\)) é dado por:\n\\[\nSQHip = (\\mathbf{C} \\hat{\\boldsymbol{\\beta}})'[\\mathbf{C (X'X)^{-1} C'}]^{-1} (\\mathbf{C} \\hat{\\boldsymbol{\\beta}})\n\\]\ncom \\(q = \\text{ posto}(\\mathbf{C}) = \\text{número linhas de }\\mathbf{C}\\) graus de liberdade.\n\nSQHip &lt;- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta\nSQHip\n\n        [,1]\n[1,] 139.541\n\ngl_hip &lt;- nrow(C)\ngl_hip\n\n[1] 2\n\nQMHip &lt;- SQHip / gl_hip\nQMHip\n\n        [,1]\n[1,] 69.7705\n\nFcalc3 &lt;- QMHip / QMRes\nFcalc3\n\n         [,1]\n[1,] 24.66455\n\np_valor3 &lt;- 1 - pf(Fcalc3, gl_hip, n - k - 1)\np_valor3\n\n             [,1]\n[1,] 0.0002226422\n\n\nDessa forma, o quadro da ANOVA para o teste \\(H_0: \\boldsymbol{\\beta_1} = \\boldsymbol{\\beta_2} = 0\\) utilizando a hipótese linear geral, fica:\n\n\n\nTeste de Regressão para hipótese linear geral - H0: Beta1 = Beta2 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0:B1=B2=0\n2\n139.541\n69.771\n24.665\n4.26\n0.000223\n\n\nResíduo\n9\n25.459\n2.829\n\n\n\n\n\nTotal\n11\n165.000\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 2, 9)\\), há evidência para rejeitarmos \\(H_0: \\boldsymbol{\\beta_1} = \\boldsymbol{\\beta_2} = 0\\) ao nível de 5% de significância.\n\n\n\n*Hipótese Linear Geral H0: C*Beta = 0 ou H0: beta1=beta2=0;\nC = {0 1 0, 0 0 1};\ngl_hip = nrow(C);\nBeta = inv(t(X)*X)*t(X)*y;\nSQHip = t(C*Beta)*inv(C*inv(t(X)*X)*t(C))*C*Beta;\nQMHip = SQHip/gl_hip;\nFcalc3 = QMHip/QMRes;\np_valor3 = 1-cdf('F',Fcalc3,gl_hip,n-k-1);\n\nprint 'Seção 8.3: H0: B1 = B2 = 0 usando HIPÓTESE LINEAR GERAL',;\nprint 'H0: B1 = B2 = 0   ' gl_hip SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.3] p_valor3[format=10.6],,\n      'Resíduo           ' gl_res SQRes[format=10.4] QMRes[format=10.4],,,,;"
  },
  {
    "objectID": "exemplo4.3.html#modelo-completo",
    "href": "exemplo4.3.html#modelo-completo",
    "title": "8  Teste de Hipótese - Exemplo 2",
    "section": "\n8.1 Modelo Completo",
    "text": "8.1 Modelo Completo\nPara estimar os parâmetros \\(\\beta\\) do modelo completo, utilizamos o Método dos Mínimos Quadrados Ordinários (MQO):\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X'X})^{-1} \\mathbf{X}' \\mathbf{y}\n\\]\nAlém disso, calcularemos as somas de quadrados e o quadrado médio do modelo completo.\n\\[\n\\begin{align}\n&SQReg = \\mathbf{y'}\\left[\\mathbf{X(X'X)}^{-1}\\mathbf{X'} - \\frac{1}n \\mathbf{J}\\right]\\mathbf{y}, \\\\\n&\\text{ com k graus de liberdade}\n\\end{align}\n\\]\n\\[\nSQTot = \\mathbf{y'}[\\mathbf{I} - \\frac{1}n \\mathbf{J}]\\mathbf{y} \\space, \\quad \\text{ com n-1 graus de liberdade}\n\\]\n\\[\nSQRes = SQTot - SQReg = \\mathbf{y'} (\\mathbf{I - X(X'X)^{-1}X')y} \\space, \\quad \\text{ com (n-k-1) graus de liberdade}\n\\]\n\n\nR\nSAS\n\n\n\n\nBeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y2\nBeta |&gt; round(4)\n\n          [,1]\nx0  -2282.9283\nx1     22.8600\nx2     21.4148\nx3     33.6097\nx11    -0.0538\nx22    -0.0077\nx33    -0.0859\nx12    -0.1234\nx13    -0.1863\nx23    -0.0341\n\n\n\nSQTotal &lt;- t(y2) %*% (In - (1 / n) * Jnn) %*% y2\nSQTotal\n\n         [,1]\n[1,] 400.4642\n\ngltotal &lt;- n - 1\ngltotal\n\n[1] 18\n\n\n\nSQReg &lt;- t(y2) %*% (X %*% solve(t(X) %*% X) %*% t(X) - (1 / n) * Jnn) %*% y2\nSQReg\n\n         [,1]\n[1,] 339.7888\n\nk &lt;- ncol(X) - 1\ngl_reg &lt;- k\ngl_reg\n\n[1] 9\n\n\n\nSQRes &lt;- SQTotal - SQReg\nSQRes\n\n         [,1]\n[1,] 60.67545\n\ngl_res &lt;- n - k - 1\ngl_res\n\n[1] 9\n\nQMRes &lt;- SQRes / gl_res\nQMRes\n\n         [,1]\n[1,] 6.741717\n\n\n\n\n\nBeta = inv(t(X)*X)*t(X)*y2;\nprint Beta[format=12.4];\n\n\n* Modelo completo;\nSQTotal = t(y2)*(In-(1/n)*Jnn)*y2;\ngltotal = n-1;\nSQReg = t(y2)*(X*inv(t(X)*X)*t(X)-(1/n)*Jnn)*y2;\nk = ncol(X)-1;\ngl_reg = k;\nSQRes = SQTotal-SQReg;\nglres = n-k-1;\nQMRes = SQRes/glres;\n\nprint 'Modelo completo:',,, gl_reg SQReg[format=10.4] SQRes[format=10.4] SQTotal[format=10.4],,,;"
  },
  {
    "objectID": "exemplo4.3.html#hipótese-1",
    "href": "exemplo4.3.html#hipótese-1",
    "title": "8  Teste de Hipótese - Exemplo 2",
    "section": "\n8.2 Hipótese 1",
    "text": "8.2 Hipótese 1\nComo primeira hipótese ao nosso modelo, assumiremos \\(H_0: \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0\\), ou seja, vamos testar se os termos de segunda ordem do modelo não são importantes na predição de \\(y_2\\).\n\\[\n\\begin{align}\nH_0&: \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0 \\\\\nH_a&: \\text{Pelo menos um dos } \\beta's \\text{ não é nulo}\n\\end{align}\n\\]\nCom essa hipótese, temos como modelo reduzido:\n\\[y_2 = \\beta_0^* + \\beta_1^*x_1 + \\beta_2^*x_2 + \\beta_3^*x_3 + \\epsilon^*\\]\n\n\nR\nSAS\n\n\n\n\nX1 &lt;- cbind(x0, x1, x2, x3)\nX1\n\n      x0  x1   x2  x3\n [1,]  1 162 23.0 3.0\n [2,]  1 162 23.0 8.0\n [3,]  1 162 30.0 5.0\n [4,]  1 162 30.0 8.0\n [5,]  1 172 25.0 5.0\n [6,]  1 172 25.0 8.0\n [7,]  1 172 30.0 5.0\n [8,]  1 172 30.0 8.0\n [9,]  1 167 27.5 6.5\n[10,]  1 177 27.5 6.5\n[11,]  1 157 27.5 6.5\n[12,]  1 167 32.5 6.5\n[13,]  1 167 22.5 6.5\n[14,]  1 167 27.5 9.5\n[15,]  1 167 27.5 3.5\n[16,]  1 177 20.0 6.5\n[17,]  1 177 20.0 6.5\n[18,]  1 160 34.0 7.5\n[19,]  1 160 34.0 7.5\n\n\nA seguir, calcularemos os graus de liberdade, somas de quadrados, quadrados médios, estatística F e p-valor do modelo reduzido.\n\nSQReg1 &lt;- t(y2) %*% (X1 %*% solve(t(X1) %*% X1) %*% t(X1) - (1 / n) * Jnn) %*% y2\nSQReg1\n\n         [,1]\n[1,] 151.0022\n\ngl_regB1 &lt;- ncol(X1) - 1\ngl_regB1\n\n[1] 3\n\nSQB2_B1 &lt;- SQReg - SQReg1\nSQB2_B1\n\n         [,1]\n[1,] 188.7866\n\ngl_regB2 &lt;- gl_reg - gl_regB1\nh &lt;- gl_regB2\nh\n\n[1] 6\n\nQMB2_B1 &lt;- SQB2_B1 / h\nQMB2_B1\n\n         [,1]\n[1,] 31.46443\n\nFcalc1 &lt;- QMB2_B1 / QMRes\nFcalc1\n\n         [,1]\n[1,] 4.667124\n\np_valor1 &lt;- 1 - pf(Fcalc1, h, gl_res)\np_valor1\n\n           [,1]\n[1,] 0.01983638\n\n\nDessa forma, o quadro da ANOVA sob a hipótese \\(H_0: \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0\\) usando a abordagem de modelo completo e modelo reduzido, fica:\n\n\n\nTeste de Regressão Global - H0: Beta4 = Beta5 = Beta6 = Beta7 = Beta8 = Beta9 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0\n6\n188.787\n31.464\n4.667\n3.37\n0.019836\n\n\nResíduo\n9\n60.675\n6.742\n\n\n\n\n\nTotal\n18\n400.464\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 6, 9)\\), há evidência para rejeitarmos \\(H_0: \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = 0\\) ao nível de 5% de significância, ou seja, os termos de segunda ordem não são todos nulos e são importantes para a predição de \\(\\mathbf{y_2}\\).\n\n\n\n* ----------------------------------------------------------------------------;\n* Modelo completo vs reduzido para testar H0: B11 = B22 = B33 = B12 = B13 = B23 = 0 ;\n* ----------------------------------------------------------------------------;\nX1 = c0||c1||c2||c3;\ngl_regB1 = ncol(X1)-1;\n* Modelo reduzido (1);\nSQReg1 = t(y2)*(X1*inv(t(X1)*X1)*t(X1)-(1/n)*Jnn)*y2;\nSQB2_B1 = SQReg - SQReg1;\ngl_regB2 = gl_reg - gl_regB1;\nh = gl_regB2;\nQMB2_B1 = SQB2_B1/h;\nFcalc1 = QMB2_B1/QMRes;\np_valor1 = 1-cdf('F',Fcalc1,h,glres);\n\nprint '------------------------------------------------------------',\n      'Teste da hipótese H01: B11 = B22 = B33 = B12 = B13 = B23 = 0',\n      'usando abordagem Modelo completo x Modelo reduzido',\n      '------------------------------------------------------------';\nprint 'g.l. do modelo completo .............................................' gl_reg,\n      'g.l. do modelo reduzido por H0: B11 = B22 = B33 = B12 = B13 = B23 = 0' gl_regB1,\n      'g.l. da diferença (Modelo completo - Modelo reduzido)................' h,,;\nprint 'Quadro de ANOVA para testar H01: B11 = B22 = B33 = B12 = B13 = B23 = 0',,\n'H01      ' h       SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc1[format=10.4] p_valor1[format=10.4],,\n'Resíduo  ' glres   SQRes[format=10.4]   QMRes[format=10.4],,\n'Total    ' gltotal SQTotal[format=10.4],,,,;"
  },
  {
    "objectID": "exemplo4.3.html#hipótese-2",
    "href": "exemplo4.3.html#hipótese-2",
    "title": "8  Teste de Hipótese - Exemplo 2",
    "section": "\n8.3 Hipótese 2",
    "text": "8.3 Hipótese 2\nTambém podemos testar se todos os termos lineares do modelo são nulos, ou seja, \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\).\n\\[\n\\begin{align}\nH_0&: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\\nH_a&: \\text{Pelo menos um dos } \\beta's \\text{ não é nulo}\n\\end{align}\n\\]\nCom essa hipótese, temos como modelo reduzido:\n\\[y_2 = \\beta_4^*x_1^2 + \\beta_5^*x_2^2 + \\beta_6^*x_3^2 + \\beta_7^*x_1x_2 + \\beta_8^*x_1x_3 + \\beta_9^*x_2x_3 + \\epsilon^*\\]\n\n\nR\nSAS\n\n\n\n\nX1 &lt;- cbind(x0, x11, x22, x33, x12, x13, x23)\nX1\n\n      x0   x11     x22   x33    x12    x13    x23\n [1,]  1 26244  529.00  9.00 3726.0  486.0  69.00\n [2,]  1 26244  529.00 64.00 3726.0 1296.0 184.00\n [3,]  1 26244  900.00 25.00 4860.0  810.0 150.00\n [4,]  1 26244  900.00 64.00 4860.0 1296.0 240.00\n [5,]  1 29584  625.00 25.00 4300.0  860.0 125.00\n [6,]  1 29584  625.00 64.00 4300.0 1376.0 200.00\n [7,]  1 29584  900.00 25.00 5160.0  860.0 150.00\n [8,]  1 29584  900.00 64.00 5160.0 1376.0 240.00\n [9,]  1 27889  756.25 42.25 4592.5 1085.5 178.75\n[10,]  1 31329  756.25 42.25 4867.5 1150.5 178.75\n[11,]  1 24649  756.25 42.25 4317.5 1020.5 178.75\n[12,]  1 27889 1056.25 42.25 5427.5 1085.5 211.25\n[13,]  1 27889  506.25 42.25 3757.5 1085.5 146.25\n[14,]  1 27889  756.25 90.25 4592.5 1586.5 261.25\n[15,]  1 27889  756.25 12.25 4592.5  584.5  96.25\n[16,]  1 31329  400.00 42.25 3540.0 1150.5 130.00\n[17,]  1 31329  400.00 42.25 3540.0 1150.5 130.00\n[18,]  1 25600 1156.00 56.25 5440.0 1200.0 255.00\n[19,]  1 25600 1156.00 56.25 5440.0 1200.0 255.00\n\n\nNovamente, calcularemos os graus de liberdade, somas de quadrados, quadrados médios, estatística F e p-valor para o novo modelo reduzido.\n\nSQReg1 &lt;- t(y2) %*% (X1 %*% solve(t(X1) %*% X1) %*% t(X1) - (1 / n) * Jnn) %*% y2\nSQReg1\n\n        [,1]\n[1,] 246.654\n\ngl_regB1 &lt;- ncol(X1) - 1\ngl_regB1\n\n[1] 6\n\nSQB2_B1 &lt;- SQReg - SQReg1\nSQB2_B1\n\n        [,1]\n[1,] 93.1348\n\ngl_regB2 &lt;- gl_reg - gl_regB1\nh &lt;- gl_regB2\nh\n\n[1] 3\n\nQMB2_B1 &lt;- SQB2_B1 / h\nQMB2_B1\n\n         [,1]\n[1,] 31.04493\n\nFcalc2 &lt;- QMB2_B1 / QMRes\nFcalc2\n\n       [,1]\n[1,] 4.6049\n\np_valor2 &lt;- 1 - pf(Fcalc2, h, gl_res)\np_valor2\n\n           [,1]\n[1,] 0.03234981\n\n\nDessa forma, o quadro da ANOVA sob a hipótese \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) usando a abordagem de modelo completo e modelo reduzido, fica:\n\n\n\nTeste de Regressão Global - H0: Beta1 = Beta2 = Beta3 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0\n3\n93.135\n31.045\n4.605\n3.86\n0.03235\n\n\nResíduo\n9\n60.675\n6.742\n\n\n\n\n\nTotal\n18\n400.464\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 3, 9)\\), há evidência para rejeitarmos \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) ao nível de 5% de significância, ou seja, pelo menos um dos termos lineares do modelo não são nulos (\\(x_1\\), \\(x_2\\) ou \\(x_3\\)), sendo importante para predizer \\(\\mathbf{y_2}\\).\n\n\n\n* ---------------------------------------------------------------;\n*  Modelo completo vs reduzido para testar H02: B1 = B2 = B3 = 0 ;\n* ---------------------------------------------------------------;\n* Modelo reduzido (2);\nX1 = c0||c11||c22||c33||c12||c13||c23;\ngl_regB1 = ncol(X1)-1;\nSQReg1 = t(y2)*(X1*inv(t(X1)*X1)*t(X1)-(1/n)*Jnn)*y2;\nSQB2_B1 = SQReg - SQReg1;\ngl_regB2 = gl_reg - gl_regB1;\nh = gl_regB2;\nQMB2_B1 = SQB2_B1/h;\nFcalc2 = QMB2_B1/QMRes;\np_valor2 = 1-cdf('F',Fcalc2,h,glres);\n\nprint '--------------------------------------------------',\n      'Teste da Hipótese H02: B1 = B2 = B3 = 0           ',\n      'usando abordagem Modelo completo x Modelo reduzido',\n      '--------------------------------------------------',;\n\nprint 'g.l. do modelo completo ................................' gl_reg,\n      'g.l. do modelo reduzido por H0: B1 = B2 = B3 = 0 .......' gl_regB1,\n      'g.l. da diferença (Modelo completo - Modelo reduzido)...' h,,;\nprint 'Quadro de ANOVA para testar H02: B1 = B2 = B3 = 0',,\n'H02      ' h       SQB2_B1[format=10.4] QMB2_B1[format=10.4] Fcalc2[format=10.4] p_valor2[format=10.4],,\n'Resíduo  ' glres   SQRes[format=10.4]   QMRes[format=10.4],,\n'Total    ' gltotal SQTotal[format=10.4],,,,;"
  },
  {
    "objectID": "exemplo4.3.html#hipótese-linear-geral",
    "href": "exemplo4.3.html#hipótese-linear-geral",
    "title": "8  Teste de Hipótese - Exemplo 2",
    "section": "\n8.4 Hipótese Linear Geral",
    "text": "8.4 Hipótese Linear Geral\nOs detalhes sobre a hipótese linear geral estão na Seção 7.2.\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C} \\boldsymbol{\\beta} = 0 \\\\\nH_a &: \\mathbf{C} \\boldsymbol{\\beta} \\ne 0\n\\end{align}\n\\]\nAqui, realizaremos o teste sobre a hipótese \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) com a seguinte proposta de matriz de coeficientes \\(\\mathbf{C}\\).\n\\[\n\\begin{align}\nH_0 &: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\\nH_0 &: \\mathbf{C} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &:\n\\begin{bmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC &lt;- matrix(\n  c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 1, 0, 0, 0, 0, 0, 0), \n  ncol = 10, nrow = 3, byrow = TRUE\n)\nC\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    1    0    0    0    0    0    0    0     0\n[2,]    0    0    1    0    0    0    0    0    0     0\n[3,]    0    0    0    1    0    0    0    0    0     0\n\n\n\nCBeta &lt;- C %*% Beta\nCBeta\n\n         [,1]\n[1,] 22.85997\n[2,] 21.41475\n[3,] 33.60975\n\nSQHip &lt;- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta\nSQHip\n\n         [,1]\n[1,] 93.13479\n\nq &lt;- nrow(C)\n\nQMHip &lt;- SQHip / q\nQMHip\n\n         [,1]\n[1,] 31.04493\n\nFcalc3 &lt;- QMHip / QMRes\nFcalc3\n\n       [,1]\n[1,] 4.6049\n\np_valor3 &lt;- 1 - pf(Fcalc3, q, gl_res)\np_valor3\n\n           [,1]\n[1,] 0.03234982\n\n\nO quadro da ANOVA sob a hipótese \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) usando a abordagem da hipótese linear geral, fica:\n\n\n\nTeste de Regressão com hipótese linear geral - H0: Beta1 = Beta2 = Beta3 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0\n3\n93.135\n31.045\n4.605\n3.86\n0.03235\n\n\nResíduo\n9\n60.675\n6.742\n\n\n\n\n\nTotal\n18\n400.464\n\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &gt; F_{tab}\\), para \\(F(0.05, 3, 9)\\), há evidência para rejeitarmos \\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\) ao nível de 5% de significância, ou seja, pelo menos um dos termos lineares do modelo não são nulos (\\(x_1\\), \\(x_2\\) ou \\(x_3\\)), sendo importante para predizer \\(\\mathbf{y_2}\\).\nCom isso, nota-se que tanto a abordagem de modelo completo e modelo reduzido, como a da hipótese linear geral geram o mesmo resultado.\n\n\n\n* ----------------------------------------------------------------;\n* Hipótese linear geral (C*Beta=0) para testar H0:B1 = B2 = B3 = 0;\n* ----------------------------------------------------------------;\nC = {0 1 0 0 0 0 0 0 0 0,\n     0 0 1 0 0 0 0 0 0 0,\n       0 0 0 1 0 0 0 0 0 0};\nCBeta = C*Beta;\nSQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;\nq = nrow(C);\nQMHip = SQHip/q;\nFCalc3 = QMHip/QMRes;\np_valor3 = 1-cdf('F',Fcalc3,q,glres);\nprint 'Hipótese H0: B1 = B2 = B3 = 0 usando abordagem C*Beta = 0',,,\n'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,\n'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,\n'Total      ' gltotal SQTotal[format=10.4],,,,;\n\n\n\n\nAgora, demonstraremos que a matriz de coeficientes \\(\\mathbf{C}\\) pode assumir mais de uma forma para o teste de uma mesma hipótese. Construiremos três matrizes de coeficientes \\(\\mathbf{C}\\) (C1, C2 e C3), tendo como hipótese \\(H_0: \\beta_1 = \\beta_2 = \\beta_3\\).\n\\[H_0: \\beta_1 = \\beta_2 = \\beta_3\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_1} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & -1 &  0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 &  1 & -1 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\\\ \\beta_6 \\\\ \\beta_7 \\\\ \\beta_8 \\\\ \\beta_9\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &:\n\\begin{bmatrix}\n\\beta_1 - \\beta_2 \\\\ \\beta_2 - \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\\iff\n\\beta_1 = \\beta2 \\quad ; \\quad \\beta_2 = \\beta3\n\\end{align}\n\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_2} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 2 & -1 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 &  1 & -1 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\\\ \\beta_6 \\\\ \\beta_7 \\\\ \\beta_8 \\\\ \\beta_9\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &:\n\\begin{bmatrix}\n2\\beta_1 - \\beta_2 - \\beta_3 \\\\ \\beta_2 - \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\\iff\n2\\beta_1 = \\beta2 + \\beta_3 \\quad ; \\quad \\beta_2 = \\beta3\n\\end{align}\n\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_3} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & 0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\\\ \\beta_6 \\\\ \\beta_7 \\\\ \\beta_8 \\\\ \\beta_9\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &:\n\\begin{bmatrix}\n\\beta_1 - \\beta_3 \\\\ \\beta_2 - \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix}\n\\iff\n\\beta_1 = \\beta3 \\quad ; \\quad \\beta_2 = \\beta3\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC1 &lt;- matrix(\n  c(0, 1, -1,  0, 0, 0, 0, 0, 0, 0,\n    0, 0,  1, -1, 0, 0, 0, 0, 0, 0), \n  ncol = 10, nrow = 2, byrow = TRUE\n)\nC1\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    1   -1    0    0    0    0    0    0     0\n[2,]    0    0    1   -1    0    0    0    0    0     0\n\n\n\nC2 &lt;- matrix(\n  c(0, 2, -1, -1, 0, 0, 0, 0, 0, 0,\n    0, 0,  1, -1, 0, 0, 0, 0, 0, 0), \n  ncol = 10, nrow = 2, byrow = TRUE\n)\nC2\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    2   -1   -1    0    0    0    0    0     0\n[2,]    0    0    1   -1    0    0    0    0    0     0\n\n\n\nC3 &lt;- matrix(\n  c(0, 1, 0,  -1, 0, 0, 0, 0, 0, 0,\n    0, 0, 1, -1, 0, 0, 0, 0, 0, 0), \n  ncol = 10, nrow = 2, byrow = TRUE\n)\nC3\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    1    0   -1    0    0    0    0    0     0\n[2,]    0    0    1   -1    0    0    0    0    0     0\n\n\nPara aplicarmos cada uma das matrizes de coeficientes, criaremos uma função.\n\nhlg &lt;- function(C) {\n\n  # Calculando CBeta\n  CBeta &lt;- C %*% Beta |&gt; round(3)\n  \n  # Calculando SQHip\n  SQHip &lt;- t(CBeta) %*% solve(C %*% solve(t(X) %*% X) %*% t(C)) %*% CBeta\n  \n  # Obtendo o número de linhas de C\n  q &lt;- nrow(C)\n  \n  # Calculando QMHip\n  QMHip &lt;- SQHip / q\n  \n  # Calculando Fcalc\n  Fcalc3 &lt;- QMHip / QMRes\n  \n  # Calculando p_valor\n  p_valor3 &lt;- 1 - pf(Fcalc3, q, gl_res)\n  \n  # Retornando resultados\n  return(list(CBeta = CBeta, SQHip = SQHip, QMHip = QMHip, Fcalc = Fcalc3, p_valor = p_valor3))\n}\n\nPara cada um dos \\(\\mathbf{C}'s\\) (C1, C2 e C3), aplicaremos o teste com a função criada hlg().\n\n# Definindo os valores para C\nC_lista &lt;- list(C1 = C1, C2 = C2, C3 = C3)\n\n# Criando uma lista para armazenar os resultados\nresultados &lt;- list()\n\n# Calculando os resultados para cada objeto C\nfor (i in 1:length(C_lista)) {\n  resultados[[paste0(\"C\", i)]] &lt;- hlg(C_lista[[i]])\n}\n\n\ntabela_resultados &lt;- data.frame(\n  SQHip = sapply(resultados, function(x) x$SQHip),\n  QMHip = sapply(resultados, function(x) x$QMHip),\n  Fcalc = sapply(resultados, function(x) x$Fcalc),\n  p_valor = sapply(resultados, function(x) x$p_valor)\n)\n\ntabela_resultados\n\n      SQHip    QMHip     Fcalc   p_valor\nC1 2.336037 1.168019 0.1732524 0.8436644\nC2 2.336037 1.168019 0.1732524 0.8436644\nC3 2.336037 1.168019 0.1732524 0.8436644\n\n\nApesar de diferentes matrizes de coeficientes \\(\\mathbf{C}\\), obtemos os mesmos valores para os testes de hipótese. Como conclusão, não temos evidências para rejeitar a hipótese nula \\(H_0: \\beta_1 = \\beta_2 = \\beta_3\\), ao nível de 5% de significância.\n\n\n\n* ------------------------------------------------------------------------- ;\n* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 1) ;\n*                           (Matriz C não é única!)                         ;\n* ------------------------------------------------------------------------- ;\nC = {0 1 -1  0 0 0 0 0 0 0,\n     0 0  1 -1 0 0 0 0 0 0};\nCBeta = C*Beta;\nSQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;\nq = nrow(C);\nQMHip = SQHip/q;\nFCalc3 = QMHip/QMRes;\np_valor3 = 1-cdf('F',Fcalc3,q,glres);\nprint 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 1)',,,\n'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,\n'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,\n'Total      ' gltotal SQTotal[format=10.4],,,,;\n\n\n* ------------------------------------------------------------------------- ;\n* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 2) ;\n* ------------------------------------------------------------------------- ;\nC = {0 2 -1 -1 0 0 0 0 0 0,\n     0 0  1 -1 0 0 0 0 0 0};\nCBeta = C*Beta;\nSQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;\nq = nrow(C);\nQMHip = SQHip/q;\nFCalc3 = QMHip/QMRes;\np_valor3 = 1-cdf('F',Fcalc3,q,glres);\nprint 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 2)',,,\n'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,\n'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,\n'Total      ' gltotal SQTotal[format=10.4],,,,;\n\n\n* -------------------------------------------------------------------------;\n* Hipótese linear geral (C*Beta=0) para testar H0: B1 = B2 = B3 (SOLUÇÃO 3);\n* -------------------------------------------------------------------------;\nC = {0 1  0 -1 0 0 0 0 0 0,\n     0 0  1 -1 0 0 0 0 0 0};\nCBeta = C*Beta;\nSQHip = t(CBeta)*inv(C*inv(t(X)*X)*t(C))*CBeta;\nq = nrow(C);\nQMHip = SQHip/q;\nFCalc3 = QMHip/QMRes;\np_valor3 = 1-cdf('F',Fcalc3,q,glres);\nprint 'H0: B1 = B2 = B3 usando C*Beta = 0 (SOLUÇÃO 3)',,,\n'Hipótese H0' q       SQHip[format=10.4] QMHip[format=10.4] Fcalc3[format=10.4] p_valor3[format=10.4],,\n'Resíduo    ' glres   SQRes[format=10.4] QMRes[format=10.4],,\n'Total      ' gltotal SQTotal[format=10.4],,,,;\nquit;"
  },
  {
    "objectID": "exemplo4.4.html#hipótese-1",
    "href": "exemplo4.4.html#hipótese-1",
    "title": "\n9  Teste de Hipótese - Exemplo 3\n",
    "section": "\n9.1 Hipótese 1",
    "text": "9.1 Hipótese 1\nA primeira hipótese que testaremos é:\n\\[\n\\begin{align}\nH_0 &: 2\\beta_1 - 2\\beta_2 = 2\\beta_2 - \\beta_3 = 0 \\\\\nH_0 &: \\mathbf{C} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & -1 &  0 \\\\\n0 & 0 &  2 & -1  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &: \\begin{cases}\n\\beta_1 - \\beta_2 = 0 \\\\\n2\\beta_2 - \\beta_3 = 0\n\\end{cases}\n\\iff\nH_0: 2\\beta_1 = 2\\beta_2 = \\beta_3 = 0\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC &lt;- matrix(\n  c(0, -2,  2,  0, \n    0,  0,  2, -1),\n  nrow = 2, byrow = TRUE\n)\nC\n\n     [,1] [,2] [,3] [,4]\n[1,]    0   -2    2    0\n[2,]    0    0    2   -1\n\n\n\nCBeta &lt;- C %*% Beta\nCBeta\n\n           [,1]\n[1,]  0.2428042\n[2,] -0.6117518\n\nSQHip &lt;- t(CBeta) %*% solve(C %*% (solve(t(X) %*% X)) %*% t(C)) %*% CBeta\nSQHip\n\n         [,1]\n[1,] 28.62324\n\ngl_Hip &lt;- nrow(C)\ngl_Hip\n\n[1] 2\n\nQMHip &lt;- SQHip / gl_Hip\nQMHip\n\n         [,1]\n[1,] 14.31162\n\n\n\nSQRes &lt;- t(y1) %*% (In - X %*% solve(t(X) %*% X) %*% t(X)) %*% y1\nSQRes\n\n         [,1]\n[1,] 80.17354\n\ngl_res &lt;- n - k - 1\ngl_res\n\n[1] 15\n\nQMRes &lt;- SQRes / gl_res\nQMRes\n\n         [,1]\n[1,] 5.344903\n\n\n\nFcalc1 &lt;- QMHip / QMRes\nFcalc1\n\n         [,1]\n[1,] 2.677621\n\nFtab1 &lt;- qf(0.95, gl_Hip, gl_res)\nFtab1\n\n[1] 3.68232\n\np_valor1 &lt;- 1 - pf(Fcalc1, gl_Hip, gl_res) \np_valor1\n\n          [,1]\n[1,] 0.1013007\n\n\nDessa forma, o quadro da ANOVA sob a hipótese \\(H_0: 2\\beta_1 - 2\\beta_2 = 2\\beta_2 - \\beta_3 = 0\\) ou \\(H_0: 2\\beta_1 = 2\\beta_2 = \\beta_3\\), fica:\n\n\n\nTeste de Regressão Global - H0: 2Beta1 = 2Beta2 = 2Beta3 = 0\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0\n2\n28.623\n14.312\n2.678\n3.682\n0.1013\n\n\nResíduo\n15\n80.174\n5.345\n\n\n\n\n\n\n\n\nDado que o \\(F_{cal} &lt; F_{tab}\\), para \\(F(0.05, 2, 15)\\), não podemos rejeitar \\(H_0: \\beta_1 - \\beta_2 = 2\\beta_2 - \\beta_3 = 0\\) ou \\(H_0: 2\\beta_1 = 2\\beta_2 = \\beta_3\\) ao nível de 5% de significância.\n\n\n\nC = {0 -2 2  0, \n     0 0  2 -1};\ngl_H0 = nrow(C);\nCBeta = C*Beta;\nSQH0 = t(CBeta)*inv(C*(inv(t(X)*X))*t(C))*CBeta;\nQMH0 = SQH0/gl_H0;\n\nSQRes = t(y)*(In - X*inv(t(X)*X)*t(X))*y;\ngl_res = n-k-1;\nQMRes = SQRes/gl_res;\n\nFcalc = QMH0/QMRes;\np_valor = 1-cdf('F',Fcalc,gl_H0,gl_res);\n\nprint 'Exemplo 8.4.1(b): Exemplo com dados de reação química (Tabela 7.4)',,\n      'Teste H0: 2B1 = 2B2 = B3 ou H0: B1 - B2 = 2B2 - B3 = 0',;\nprint 'H0        ' gl_H0  SQH0[format=8.4]  QMH0[format=8.4]  Fcalc[format=8.4] p_valor[format=8.4],,\n      'Resíduo   ' gl_res SQRes[format=8.4] QMRes[format=8.4],,,,;"
  },
  {
    "objectID": "exemplo4.4.html#hipótese-2",
    "href": "exemplo4.4.html#hipótese-2",
    "title": "\n9  Teste de Hipótese - Exemplo 3\n",
    "section": "\n9.2 Hipótese 2",
    "text": "9.2 Hipótese 2\nAgora, testaremos a hipótese \\(H_0: \\beta_1 = \\beta_2 = \\beta_3\\) utilizando diferentes matrizes \\(\\mathbf{C}\\):\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3\n\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_1} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & -1 &  0 \\\\\n0 & 0 &  1 & -1  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &: \\begin{cases}\n\\beta_1 - \\beta_2 = 0 \\\\\n\\beta_2 - \\beta_3 = 0\n\\end{cases}\n\\iff\n\\beta_1 = \\beta2 \\quad ; \\quad \\beta_2 = \\beta_3\n\\end{align}\n\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_2} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 1 & -1 &  0 \\\\\n0 & 1 &  0 & -1  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &: \\begin{cases}\n\\beta_1 - \\beta_2 = 0 \\\\\n\\beta_1 - \\beta_3 = 0\n\\end{cases}\n\\iff\n\\beta_1 = \\beta_2 \\quad ; \\quad \\beta_1 = \\beta_3\n\\end{align}\n\\]\n\\[\n\\begin{align}\nH_0 &: \\mathbf{C_3} \\boldsymbol{\\beta} =\n\\begin{bmatrix}\n0 & 2 & -1 & -1 \\\\\n0 & 0 &  1 & -1  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ 0\n\\end{bmatrix} \\\\\nH_0 &: \\begin{cases}\n2\\beta_1 - \\beta_2 - \\beta_3 = 0 \\\\\n\\beta_2 - \\beta_3 = 0\n\\end{cases}\n\\iff\n2\\beta_1 = \\beta_2 + \\beta_3 \\quad ; \\quad \\beta_2 = \\beta_3\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC1 &lt;- matrix(\n  c(0, 1, -1,  0, \n    0, 0,  1, -1),\n  nrow = 2, byrow = TRUE\n)\nC1\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1   -1    0\n[2,]    0    0    1   -1\n\nSQ_C1Beta &lt;- t(C1 %*% Beta) %*% solve(C1 %*% (solve(t(X) %*% X)) %*% t(C1)) %*% C1 %*% Beta\nSQ_C1Beta\n\n         [,1]\n[1,] 22.63088\n\n\n\nC2 &lt;- matrix(\n  c(0, 1, -1,  0, \n    0, 1,  0, -1),\n  nrow = 2, byrow = TRUE\n)\nC2\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1   -1    0\n[2,]    0    1    0   -1\n\nSQ_C2Beta &lt;- t(C2 %*% Beta) %*% solve(C2 %*% (solve(t(X) %*% X)) %*% t(C2)) %*% C2 %*% Beta\nSQ_C2Beta\n\n         [,1]\n[1,] 22.63088\n\n\n\nC3 &lt;- matrix(\n  c(0, 2, -1, -1, \n    0, 0,  1, -1),\n  nrow = 2, byrow = TRUE\n)\nC3\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    2   -1   -1\n[2,]    0    0    1   -1\n\nSQ_C3Beta &lt;- t(C3 %*% Beta) %*% solve(C3 %*% (solve(t(X) %*% X)) %*% t(C3)) %*% C3 %*% Beta\nSQ_C3Beta\n\n         [,1]\n[1,] 22.63088\n\n\n\n\n\nTeste de Regressão Hipótese Linear Geral para diversas matrizes C - H0: Beta1 = Beta2 = Beta3 = 0\n\nSQ_C1\nSQ_C2\nSQ_C3\n\n\n22.63088\n22.63088\n22.63088\n\n\n\n\n\n\n\n* Testar a hipótese H0: B1=B2=B3 usando a hipótese linear geral;\nC1 = {0  1 -1  0, \n      0  0  1 -1};\nSQ_C1Beta = t(C1*Beta)*inv(C1*(inv(t(X)*X))*t(C1))*C1*Beta;\nC2 = {0  1 -1  0, \n      0  1  0 -1};\nSQ_C2Beta = t(C2*Beta)*inv(C2*(inv(t(X)*X))*t(C2))*C2*Beta;\nC3 = {0  2 -1 -1, \n      0  0  1 -1};\nSQ_C3Beta = t(C3*Beta)*inv(C3*(inv(t(X)*X))*t(C3))*C3*Beta;\n\nprint 'SQH0 para a hipótese H0: B1=B2=B3 usando diferentes matrizes C, em C*Beta=0',,,\n     SQ_C1Beta[format=12.4] SQ_C2Beta[format=12.4]SQ_C3Beta[format=12.4];"
  },
  {
    "objectID": "exemplo4.4.html#hipótese-3",
    "href": "exemplo4.4.html#hipótese-3",
    "title": "\n9  Teste de Hipótese - Exemplo 3\n",
    "section": "\n9.3 Hipótese 3",
    "text": "9.3 Hipótese 3\nO próximo teste de hipótese a ser realizado é:\n\\[\nH_0: \\beta_1 = \\beta_2\n\\]\nRealizaremos o teste utilizando a hipótese linear geral e a abordagem do modelo completo e modelo reduzido.\n\n\nR\nSAS\n\n\n\nPela hipótese linear geral, dada a matriz \\(\\mathbf{C} = [0, 1, -1, 0]\\), temos:\n\nC &lt;- matrix(c(0, 1, -1, 0), nrow = 1, byrow = TRUE)\nC\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1   -1    0\n\n\n\nCBeta &lt;- C %*% Beta\nCBeta\n\n           [,1]\n[1,] -0.1214021\n\ngl_Hip &lt;- nrow(C)\ngl_Hip\n\n[1] 1\n\nSQHip &lt;- t(CBeta) %*% solve(C %*% (solve(t(X) %*% X)) %*% t(C)) %*% CBeta\nSQHip\n\n        [,1]\n[1,] 4.37816\n\nQMHip &lt;- SQHip / gl_Hip\nQMHip\n\n        [,1]\n[1,] 4.37816\n\nFcalc2 &lt;- QMHip / QMRes\nFcalc2\n\n         [,1]\n[1,] 0.819128\n\np_valor2 &lt;- 1 - pf(Fcalc2, gl_Hip, gl_res)\np_valor2\n\n          [,1]\n[1,] 0.3797428\n\n\n\n\n\nTeste de Regressão com Hipótese Linear Geral - H0: Beta1 = Beta2\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nH0\n1\n4.378\n4.378\n0.819\n4.543\n0.3797\n\n\nResíduo\n15\n80.174\n5.345\n\n\n\n\n\n\n\n\nPelo método do modelo completo e modelo reduzido, note que a soma de quadrados da hipótese é a mesma da hipótese linear geral.\n\nx12 &lt;- x1 + x2           # Modelo reduzido: y = B0 + B12(x1+x2) + B3x3 + e\nx12\n\n [1] 185.0 185.0 192.0 192.0 197.0 197.0 202.0 202.0 194.5 204.5 184.5 199.5\n[13] 189.5 194.5 194.5 197.0 197.0 194.0 194.0\n\nXr &lt;- cbind(x0, x12, x3) # Matriz Xr do modelo reduzido\nXr\n\n      x0   x12  x3\n [1,]  1 185.0 3.0\n [2,]  1 185.0 8.0\n [3,]  1 192.0 5.0\n [4,]  1 192.0 8.0\n [5,]  1 197.0 5.0\n [6,]  1 197.0 8.0\n [7,]  1 202.0 5.0\n [8,]  1 202.0 8.0\n [9,]  1 194.5 6.5\n[10,]  1 204.5 6.5\n[11,]  1 184.5 6.5\n[12,]  1 199.5 6.5\n[13,]  1 189.5 6.5\n[14,]  1 194.5 9.5\n[15,]  1 194.5 3.5\n[16,]  1 197.0 6.5\n[17,]  1 197.0 6.5\n[18,]  1 194.0 7.5\n[19,]  1 194.0 7.5\n\nSQHip2 &lt;- t(y1) %*% (X %*% solve(t(X) %*% X) %*% t(X) - Xr %*% solve(t(Xr) %*% Xr) %*% t(Xr)) %*% y1\nSQHip2\n\n        [,1]\n[1,] 4.37816\n\n\n\n\n\n* Teste da hipótese H0: Beta1=Beta2;\n* (1) Usando C*Beta=0;\nC = {0  1 -1  0};\ngl_H0 = nrow(C);\n\nCBeta = C*Beta;\nSQH0 = t(CBeta)*inv(C*(inv(t(X)*X))*t(C))*CBeta;\nQMH0 = SQH0/gl_H0;\nFcalc = QMH0/QMRes;\np_valor = 1-cdf('F',Fcalc,gl_H0,gl_res);\n\nprint 'Teste H0: B1 = B2 usando Hipótese Linear Geral',;\nprint 'H0        ' gl_H0  SQH0[format=8.4]  QMH0[format=8.4]  Fcalc[format=8.4] p_valor[format=8.4],,\n      'Resíduo   ' gl_res SQRes[format=8.4] QMRes[format=8.4],,,,;\n\n* (2) Incorporando H0: Beta1 = Beta2 ao modelo;\nx12=x1+x2; * Modelo reduzido: y = B0 + B12(x1+x2) + B3x3 + e;\nXr = jn||x12||x3; * Matriz Xr do modelo reduzido;\n\nSQH0r = t(y)*(X*inv(t(X)*X)*t(X)-Xr*inv(t(Xr)*Xr)*t(Xr))*y;\nprint 'SQ de H0:B1=B2, usando modelo completo x modelo reduzido:'\n    ,,SQH0r[format=12.4]; \n\nquit;"
  },
  {
    "objectID": "exemplo4.5.html",
    "href": "exemplo4.5.html",
    "title": "\n10  Método de Bonferroni e Método de Scheffé\n",
    "section": "",
    "text": "Em muitos dos casos, estamos interessados em realizar diversos testes de hipótese separados, como:\n\\[\n\\begin{align}\nH_0 &: \\beta_j = 0 , \\quad \\text{para  j = 1,2,...,k} \\\\\n&\\text{ou} \\\\\nH_0 &: \\boldsymbol{a'_i\\beta} = 0 , \\quad \\text{para  i = 1,2,...}\n\\end{align}\n\\]\nQuando testamos diversas hipóteses como as apresentadas anteriormente, temos dois diferentes níveis de significância (\\(\\alpha\\)):\n\nNível de significância geral (Familywise) (\\(\\alpha_f\\));\nNível de significância por comparação (Comparison-wise) (\\(\\alpha_c\\)).\n\nQuando realizamos um único teste de hipótese (como os apresentados nos capítulos anteriores), utilizamos um nível de significância geral \\(\\alpha_f\\).\nContudo, ao realizar mais de um teste de hipótese para um mesmo caso, utilizamos um nível de significância por comparação \\(\\alpha_c\\). Isso acarreta em um aumento do nível de significância geral (\\(\\alpha_f\\)). Probabilisticamente, tem-se:\n\\[\\alpha_f = 1 - (1 - \\alpha_c)^k\\]\nem que \\(k\\) é o número de execuções do teste de hipótese ao nível \\(\\alpha_c\\) de significância por teste realizado.\nCom isso, fixando um nível de significância \\(\\alpha_c = 0,05\\) para cada teste, conforme aumentarmos o número de testes, o nível de significância geral (\\(\\alpha_f\\)) aumenta.\n\n\n\n\nNº testes\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nAlpha f\n0.05\n0.0975\n0.1426\n0.1855\n0.2262\n0.2649\n0.3017\n0.3366\n0.3698\n0.4013\n\n\n\n\nPara isso, serão apresentados dois métodos de comparação que protegem a inflação do nível \\(\\alpha\\)-global quando diversos testes são feitos: método de Bonferroni e método de Scheffé.\nComo exemplo, novamente, utilizaremos os dados do Capítulo 6.\n\n\n\n\nObs\ny\nx1\nx2\n\n\n\n1\n2\n0\n2\n\n\n2\n3\n2\n6\n\n\n3\n2\n2\n7\n\n\n4\n7\n2\n5\n\n\n5\n6\n4\n9\n\n\n6\n8\n4\n8\n\n\n7\n10\n4\n7\n\n\n8\n7\n6\n10\n\n\n9\n8\n6\n11\n\n\n10\n12\n6\n9\n\n\n11\n11\n8\n15\n\n\n12\n14\n8\n13\n\n\n\n\n\n\n\nR\nSAS\n\n\n\n\ny &lt;- c(2,3,2,7,6,8,10,7,8,12,11,14)\n\nn &lt;- length(y)\n\nx0 &lt;- rep(1, n)\n\nx1 &lt;- c(0,2,2,2,4,4,4,6,6,6,8,8)\n\nx2 &lt;- c(2,6,7,5,9,8,7,10,11,9,15,13)\n\nX &lt;- cbind(x0, x1, x2)\nX\n\n      x0 x1 x2\n [1,]  1  0  2\n [2,]  1  2  6\n [3,]  1  2  7\n [4,]  1  2  5\n [5,]  1  4  9\n [6,]  1  4  8\n [7,]  1  4  7\n [8,]  1  6 10\n [9,]  1  6 11\n[10,]  1  6  9\n[11,]  1  8 15\n[12,]  1  8 13\n\nk &lt;- ncol(X) - 1\n\nIn &lt;- diag(n)\n\n\nBeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nBeta\n\n        [,1]\nx0  5.375394\nx1  3.011830\nx2 -1.285489\n\na1 &lt;- matrix(data = c(0, 1, 0), ncol = 1, byrow = TRUE)\na1\n\n     [,1]\n[1,]    0\n[2,]    1\n[3,]    0\n\nBeta1 &lt;- t(a1) %*% Beta\nBeta1\n\n        [,1]\n[1,] 3.01183\n\na2 &lt;- matrix(data = c(0,0,1), ncol = 1, byrow = TRUE)\na2\n\n     [,1]\n[1,]    0\n[2,]    0\n[3,]    1\n\nBeta2 &lt;- t(a2) %*% Beta\nBeta2\n\n          [,1]\n[1,] -1.285489\n\n\n\nSQRes &lt;- t(y) %*% (In - X %*% solve(t(X) %*% X) %*% t(X)) %*% y\nSQRes\n\n         [,1]\n[1,] 25.45899\n\ngl_res &lt;- n - k - 1\ngl_res\n\n[1] 9\n\nQMRes &lt;- SQRes / gl_res\nQMRes     # QMRes = s²\n\n         [,1]\n[1,] 2.828777\n\n\nVamos assumir que os testes para \\(H_0: \\beta_j = 0\\), para \\(j = 1,2,\\dots,k\\), serão executados sem considerar se a hipótese global \\(H_0: \\boldsymbol{\\beta_1 = 0}\\) foi rejeitada.\nUsamos a seguinte estatística:\n\\[\nt_j = \\frac{\\hat{\\beta}_j}{s\\sqrt{g_{(j+1,j+1)}}}\n\\]\n\ns &lt;- sqrt(QMRes)\ns\n\n         [,1]\n[1,] 1.681897\n\ng &lt;- solve(t(X) %*% X)\ng\n\n           x0         x1          x2\nx0  0.9747634  0.2429022 -0.22870662\nx1  0.2429022  0.1620662 -0.11119874\nx2 -0.2287066 -0.1111987  0.08359621\n\nt1 &lt;- Beta1 / (s %*% sqrt(t(a1) %*% g %*% a1))\nt1\n\n         [,1]\n[1,] 4.448205\n\nt2 &lt;- Beta2 / (s %*% sqrt(t(a2) %*% g %*% a2))\nt2\n\n          [,1]\n[1,] -2.643478\n\nt_tab &lt;- qt(0.975, n - k - 1)\nt_tab\n\n[1] 2.262157\n\np_valor_t1 &lt;- 2 * (1 - pt(abs(t1), gl_res))\np_valor_t1\n\n            [,1]\n[1,] 0.001604352\n\np_valor_t2 &lt;- 2 * (1 - pt(abs(t2), gl_res))\np_valor_t2\n\n           [,1]\n[1,] 0.02676076\n\n\nAqui, consideramos que foram realizados dois testes, com nível de significância de 5% para cada teste.\n\n\n\n\nTeste\nt_calc\nt_tab\np-valor (5%)\n\n\n\nH0: Beta1 = 0\n4.448205\n2.262157\n0.0016\n\n\nH0: Beta2 = 0\n-2.643478\n2.262157\n0.0268\n\n\n\n\n\n\np &lt;- 1 - 0.05 / (2 * k)\np\n\n[1] 0.9875\n\nt_tab_Bon &lt;- qt(p, n - k - 1)\nt_tab_Bon\n\n[1] 2.685011\n\nt_tab_Scheffe &lt;- sqrt((k + 1) %*% qf(0.95, k + 1, n - k - 1)) |&gt; as.numeric()\nt_tab_Scheffe\n\n[1] 3.404063\n\n\n\n\n\n\nParâmetros\nt_calc\nt_tab\nt_Bonferroni\nt_Scheffé\n\n\n\nBeta1\n4.448205\n2.262157\n2.685011\n3.404063\n\n\nBeta2\n-2.643478\n2.262157\n2.685011\n3.404063\n\n\n\n\n\nNota-se que o valor do \\(t\\) calculado (t_calc) para o estimador de \\(\\beta_1\\) é maior do que as estatísticas de \\(t\\) tabelado, de \\(t\\) de Bonferroni e de \\(t\\) de Scheffé, assim, rejeita-se a hipótese \\(H_0: \\beta_1 = 0\\), com um nível de significância geral \\(\\alpha_f = 0, 05\\).\nPor outro lado, para o estimador de \\(\\beta_2\\), obteve-se um valor absoluto de \\(t\\) maior que o \\(t\\) tabelado, porém inferior a do \\(t\\) de Bonferroni e do \\(t\\) de Scheffé. Assim, utilizando os dois métodos de nível de significância por comparação, não temos evidências para rejeitar a hipótese \\(H_0: \\beta_2 = 0\\).\n\n\n\nproc iml;\ny = {2,3,2,7,6,8,10,7,8,12,11,14};\nn = nrow(y);\nx0 = j(n,1,1);\nx1 = {0,2,2,2,4,4,4,6,6,6,8,8};\nx2 = {2,6,7,5,9,8,7,10,11,9,15,13};\nX = x0||x1||x2;\nk = ncol(X)-1;\nIn = I(n);\n\n\nBeta = inv(t(X)*X)*t(X)*y;\na0 = {1,0,0}; Beta0 = t(a0)*Beta; \na1 = {0,1,0}; Beta1 = t(a1)*Beta;\na2 = {0,0,1}; Beta2 = t(a2)*Beta;\n\n\nSQRes = t(y)*(In - X*inv(t(X)*X)*t(X))*y;\ngl_res = n-k-1;\nQMRes = SQRes/gl_res;\ns = sqrt(QMRes);\nG = inv(t(X)*X);\n\n\nt1 = Beta1/(s*sqrt(t(a1)*G*a1));\nt2 = Beta2/(s*sqrt(t(a2)*G*a2));\np_valor_t1 = 2*(1-cdf('t',abs(t1),gl_res));\np_valor_t2 = 2*(1-cdf('t',abs(t2),gl_res));\n\n\np = 1-0.05/(2*k);\nt_tab = tinv(0.975,n-k-1);                      * calcula t-tabelado;\nt_Bon = tinv(p,n-k-1);                          * calcula t-tabelado para Método de Bonferroni;\nt_Scheffe = sqrt((k+1)*finv(0.95,k+1,n-k-1));   * calcula t-tabelado para Método de Scheffé;\n\n\nprint 'Exemplo 8.5.2' ,, 'Testes de hipótese H0: Bi = 0 vs Ha: Bi dif 0',,\n      'H01: B1 = 0  ' 't_cal1 =' t1[format=8.4] '     p-valor = ' p_valor_t1[format=10.4],,\n      'H02: B2 = 0  ' 't_cal2 =' t2[format=8.4] '     p-valor = ' p_valor_t2[format=10.4],,,,\n      '----------------------------------------------',\n      'alfa = 5% =&gt; t(0,025; 9 g.l.) ='  t_tab[format=12.4],\n      '----------------------------------------------',,,\n      'Método de Bonferroni', 't(0,0125; 9 g.l.) =' t_Bon[format=12.4],,,,\n      'Método de Scheffé   ', 't-Scheffé         =' t_Scheffe[format=12.4];\nquit;"
  },
  {
    "objectID": "exemplo5.1.html#introdução",
    "href": "exemplo5.1.html#introdução",
    "title": "\n11  ANOVA balanceada com um fator - Estimação\n",
    "section": "\n11.1 Introdução",
    "text": "11.1 Introdução\nO modelo de análise de variância (ANOVA) consiste em comparar as médias de tratamentos para alguma variável resposta. O modelo matemático pode ser representado da seguinte maneira:\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij} \\space \\text{ , para } i = 1,\\dots,a \\space \\text{ e } \\space j = 1,\\dots,n\n\\]\nem que:\n\n\\(y_{ij}\\) é o valor observado na \\(j\\)−ésima repetição do \\(i\\)−ésimo tratamento;\n\\(\\mu\\) é uma constante comum a todas as observações, geralmente a média geral;\n\\(\\tau_i\\) é o efeito do \\(i\\)−ésimo tratamento;\n\\(\\epsilon_{ij}\\) é o erro experimental observado na \\(j\\)−ésima repetição do \\(i\\)−ésimo tratamento.\n\nQuando o modelo é representado na forma matricial \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\), a matriz de delineamento \\(\\mathbf{X}\\) não é de posto completo (posto incompleto), sendo assim um modelo dito superparametrizado. Dessa forma, é necessário adequar a metodologia aplicada nos modelos de regressão.\nNeste primeiro momento, trataremos dos modelos ANOVA balanceados, ou seja, todos os tratamentos apresentam número igual de observações, com um fator de tratamento (one-way model).\nComo exemplo, temos o seguinte caso: Um pesquisador deseja comparar dois aditivos usados para melhorar o desempenho da gasolina. Suponhamos que:\n\nSem aditivos: carro percorre \\(\\mu\\) quilômetros por litro;\nCom aditivo 1: acréscimo de \\(tau_1\\) quilômetros por litro;\nCom aditivo 2: acréscimo de \\(tau_2\\) quilômetros por litro.\n\nConsiderando seis carros idênticos e escolhendo, aleatoriamente, três carros para adicionar o aditivo 1 e três carros para adicionar o aditivo 2, ou seja, três repetições para cada tipo de aditivo, o modelo matemático fica:\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\space i = 1,2 \\space j = 1,2,3\n\\]\nem que \\(y_{ij}\\) é a quilometragem por litro observada no \\(j\\)-ésimo carro que recebeu o \\(i\\)-ésimo aditivo.\nMatricialmente, \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\), temos:\n\\[\n\\begin{cases}\n\\mu + \\tau_1 + \\epsilon_{11} \\\\\n\\mu + \\tau_1 + \\epsilon_{12} \\\\\n\\mu + \\tau_1 + \\epsilon_{13} \\\\\n\\mu + \\tau_2 + \\epsilon_{21} \\\\\n\\mu + \\tau_2 + \\epsilon_{22} \\\\\n\\mu + \\tau_2 + \\epsilon_{23} \\\\\n\\end{cases}\n=\n\\begin{bmatrix}\ny_{11} \\\\ y_{12} \\\\ y_{13} \\\\ y_{21} \\\\ y_{22} \\\\ y_{23}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu \\\\ \\tau_1 \\\\ \\tau_2\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{13} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{23}\n\\end{bmatrix}\n\\]\nNote que a matriz \\(\\mathbf{X}\\), de dimensão \\(6 \\times 3\\), é de posto incompleto (posto(\\(\\mathbf{X}\\)) = 2), assim, não é possível estimar diretamente os parâmetros do vetor \\(\\boldsymbol{\\beta} = [\\mu,\\tau_1,\\tau_2]'\\), pois apresentam infinitas soluções (não apresenta solução única).\nA seguir, serão apresentados métodos para estimar os parâmetros do vetor \\(\\boldsymbol{\\beta}\\), considerando os seguintes valores de \\(y_{ij}\\):\n\n\n\n\nAditivo 1 (y1)\nAditivo 2 (y2)\n\n\n\n14\n18\n\n\n16\n19\n\n\n15\n17"
  },
  {
    "objectID": "exemplo5.1.html#estimação-dos-parâmetros",
    "href": "exemplo5.1.html#estimação-dos-parâmetros",
    "title": "\n11  ANOVA balanceada com um fator - Estimação\n",
    "section": "\n11.2 Estimação dos parâmetros",
    "text": "11.2 Estimação dos parâmetros\nOs dados descritos anteriormente são os seguintes:\n\n\nR\nSAS\n\n\n\n\ny &lt;- as.vector(c(14,16,15,18,19,17))\ny\n\n[1] 14 16 15 18 19 17\n\nX &lt;- matrix(c(\n  1, 1, 0, \n  1, 1, 0,\n  1, 1, 0, \n  1, 0, 1, \n  1, 0, 1, \n  1, 0, 1\n),\nncol = 3, byrow = TRUE\n)\nX\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    1    0\n[3,]    1    1    0\n[4,]    1    0    1\n[5,]    1    0    1\n[6,]    1    0    1\n\n\nA matriz \\(\\mathbf{X'X}\\) (XLX) e o vetor \\(\\mathbf{X'y}\\) (XLy) são calculados a seguir:\n\nXLX &lt;- t(X) %*% X\nXLX\n\n     [,1] [,2] [,3]\n[1,]    6    3    3\n[2,]    3    3    0\n[3,]    3    0    3\n\nXLy &lt;- t(X) %*% y\nXLy\n\n     [,1]\n[1,]   99\n[2,]   45\n[3,]   54\n\n\nCom a matriz \\(\\mathbf{X'X}\\), obtemos o número de observações totais (6) e o número de observações dos tratamentos (3 para cada aditivo). O vetor \\(\\mathbf{X'y}\\) nos retorna o total dos valores observados por tratamento (45 km/L para o aditivo 1 e 54 km/L para o aditivo 2), além do total global (99 km/L).\nPara calcular o posto das matrizes \\(\\mathbf{X}\\) e \\(\\mathbf{X'X}\\), utilizaremos a função ginv() do pacote MASS, que calcula a inversa generalizada de uma matriz.\n\ninstall.packages(\"MASS\")\nlibrary(MASS)\n\nO posto de uma matriz idempotente é o traço do produto de uma matriz pela sua inversa generalizada, ou seja, é a soma dos elementos da diagonal.\n\nrank_X &lt;- sum(diag(X %*% ginv(X)))\nrank_X\n\n[1] 2\n\nrank_XLX &lt;- sum(diag(XLX %*% ginv(XLX)))\nrank_XLX\n\n[1] 2\n\n\nFazendo a diferença entre o número de parâmetros do modelo (p) com o posto da matriz de delineamento \\(\\mathbf{X}\\) (k), confirmamos que se trata de uma matriz de posto incompleto (defRank).\n\n# Número de parâmetros\np &lt;- ncol(X)\np\n\n[1] 3\n\n# Posto(X)\nk &lt;- rank_X\nk\n\n[1] 2\n\n# Deficiência de rank - mostra que é de posto incompleto\ndefRank &lt;- p - k\ndefRank\n\n[1] 1\n\n\nPara estimar o vetor de parâmetros \\(\\boldsymbol{\\beta} = [\\mu,\\tau_1,\\tau_2]'\\), utilizamos o Método dos Mínimos Quadrados Ordinários (MQO) que busca o \\(\\boldsymbol{\\hat\\beta}\\) que minimize a soma de quadrados dos desvios. O sistema de equações normais é dado por:\n\\[\n\\mathbf(X'X) \\boldsymbol{\\hat\\beta} = \\mathbf{X' y}\n\\]\nAqui, considera-se que o modelo matricial \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) tem \\(E(\\mathbf{y}) = \\mathbf{X} \\boldsymbol{\\beta}\\), \\(cov(\\mathbf{y}) = \\sigma^2 \\mathbf{I}\\) e que \\(\\mathbf{X}\\) é \\(n \\times p\\) de posto \\(k &lt; p \\le n\\).\nPara obter uma possível solução de \\(\\boldsymbol{\\hat\\beta}\\), podemos utilizar qualquer inversa generalizada de \\(\\mathbf{(X'X)^-}\\), a partir da seguinte operação:\n\\[\\boldsymbol{\\hat\\beta} = \\mathbf{(X'X)^- X' y}\\]\n\nBeta0 &lt;- ginv(XLX) %*% t(X) %*% y\nBeta0\n\n     [,1]\n[1,]   11\n[2,]    4\n[3,]    7\n\ny_hat0 &lt;- X %*% Beta0\ny_hat0\n\n     [,1]\n[1,]   15\n[2,]   15\n[3,]   15\n[4,]   18\n[5,]   18\n[6,]   18\n\n\nOutro método de estimação de \\(\\boldsymbol{\\hat\\beta}\\) é impondo retrições nos parâmetros (condições marginais). Abordaremos três tipos:\n\nSolução com restrição \\(\\tau_1 = 0\\): é a retrição imposta pelo software R.\n\n\nTR &lt;- c(0, 1, 0)\nTR\n\n[1] 0 1 0\n\nW &lt;- rbind(X, TR)\nW\n\n   [,1] [,2] [,3]\n      1    1    0\n      1    1    0\n      1    1    0\n      1    0    1\n      1    0    1\n      1    0    1\nTR    0    1    0\n\nz &lt;- matrix(c(y, 0), ncol = 1)\nz\n\n     [,1]\n[1,]   14\n[2,]   16\n[3,]   15\n[4,]   18\n[5,]   19\n[6,]   17\n[7,]    0\n\nBeta1 &lt;- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)\nBeta1\n\n     [,1]\n[1,]   15\n[2,]    0\n[3,]    3\n\ny_hat1 &lt;- X %*% Beta1\ny_hat1\n\n     [,1]\n[1,]   15\n[2,]   15\n[3,]   15\n[4,]   18\n[5,]   18\n[6,]   18\n\n\n\nSolução com restrição \\(\\tau_2 = 0\\): é a retrição imposta pelo software SAS.\n\n\nTS &lt;- c(0, 0, 1)\nTS\n\n[1] 0 0 1\n\nW &lt;- rbind(X, TS)\nW\n\n   [,1] [,2] [,3]\n      1    1    0\n      1    1    0\n      1    1    0\n      1    0    1\n      1    0    1\n      1    0    1\nTS    0    0    1\n\nz &lt;- matrix(c(y, 0), ncol = 1)\nz\n\n     [,1]\n[1,]   14\n[2,]   16\n[3,]   15\n[4,]   18\n[5,]   19\n[6,]   17\n[7,]    0\n\nBeta2 &lt;- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)\nBeta2\n\n     [,1]\n[1,]   18\n[2,]   -3\n[3,]    0\n\ny_hat2 &lt;- X %*% Beta2\ny_hat2\n\n     [,1]\n[1,]   15\n[2,]   15\n[3,]   15\n[4,]   18\n[5,]   18\n[6,]   18\n\n\n\nSolução com restrição \\(\\tau_1 + \\tau_2 = 0\\): é a retrição comumente utilizada na estatística experimental.\n\n\nTE &lt;- c(0, 1, 1)\nTE\n\n[1] 0 1 1\n\nW &lt;- rbind(X, TE)\nW\n\n   [,1] [,2] [,3]\n      1    1    0\n      1    1    0\n      1    1    0\n      1    0    1\n      1    0    1\n      1    0    1\nTE    0    1    1\n\nz &lt;- matrix(c(y, 0), ncol = 1)\nz\n\n     [,1]\n[1,]   14\n[2,]   16\n[3,]   15\n[4,]   18\n[5,]   19\n[6,]   17\n[7,]    0\n\nBeta3 &lt;- round(solve(t(W) %*% W) %*% t(W) %*% z, 2)\nBeta3\n\n     [,1]\n[1,] 16.5\n[2,] -1.5\n[3,]  1.5\n\ny_hat3 &lt;- X %*% Beta3\ny_hat3\n\n     [,1]\n[1,]   15\n[2,]   15\n[3,]   15\n[4,]   18\n[5,]   18\n[6,]   18\n\n\nO resumo dos resultados estão descritos a seguir:\n\n\n\nEstimação de beta de acordo com a restrição\n\nInv. Generaliz.\ntau1 = 0\ntau2 = 0\ntau1 + tau2 = 0\n\n\n\n11\n15\n18\n16.5\n\n\n4\n0\n-3\n-1.5\n\n\n7\n3\n0\n1.5\n\n\n\n\n\n\nEstimação de y (y_hat) de acordo com a restrição\n\ny\nInv. Generaliz.\ntau1 = 0\ntau2 = 0\ntau1 + tau2 = 0\n\n\n\n14\n15\n15\n15\n15\n\n\n16\n15\n15\n15\n15\n\n\n15\n15\n15\n15\n15\n\n\n18\n18\n18\n18\n18\n\n\n19\n18\n18\n18\n18\n\n\n17\n18\n18\n18\n18\n\n\n\n\n\n\n\n\nproc iml;\nreset fuzz;\ny = {14,16,15,18,19,17}; * vetor coluna y;\nX = {1 1 0, \n     1 1 0,\n     1 1 0, \n     1 0 1, \n     1 0 1, \n     1 0 1};\nXLX = t(X)*X;\nXLy = t(X)*y;\nprint XLX XLy;\n\nrank_X = trace(X*ginv(X));\nrank_XLX = trace(XLX*ginv(XLX));\nprint rank_X rank_XLX;\n\np = ncol(X);\nk = round(trace(Ginv(XLX)*XLX));  * Cálculo do rank da matriz X;\ndefRank = p-k;\nprint 'Número de parâmetros =' p,,\n      'Posto(X) =' k,,    \n      'Deficiência de rank =' defRank;\n\nBeta0 = Ginv(XLX)*t(X)*y; * Solução usando inversa-G;\ny_hat0 = X*Beta0;\nprint 'Solução com inversa-G:       ' Beta0 y y_hat0;\n\nTR = {0 1 0}; * Restrição do R: t1=0;\nz = y//{0};\nW = X//TR;\nBeta1 = inv(t(W)*W)*t(W)*z;\ny_hat1 = X*Beta1;\nprint 'Solução com restrição: t1=0 (R)        ' Beta1 y_hat1;\n\nTS = {0 0 1}; * Restrição do SAS: t2=0;\nz = y//{0};\nW = X//TS;\nBeta2 = inv(t(W)*W)*t(W)*z;\ny_hat2 = X*Beta2;\nprint 'Solução com restrição: t2=0 (SAS)      ' Beta2 y_hat2;\n\nT = {0 1 1}; * Restrição Estatística Experimental: t1+t2=0;\nz = y//{0};\nW = X//T;\nBeta3 = inv(t(W)*W)*t(W)*z;\ny_hat3 = X*Beta3;\nprint 'Solução com restrição: t1+t2=0 (EstExp)' Beta3 y_hat3;"
  },
  {
    "objectID": "exemplo5.1.html#funções-estimáveis",
    "href": "exemplo5.1.html#funções-estimáveis",
    "title": "\n11  ANOVA balanceada com um fator - Estimação\n",
    "section": "\n11.3 Funções estimáveis",
    "text": "11.3 Funções estimáveis\nUma função \\(\\boldsymbol{\\lambda'\\beta}\\) é estimável no modelo \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) se existe um vetor \\(\\mathbf{a}\\) tal que \\(E(\\mathbf{a'y}) = \\boldsymbol{\\lambda'\\beta}\\).\nA seguir, verificaremos se algumas combinações lineares são estimáveis de forma única.\n\n\nR\nSAS\n\n\n\n\n\n\\(\\beta = \\mu + \\tau_1\\):\n\n\nL1 &lt;- c(1, 1, 0)\nL1\n\n[1] 1 1 0\n\nL1Beta0 &lt;- L1 %*% Beta0\nL1Beta0\n\n     [,1]\n[1,]   15\n\nL1Beta1 &lt;- L1 %*% Beta1\nL1Beta1\n\n     [,1]\n[1,]   15\n\nL1Beta2 &lt;- L1 %*% Beta2\nL1Beta2\n\n     [,1]\n[1,]   15\n\nL1Beta3 &lt;- L1 %*% Beta3\nL1Beta3\n\n     [,1]\n[1,]   15\n\n\nDado que o produto da combinação linear pelos vetores estimados de \\(\\boldsymbol{\\beta}\\) são iguais (invariantes), dizemos que \\(\\beta = \\mu + \\tau_1\\) é estimável. Portanto, independentemente da estimativa do vetor de parâmetros \\(\\boldsymbol{\\beta}\\), as estimativas são iguais.\n\n\n\\(\\beta = \\tau_1 + \\tau_2\\):\n\n\nL2 &lt;- c(0, 1, 1)\nL2\n\n[1] 0 1 1\n\nL2Beta0 &lt;- L2 %*% Beta0\nL2Beta0\n\n     [,1]\n[1,]   11\n\nL2Beta1 &lt;- L2 %*% Beta1\nL2Beta1\n\n     [,1]\n[1,]    3\n\nL2Beta2 &lt;- L2 %*% Beta2\nL2Beta2\n\n     [,1]\n[1,]   -3\n\nL2Beta3 &lt;- L2 %*% Beta3\nL2Beta3\n\n     [,1]\n[1,]    0\n\n\nDado que o produto da combinação linear pelos vetores estimados de \\(\\boldsymbol{\\beta}\\) são diferentes, dizemos que \\(\\beta = \\tau_1 + \\tau_2\\) não é estimável. Portanto, funções não estimáveis não são invariantes e resultam em estimativas diferentes.\n\n\n\nL1 = t({1,1,0}); * mi+t1 é estimável!;\nL1Beta0 = L1*Beta0;\nL1Beta1 = L1*Beta1;\nL1Beta2 = L1*Beta2;\nL1Beta3 = L1*Beta3;\nprint '-------------------------------',\n      'Função estimável: LBeta = mi+t1',\n      '-------------------------------',;\nprint '  Estimativa R:     ' L1Beta1,, \n      '  Estimativa SAS:   ' L1Beta2,,\n      '  Estimativa EstExp:' L1Beta3;\n\nL2 = {0 1 1}; * t1+t2 NÃO é estimável!;\nL2Beta0 = L2*Beta0;\nL2Beta1 = L2*Beta1;\nL2Beta2 = L2*Beta2;\nL2Beta3 = L2*Beta3;\nprint '-----------------------------------------',\n      'Função NÃO estimável: L2Beta = T1 + T2   ',\n      '-----------------------------------------',;\nprint '  Estimativa R:     ' L2Beta1,, \n      '  Estimativa SAS:   ' L2Beta2,, \n      '  Estimativa EstExp:' L2Beta3;\nquit;"
  },
  {
    "objectID": "exemplo5.2.html#estimação-de-parâmetros",
    "href": "exemplo5.2.html#estimação-de-parâmetros",
    "title": "\n12  ANOVA balanceada com um fator - Teste de Hipótese\n",
    "section": "\n12.1 Estimação de parâmetros",
    "text": "12.1 Estimação de parâmetros\n\n\nR\nSAS\n\n\n\n\ny &lt;- as.vector(\n  c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63,\n    20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38,\n    20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27)\n)\ny\n\n [1] 14.29 19.10 19.09 16.25 15.09 16.61 19.63 20.06 20.64 18.00 19.56 19.47\n[13] 19.07 18.38 20.04 26.23 22.74 24.04 23.37 25.02 23.27\n\n\nA matriz de delineamento \\(\\mathbf{X}\\) do modelo matricial \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) é dada por:\n\nX &lt;- matrix(c(\n  rep(1,21),\n  rep(1,7),rep(0,14),\n  rep(0,7),rep(1,7),rep(0,7),\n  rep(0,14),rep(1,7)\n),\nncol = 4, byrow = FALSE)\nX\n\n      [,1] [,2] [,3] [,4]\n [1,]    1    1    0    0\n [2,]    1    1    0    0\n [3,]    1    1    0    0\n [4,]    1    1    0    0\n [5,]    1    1    0    0\n [6,]    1    1    0    0\n [7,]    1    1    0    0\n [8,]    1    0    1    0\n [9,]    1    0    1    0\n[10,]    1    0    1    0\n[11,]    1    0    1    0\n[12,]    1    0    1    0\n[13,]    1    0    1    0\n[14,]    1    0    1    0\n[15,]    1    0    0    1\n[16,]    1    0    0    1\n[17,]    1    0    0    1\n[18,]    1    0    0    1\n[19,]    1    0    0    1\n[20,]    1    0    0    1\n[21,]    1    0    0    1\n\n\nNo objeto kn, guardaremos o número de vlaores observados; no p, o número de variáveis; e no k, o posto de \\(\\mathbf{X}\\).\n\nkn &lt;- length(y)\nkn\n\n[1] 21\n\np &lt;- ncol(X)\np\n\n[1] 4\n\nk &lt;- sum(diag(X %*% ginv(X)))\nk\n\n[1] 3\n\n\nCalculando \\(\\mathbf{X'X}\\) e \\(\\mathbf{X'y}\\), temos:\n\nXLX &lt;- t(X) %*% X\nXLX\n\n     [,1] [,2] [,3] [,4]\n[1,]   21    7    7    7\n[2,]    7    7    0    0\n[3,]    7    0    7    0\n[4,]    7    0    0    7\n\nXLy &lt;- t(X) %*% y\nXLy\n\n       [,1]\n[1,] 419.95\n[2,] 120.06\n[3,] 135.18\n[4,] 164.71\n\n\nCom os resultados anteriores, calcularemos duas estimativas de \\(\\boldsymbol{\\beta}\\):\n\nVetor com as estimativas a partir da inversa generalizada:\n\n\nBeta &lt;- ginv(XLX) %*% XLy\nBeta\n\n          [,1]\n[1,] 14.998214\n[2,]  2.153214\n[3,]  4.313214\n[4,]  8.531786\n\n\n\nVetor com as estimativas a partir da inversa generalizada utilizando a submatriz diagonal:\n\n\nigXLX &lt;- (1/7) * matrix(\n  c(0, 0, 0, 0,\n    0, 1, 0, 0,\n    0, 0, 1, 0,\n    0, 0, 0, 1),\n  ncol = 4, byrow = TRUE\n)\nigXLX |&gt; fractions()\n\n     [,1] [,2] [,3] [,4]\n[1,]   0    0    0    0 \n[2,]   0  1/7    0    0 \n[3,]   0    0  1/7    0 \n[4,]   0    0    0  1/7 \n\nBeta2 &lt;- igXLX %*% XLy\nBeta2\n\n         [,1]\n[1,]  0.00000\n[2,] 17.15143\n[3,] 19.31143\n[4,] 23.53000\n\n\nA partir das duas soluções de \\(\\mathbf{\\beta}'s\\), verificaremos se as seguintes funções são estimáveis:\n\n\n\\(\\beta = \\alpha_1 - \\alpha_2\\):\n\n\nL1 &lt;- c(0, 1, -1, 0)\nL1\n\n[1]  0  1 -1  0\n\nL1Beta &lt;- t(L1) %*% Beta\nL1Beta\n\n      [,1]\n[1,] -2.16\n\nL1Beta2 &lt;- t(L1) %*% Beta2\nL1Beta2\n\n      [,1]\n[1,] -2.16\n\n\n\n\n\\(\\beta = \\alpha_1 + \\alpha_2 + \\alpha_3\\):\n\n\nL2 &lt;- c(0, 1, 1, 1)\nL2\n\n[1] 0 1 1 1\n\nL2Beta &lt;- t(L2) %*% Beta\nL2Beta\n\n         [,1]\n[1,] 14.99821\n\nL2Beta2 &lt;- t(L2) %*% Beta2\nL2Beta2\n\n         [,1]\n[1,] 59.99286\n\n\nComo se pode notar, \\(\\beta = \\alpha_1 - \\alpha_2\\) é uma função estimável e \\(\\beta = \\alpha_1 + \\alpha_2 + \\alpha_3\\) é uma função não estimável.\n\n\n\noptions nodate nocenter ps=1000;\n\nproc iml;\n*reset print;\nreset fuzz;\ny = {14.29,19.10,19.09,16.25,15.09,16.61,19.63,\n     20.06,20.64,18.00,19.56,19.47,19.07,18.38,\n     20.04,26.23,22.74,24.04,23.37,25.02,23.27};\nX = {1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,1 1 0 0,\n     1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,1 0 1 0,\n     1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1,1 0 0 1};\nprint X[format=5.0] y[format=8.2];\nkn = nrow(y); \np = ncol(X); \nk = round(trace(X*ginv(X))); * Calcula o posto de XlinhaX;\nprint 'rank(X) =' k;\nXLX = t(X)*X;\nXLy = t(X)*y;\nprint 'Sistema de equações normais:' XLX XLy;\n\nBeta = ginv(XLX)*XLy;   * Usando inversa generalizada de Moore-Penrose;\n\nigXLX = (1/7)*{0 0 0 0,\n               0 1 0 0, \n               0 0 1 0, \n               0 0 0 1}; \nBeta2 = igXLX*XLy;\nprint 'Duas soluções:' Beta[format=12.4] Beta2[format=12.4];\n\nL = {0, 1,-1,0}; * Lambda da função alfa1-alfa2 (estimável!);\nLBeta = t(L)*Beta;\nLBeta2 = t(L)*Beta2;\nprint 'Função estimável: alfa1-alfa2:',,LBeta[format=12.4] LBeta2[format=12.4];\n\nL = {0,1,1,1}; * Lambda da função alfa1+alfa2+alfa3 (NÃO estimável!);\nLBeta = t(L)*Beta;\nLBeta2 = t(L)*Beta2;\nprint 'Função NÃO estimável: alfa1+alfa2+alfa3:',,LBeta[format=12.4] LBeta2[format=12.4];"
  },
  {
    "objectID": "exemplo5.2.html#teste-de-hipótese",
    "href": "exemplo5.2.html#teste-de-hipótese",
    "title": "\n12  ANOVA balanceada com um fator - Teste de Hipótese\n",
    "section": "\n12.2 Teste de hipótese",
    "text": "12.2 Teste de hipótese\nA seguir, testaremos a seguinte hipótese:\n\\[\n\\begin{align}\nH_0&: \\mu_1 = \\mu_2 = \\mu_3 \\\\\nH_a&: \\text{Pelo menos duas médias diferem entre si}\n\\end{align}\n\\]\nComo \\(\\mu_i = \\mu + \\alpha_i\\), a hipótese anterior é equivalente a seguinte:\n\\[\n\\begin{align}\nH_0&: \\alpha_1 = \\alpha_2 = \\alpha_3 \\\\\nH_a&: \\text{Pelo menos dois tratamentos diferem entre si}\n\\end{align}\n\\]\nUtilizaremos duas abordagens para a realização do teste de hipótese: modelo completo x modelo reduzido e contrastes.\n\n12.2.1 Modelo Completo x Modelo Reduzido\n\n\nR\nSAS\n\n\n\n\nIn &lt;- diag(kn)\nJn &lt;- matrix(1, nrow = kn, ncol = kn)\n\nA soma de quadrados total (SQTotal) é dada por:\n\\[\nSQTotal = \\mathbf{y'} \\left(\\mathbf{I} - \\frac{1}n \\mathbf{J}\\right) \\mathbf{y}\n\\]\nE seus graus de liberdade, pelo traço do produto de sua matriz núcleo pela inversa generalizada.\n\n# Total\nTot &lt;- In - (1 / kn) * Jn\n\nSQTotal &lt;- t(y) %*% Tot %*% y\nSQTotal\n\n         [,1]\n[1,] 202.3126\n\ngl_total &lt;- round(sum(diag(Tot %*% ginv(Tot))))\ngl_total\n\n[1] 20\n\n\nA soma de quadrados para os \\(\\alpha's\\) ajustada para \\(\\mu\\) (\\(SQ(\\alpha \\mid \\mu)\\)) pode ser expressa como uma forma quadrática de \\(\\mathbf{y}\\):\n\\[\nSQ(\\alpha \\mid \\mu) = \\mathbf{y'} \\left[\\mathbf{X(X'X)^-X'} - \\frac{1}n \\mathbf{J}\\right] \\mathbf{y}\n\\]\n\n# Tratamentos\nA &lt;- X %*% ginv(t(X) %*% X) %*% t(X) - (1 / kn) * Jn\n\nSQTrat &lt;- t(y) %*% A %*% y\nSQTrat\n\n         [,1]\n[1,] 147.3456\n\ngl_trat &lt;- round(sum(diag(A %*% ginv(A))))\ngl_trat\n\n[1] 2\n\nQMTrat &lt;- SQTrat / gl_trat\nQMTrat\n\n        [,1]\n[1,] 73.6728\n\n\nA soma de quadrados dos resíduos (SQRes) é dada por:\n\\[\nSQRes = \\mathbf{y'} \\left[\\mathbf{I} - \\mathbf{X(X'X)^-X'}\\right] \\mathbf{y}\n\\]\n\n# Resíduo\nB &lt;- In - X %*% ginv(t(X) %*% X) %*% t(X)\n\nSQRes &lt;- t(y) %*% B %*% y\nSQRes\n\n         [,1]\n[1,] 54.96697\n\ngl_res &lt;- round(sum(diag(B %*% ginv(B))))\ngl_res\n\n[1] 18\n\nQMRes &lt;- SQRes / gl_res\nQMRes\n\n         [,1]\n[1,] 3.053721\n\n\nCom os quadrados médios de tratamento (QMTrat) e dos resíduos (QMRes), calculamos a estatística F, bem como o p-valor.\n\nFcalc &lt;- QMTrat / QMRes\nFcalc\n\n         [,1]\n[1,] 24.12559\n\nftab &lt;- qf(0.95, gl_trat, gl_res)\nftab\n\n[1] 3.554557\n\np_valor &lt;- 1 - pf(Fcalc, gl_trat, gl_res)\np_valor\n\n               [,1]\n[1,] 0.000008066967\n\n\nA seguir, os resultado estão descritos no quadro de ANOVA:\n\n\n\nANOVA para os dados de ácido ascórbico\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nTratamento\n2\n147.346\n73.673\n24.126\n3.555\n0\n\n\nResíduo\n18\n54.967\n3.054\n\n\n\n\n\nTotal\n20\n202.313\n\n\n\n\n\n\n\n\n\nDado que \\(F_{cal} &gt; F_{tab}\\), para \\(F_{(0,05;2;18)}\\), rejeita-se a hipótese \\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\), indicando que as médias de pelo menos dois métodos de congelamento diferem entre si.\n\n\n\nT = I(kn)-(1/kn)*J(kn,kn,1);    * Matriz núcleo;\nSQTotal = t(y)*T*y;             * Calcula SQTotal corrigida pela média;\ngl_total = round(trace(T*ginv(T))); \n\nA = X*ginv(t(X)*X)*t(X) -(1/kn)*J(kn,kn,1);     * Matriz núcleo;\nSQTrat = t(y)*A*y;                              * Calcula SQTrat;\ngl_trat = round(trace(A*ginv(A)));\nQMTrat = SQTrat/gl_trat;\n\nB = I(kn) - X*ginv(t(X)*X)*t(X);    * Matriz núcleo;\nSQRes = t(y)*B*y;                   * Calcula SQResiduo;\ngl_res = round(trace(B*ginv(B)));\nQMRes = SQRes/gl_res;\n\nFcalc = QMTrat/QMRes;\np_valor = 1-cdf('F',Fcalc,gl_trat,gl_Res);\n\nprint 'TABELA 12.3 - ANOVA para os dados de ácido ascórbico da Tabela 13.2',; \nprint 'Método ' gl_trat  SQTrat[format=12.4] QMTrat[format=12.4] Fcalc[format=10.4] p_valor[format=12.4],,\n      'Resíduo' gl_res   SQRes[format=12.4]  QMRes[format=12.4],,\n      'Total  ' gl_total SQTotal[format=12.4];\n\n\n\n\n\n12.2.2 Contrastes\nCaso 1: As linhas são linearmente independentes (l.i.) e ortogonais\nConsiderando os constrastes ortogonais \\(2\\mu_1 - \\mu_2 - \\mu_3\\) e \\(\\mu_2 - \\mu_3\\), podemos representá-los da seguinte maneira:\n\\[\n\\begin{align}\n2\\mu_1 - \\mu_2 - \\mu_3 &= 2\\alpha_1 - \\alpha_2 - \\alpha_3 = [0,2,-1,-1]\\boldsymbol{\\beta} = \\mathbf{c'_1}\\boldsymbol{\\beta}\\\\\n\\mu_2 - \\mu_3 &= \\alpha_2 - \\alpha_3 = [0,0,1,-1]\\boldsymbol{\\beta} = \\mathbf{c'_2}\\boldsymbol{\\beta}\n\\end{align}\n\\]\nAs hipóteses \\(H_{01}: \\mathbf{c'_1}\\boldsymbol{\\beta} = 0\\) e \\(H_{02}: \\mathbf{c'_2}\\boldsymbol{\\beta} = 0\\) comparam a média do primeiro tratamento com a dos outros dois e a média do segundo tratamento com a do terceiro, respectivamente.\n\\[\n\\begin{align}\nH_{01} &:\\mathbf{c'_1}\\boldsymbol{\\beta} = 0 \\Leftrightarrow \\mu_1 = \\frac{\\mu_2 + \\mu_3}2 \\\\\nH_{02} &: \\mathbf{c'_2}\\boldsymbol{\\beta} = 0 \\Leftrightarrow \\mu_2 = \\mu_3\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC1 &lt;- c(0, 2, -1, -1)\nC1Beta &lt;- C1 %*% Beta\nC1Beta\n\n          [,1]\n[1,] -8.538571\n\nC2 &lt;- c(0, 0, 1, -1)\nC2Beta &lt;- C2 %*% Beta\nC2Beta\n\n          [,1]\n[1,] -4.218571\n\n# Partição da SQTrat\nSQC1Beta &lt;- t(C1Beta) %*% solve(t(C1) %*% ginv(t(X) %*% X) %*% C1) %*% (C1Beta)\nSQC1Beta\n\n        [,1]\n[1,] 85.0584\n\nSQC2Beta &lt;- t(C2Beta) %*% solve(t(C2) %*% ginv(t(X) %*% X) %*% C2) %*% (C2Beta)\nSQC2Beta\n\n         [,1]\n[1,] 62.28721\n\nSomaSQ &lt;- SQC1Beta + SQC2Beta\nSomaSQ\n\n         [,1]\n[1,] 147.3456\n\nSQTrat\n\n         [,1]\n[1,] 147.3456\n\n# QM\nQMC1Beta &lt;- SQC1Beta / 1\nQMC1Beta\n\n        [,1]\n[1,] 85.0584\n\nQMC2Beta &lt;- SQC2Beta / 1\nQMC2Beta\n\n         [,1]\n[1,] 62.28721\n\n# F e p-valor\nFC1 &lt;- QMC1Beta / QMRes\nFC1\n\n         [,1]\n[1,] 27.85402\n\nFC2 &lt;- QMC2Beta / QMRes\nFC2\n\n         [,1]\n[1,] 20.39715\n\nftab &lt;- qf(0.95, 1, 18)\nftab\n\n[1] 4.413873\n\np_valorC1 &lt;- 1 - pf(FC1, 1, gl_res)\np_valorC1\n\n              [,1]\n[1,] 0.00005110228\n\np_valorC2 &lt;- 1 - pf(FC2, 1, gl_res)\np_valorC2\n\n            [,1]\n[1,] 0.000267208\n\n\n\n\n\nANOVA para contrastes ortogonais dos dados de ácido ascórbico\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nContraste c'1\n1\n85.058\n85.058\n27.854\n4.414\n0.00005\n\n\nContraste c'2\n1\n62.287\n62.287\n20.397\n4.414\n0.00027\n\n\nResíduo\n18\n54.967\n3.054\n\n\n\n\n\nTotal\n20\n202.313\n\n\n\n\n\n\n\n\n\nAmbos os \\(F_{cal}\\) são superiores ao valor tabelado \\(F_{(0,05;1,18)}\\). Assim, Ambas as hipóteses \\(H_{01}\\) e \\(H_{02}\\) são rejeitadas e se conclui que:\n\\[\n\\mu_1 \\ne \\frac{\\mu_2 + \\mu_3}2 \\quad e \\quad \\mu_2 \\ne \\mu_3\n\\]\n\n\n\nC1 ={0 2 -1 -1};\nC2 ={0 0  1 -1};\nC1Beta = C1*Beta;\nC2Beta = C2*Beta;\nSQC1Beta = t(C1*Beta)*inv(C1*ginv(t(X)*X)*t(C1))*C1*Beta;\nSQC2Beta = t(C2*Beta)*inv(C2*ginv(t(X)*X)*t(C2))*C2*Beta;\nSoma = SQC1Beta + SQC2Beta;\nprint 'Partição da SQTrat usando k=2 contrastes l.i. e ortogonais',,\n      'SQcontraste1 = ' SQC1Beta[format=8.4],, 'SQcontraste2 = ' SQC2Beta[format=8.4],,\n      'Soma SQs     = ' Soma[format=8.4] '    SQTrat = ' SQTrat[format=8.4];\n\n\n\n\nCaso 2: As linhas são l.i. e não ortogonais\nAgora, consideraremos os constrastes não ortogonais \\(2\\mu_1 - \\mu_2 - \\mu_3\\) e \\(\\mu_1 - \\mu_3\\), podemos representá-los da seguinte maneira:\n\\[\n\\begin{align}\n2\\mu_1 - \\mu_2 - \\mu_3 &= 2\\alpha_1 - \\alpha_2 - \\alpha_3 = [0,2,-1,-1]\\boldsymbol{\\beta} = \\mathbf{c'_1}\\boldsymbol{\\beta}\\\\\n\\mu_1 - \\mu_3 &= \\alpha_1 - \\alpha_3 = [0,1,0,-1]\\boldsymbol{\\beta} = \\mathbf{c'_3}\\boldsymbol{\\beta}\n\\end{align}\n\\]\nAs hipóteses \\(H_{01}: \\mathbf{c'_1}\\boldsymbol{\\beta} = 0\\) e \\(H_{03}: \\mathbf{c'_3}\\boldsymbol{\\beta} = 0\\) são:\n\\[\n\\begin{align}\nH_{01} &:\\mathbf{c'_1}\\boldsymbol{\\beta} = 0 \\Leftrightarrow \\mu_1 = \\frac{\\mu_2 + \\mu_3}2 \\\\\nH_{03} &: \\mathbf{c'_3}\\boldsymbol{\\beta} = 0 \\Leftrightarrow \\mu_1 = \\mu_3\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nC1 &lt;- c(0, 2, -1, -1)\nC1Beta &lt;- C1 %*% Beta\nC1Beta\n\n          [,1]\n[1,] -8.538571\n\nC3 &lt;- c(0, 1, 0, -1)\nC3Beta &lt;- C3 %*% Beta\nC3Beta\n\n          [,1]\n[1,] -6.378571\n\n# Partição da SQTrat\nSQC1Beta &lt;- t(C1Beta) %*% solve(t(C1) %*% ginv(t(X) %*% X) %*% C1) %*% (C1Beta)\nSQC1Beta\n\n        [,1]\n[1,] 85.0584\n\nSQC3Beta &lt;- t(C3Beta) %*% solve(t(C3)%*% ginv(t(X) %*% X) %*% C3) %*% (C3Beta)\nSQC3Beta\n\n         [,1]\n[1,] 142.4016\n\n# QM\nQMC1Beta &lt;- SQC1Beta / 1\nQMC1Beta\n\n        [,1]\n[1,] 85.0584\n\nQMC3Beta &lt;- SQC3Beta / 1\nQMC3Beta\n\n         [,1]\n[1,] 142.4016\n\n# F e p-valor\nFC1 &lt;- QMC1Beta / QMRes\nFC1\n\n         [,1]\n[1,] 27.85402\n\nFC3 &lt;- QMC3Beta / QMRes\nFC3\n\n         [,1]\n[1,] 46.63217\n\nftab &lt;- qf(0.95, 1, 18)\nftab\n\n[1] 4.413873\n\np_valorC1 &lt;- 1 - pf(FC1, 1, gl_res)\np_valorC1\n\n              [,1]\n[1,] 0.00005110228\n\np_valorC3 &lt;- 1 - pf(FC3, 1, gl_res)\np_valorC3\n\n               [,1]\n[1,] 0.000002160348\n\n\n\n\n\nANOVA para contrastes não ortogonais dos dados de ácido ascórbico\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nContraste c'1\n1\n85.058\n85.058\n27.854\n4.414\n0.00005\n\n\nContraste c'2\n1\n142.402\n142.402\n46.632\n4.414\n0.00000\n\n\nResíduo\n18\n54.967\n3.054\n\n\n\n\n\nTotal\n20\n202.313\n\n\n\n\n\n\n\n\n\nAmbos os \\(F_{cal}\\) são superiores ao valor tabelado \\(F_{(0,05;1,18)}\\). Assim, Ambas as hipóteses \\(H_{01}\\) e \\(H_{03}\\) são rejeitadas e se conclui que:\n\\[\n\\mu_1 \\ne \\frac{\\mu_2 + \\mu_3}2 \\quad e \\quad \\mu_1 \\ne \\mu_3\n\\]\nContudo, nota-se que a soma das somas de quadrados dos contrastes não ortogonais não resulta na soma de quadrados dos tratamentos:\n\nSomaSQ &lt;- SQC1Beta + SQC3Beta\nSomaSQ\n\n       [,1]\n[1,] 227.46\n\nSQTrat\n\n         [,1]\n[1,] 147.3456\n\n\n\n\nSoma das SQ de contrastes não ortogonais = 227.46 \n SQTrat = 147.3456\n\n\n\n\n\nC1 ={0  2 -1 -1};\nC2 ={0  1  0 -1};\nC1Beta = C1*Beta;\nC2Beta = C2*Beta;\nSQC1Beta = t(C1*Beta)*inv(C1*ginv(t(X)*X)*t(C1))*C1*Beta;\nSQC2Beta = t(C2*Beta)*inv(C2*ginv(t(X)*X)*t(C2))*C2*Beta;\nSoma = SQC1Beta + SQC2Beta;\nprint 'Partição da SQTrat usando contrastes l.i. mas não ortogonais',,\n      'SQcontraste1 = ' SQC1Beta[format=8.4],, 'SQcontraste2 = 'SQC2Beta[format=8.4],,\n      'Soma SQs     = ' Soma[format=8.4] '    SQTrat = ' SQTrat[format=8.4];\nquit;"
  },
  {
    "objectID": "exemplo5.3.html#introdução",
    "href": "exemplo5.3.html#introdução",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.1 Introdução",
    "text": "13.1 Introdução\nNesta seção, será apresentada a análise de variância de modelos balanceados com dois fatores de tratamento (two-way ANOVA) e com interação. Seu modelo pode ser representado como:\n\\[\n\\begin{align}\ny_{ijk} &= \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ijk} \\\\\ni = 1,2,\\dots,a & \\quad\\quad j = 1,2,\\dots,b \\quad\\quad k = 1,2,\\dots,n\n\\end{align}\n\\]\n\n\\(\\alpha_i\\) é o efeito do \\(i\\)-ésimo nível do fator \\(A\\);\n\\(\\beta_j\\) é o efeito do \\(j\\)-ésimo nível do fator \\(B\\);\n\\(\\gamma_{ij}\\) é o efeito da interação entre o \\(i\\)-ésimo nível do fator \\(A\\) e o \\(j\\)-ésimo nível do fator \\(B\\).\n\nComo suposições, assumiremos:\n\n\\(E(\\epsilon_{ijk}) = 0\\) para todo \\(i,j,k\\);\n\\(var(\\epsilon_{ijk}) = \\sigma^2\\) para todo \\(i,j,k\\);\n\\(cov(\\epsilon_{ijk}, \\epsilon_{i'j'k'}) = 0\\) para todo \\((i,j,k) \\ne (i',j',k')\\);\n\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\) para todo \\(i,j,k\\).\n\nDessa maneira, podemos reescrever o modelo balanceado na forma reparametrizada:\n\\[\n\\begin{align}\ny_{ijk} &= \\mu_{ij} + \\epsilon_{ijk} \\\\\ni = 1,2,\\dots,a \\quad\\quad j &= 1,2,\\dots,b \\quad\\quad k = 1,2,\\dots,n\n\\end{align}\n\\]\nEm que \\(\\mu_{ij} = E(y_{ijk})\\) é a média de uma observação \\(k\\) na casela \\((ij)\\).\nPara ilustrar, considere o seguinte exemplo: O conteúdo da mistura de três tipos de queijo produzidos por dois métodos foi anotado por Marcuse (1949) (formato alterado). Duas peças de queijo foram medidas para cada tipo e cada método. Designando Método como o fator A e Tipo como o fator B, então a = 2, b = 3 e n = 2.\n\n\nMétodo \\ Tipo\n1\n2\n3\n\n\n\n1\n39,02\n35,74\n37,02\n\n\n1\n38,79\n35,41\n36,00\n\n\n2\n38,96\n35,58\n35,70\n\n\n2\n39,01\n35,52\n36,04\n\n\n\nNovamente, utilizaremos a função ginv() do pacote MASS, que calcula a inversa generalizada de uma matriz.\n\ninstall.packages(\"MASS\")\nlibrary(MASS)"
  },
  {
    "objectID": "exemplo5.3.html#estimação-dos-parâmetros",
    "href": "exemplo5.3.html#estimação-dos-parâmetros",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.2 Estimação dos parâmetros",
    "text": "13.2 Estimação dos parâmetros\n\n\nR\nSAS\n\n\n\n\ny &lt;- c(39.02, 38.79, 35.74, 35.41, 37.02, 36.00, 38.96, 39.01, 35.58, 35.52, 35.70, 36.04)\ny\n\n [1] 39.02 38.79 35.74 35.41 37.02 36.00 38.96 39.01 35.58 35.52 35.70 36.04\n\nX &lt;- matrix(c(\n  1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n  1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n  1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n  1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n  1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n  1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n  1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n  1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n  1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n  1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n  1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n  1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1\n), \nnrow = 12, byrow = TRUE)\nX\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]    1    1    0    1    0    0    1    0    0     0     0     0\n [2,]    1    1    0    1    0    0    1    0    0     0     0     0\n [3,]    1    1    0    0    1    0    0    1    0     0     0     0\n [4,]    1    1    0    0    1    0    0    1    0     0     0     0\n [5,]    1    1    0    0    0    1    0    0    1     0     0     0\n [6,]    1    1    0    0    0    1    0    0    1     0     0     0\n [7,]    1    0    1    1    0    0    0    0    0     1     0     0\n [8,]    1    0    1    1    0    0    0    0    0     1     0     0\n [9,]    1    0    1    0    1    0    0    0    0     0     1     0\n[10,]    1    0    1    0    1    0    0    0    0     0     1     0\n[11,]    1    0    1    0    0    1    0    0    0     0     0     1\n[12,]    1    0    1    0    0    1    0    0    0     0     0     1\n\n\nO modelo matricial \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) é dado por:\n\\[\n\\begin{cases}\ny_{111} = \\mu + \\alpha_1 + \\beta_1 + \\gamma_{11}\\\\\ny_{112} = \\mu + \\alpha_1 + \\beta_1 + \\gamma_{11}\\\\\ny_{121} = \\mu + \\alpha_1 + \\beta_2 + \\gamma_{12}\\\\\ny_{122} = \\mu + \\alpha_1 + \\beta_2 + \\gamma_{12}\\\\\ny_{131} = \\mu + \\alpha_1 + \\beta_3 + \\gamma_{21}\\\\\ny_{132} = \\mu + \\alpha_1 + \\beta_3 + \\gamma_{21}\\\\\ny_{211} = \\mu + \\alpha_2 + \\beta_1 + \\gamma_{22}\\\\\ny_{212} = \\mu + \\alpha_2 + \\beta_1 + \\gamma_{22}\\\\\ny_{221} = \\mu + \\alpha_2 + \\beta_2 + \\gamma_{31}\\\\\ny_{222} = \\mu + \\alpha_2 + \\beta_2 + \\gamma_{31}\\\\\ny_{231} = \\mu + \\alpha_2 + \\beta_3 + \\gamma_{32}\\\\\ny_{232} = \\mu + \\alpha_2 + \\beta_3 + \\gamma_{32}\n\\end{cases}\n\\]\n\\[\n\\begin{bmatrix}\ny_{111} \\\\ y_{112} \\\\ y_{121} \\\\ y_{122} \\\\ y_{131} \\\\ y_{132} \\\\ y_{211} \\\\ y_{212} \\\\ y_{221} \\\\ y_{222} \\\\ y_{231} \\\\ y_{232}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n39,02 \\\\ 38,79 \\\\ 35,74 \\\\ 35,41 \\\\ 37,02 \\\\ 36,00 \\\\ 38,96 \\\\ 39,01 \\\\ 35,58 \\\\ 35,52 \\\\ 35,70 \\\\ 36,04\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\gamma_{11} \\\\ \\gamma_{12} \\\\ \\gamma_{21} \\\\ \\gamma_{22} \\\\ \\gamma_{31} \\\\ \\gamma_{32}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_{111} \\\\ \\epsilon_{112} \\\\ \\epsilon_{121} \\\\ \\epsilon_{122} \\\\ \\epsilon_{131} \\\\ \\epsilon_{132} \\\\ \\epsilon_{211} \\\\ \\epsilon_{212} \\\\ \\epsilon_{221} \\\\ \\epsilon_{222} \\\\ \\epsilon_{231} \\\\ \\epsilon_{232}\n\\end{bmatrix}\n\\]\nNota-se que o modelo apresenta 12 parâmetros e o posto(\\(\\mathbf{X}\\)) = 6, ou seja, a matriz \\(\\mathbf{X}\\) é de posto incompleto.\n\nrank_X &lt;- sum(diag(ginv(X) %*% X))\nrank_X\n\n[1] 6\n\nnpar &lt;- ncol(X)    # número de parâmetros\nnpar\n\n[1] 12\n\n\n\na &lt;- 2    # níveis do fator A\nb &lt;- 3    # níveis do fator B\nn &lt;- 2    # número de repeticões\n\nabn &lt;- a * b * n    # número total de observacões\nabn\n\n[1] 12\n\nX0 &lt;- X[, 1]        # constante\nX0\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1\n\nXA &lt;- X[, 2:3]      # fator A\nXA\n\n      [,1] [,2]\n [1,]    1    0\n [2,]    1    0\n [3,]    1    0\n [4,]    1    0\n [5,]    1    0\n [6,]    1    0\n [7,]    0    1\n [8,]    0    1\n [9,]    0    1\n[10,]    0    1\n[11,]    0    1\n[12,]    0    1\n\nXB &lt;- X[, 4:6]      # fator B\nXB\n\n      [,1] [,2] [,3]\n [1,]    1    0    0\n [2,]    1    0    0\n [3,]    0    1    0\n [4,]    0    1    0\n [5,]    0    0    1\n [6,]    0    0    1\n [7,]    1    0    0\n [8,]    1    0    0\n [9,]    0    1    0\n[10,]    0    1    0\n[11,]    0    0    1\n[12,]    0    0    1\n\nXAB &lt;- X[, 7:12]    # combinacão dos níveis dos dois fatores\nXAB\n\n      [,1] [,2] [,3] [,4] [,5] [,6]\n [1,]    1    0    0    0    0    0\n [2,]    1    0    0    0    0    0\n [3,]    0    1    0    0    0    0\n [4,]    0    1    0    0    0    0\n [5,]    0    0    1    0    0    0\n [6,]    0    0    1    0    0    0\n [7,]    0    0    0    1    0    0\n [8,]    0    0    0    1    0    0\n [9,]    0    0    0    0    1    0\n[10,]    0    0    0    0    1    0\n[11,]    0    0    0    0    0    1\n[12,]    0    0    0    0    0    1\n\n\nA seguir, estimaremos o vetor de parâmetros \\(\\boldsymbol{\\beta}\\) usando a inversa generalizada de Moore-Penrose e usando a inversa generalizada de \\(X'X\\).\n\nEstimação de \\(\\boldsymbol{\\beta}\\) usando inversa generalizada de Moore-Penrose:\n\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\mathbf{(X'X)^- X'y}\n\\]\n\nBeta &lt;- ginv(t(X) %*% X) %*% t(X) %*% y\nBeta\n\n           [,1]\n [1,] 18.449583\n [2,]  9.297917\n [3,]  9.151667\n [4,]  7.513750\n [5,]  5.258750\n [6,]  5.677083\n [7,]  3.643750\n [8,]  2.568750\n [9,]  3.085417\n[10,]  3.870000\n[11,]  2.690000\n[12,]  2.591667\n\n\n\nEstimação de \\(\\boldsymbol{\\beta}\\) usando inversa generalizada (Searle) de \\(\\mathbf{X'X}\\):\n\nUma inversa generalizada de \\(\\mathbf{X'X}\\) é dada por:\n\\[\n(\\mathbf{X'X})^- = \\frac{1}2\n\\begin{bmatrix}\n\\mathbf{0} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{I_6}\n\\end{bmatrix}\n\\]\nUtilizando esta inversa generalizada no sistema de equações normais \\(\\hat{\\boldsymbol{\\beta}} = \\mathbf{(X'X)^- X'y}\\), obtemos a seguinte estimativa:\n\nXLX &lt;- t(X) %*% X\nXLX\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]   12    6    6    4    4    4    2    2    2     2     2     2\n [2,]    6    6    0    2    2    2    2    2    2     0     0     0\n [3,]    6    0    6    2    2    2    0    0    0     2     2     2\n [4,]    4    2    2    4    0    0    2    0    0     2     0     0\n [5,]    4    2    2    0    4    0    0    2    0     0     2     0\n [6,]    4    2    2    0    0    4    0    0    2     0     0     2\n [7,]    2    2    0    2    0    0    2    0    0     0     0     0\n [8,]    2    2    0    0    2    0    0    2    0     0     0     0\n [9,]    2    2    0    0    0    2    0    0    2     0     0     0\n[10,]    2    0    2    2    0    0    0    0    0     2     0     0\n[11,]    2    0    2    0    2    0    0    0    0     0     2     0\n[12,]    2    0    2    0    0    2    0    0    0     0     0     2\n\niXLX &lt;- (1 / 2) * kronecker(\n  matrix(c(0,0,0,1), byrow = TRUE, ncol = 2), diag(6)\n)\nfractions(iXLX)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n [1,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [2,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [3,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [4,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [5,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [6,]   0    0    0    0    0    0    0    0    0    0     0     0  \n [7,]   0    0    0    0    0    0  1/2    0    0    0     0     0  \n [8,]   0    0    0    0    0    0    0  1/2    0    0     0     0  \n [9,]   0    0    0    0    0    0    0    0  1/2    0     0     0  \n[10,]   0    0    0    0    0    0    0    0    0  1/2     0     0  \n[11,]   0    0    0    0    0    0    0    0    0    0   1/2     0  \n[12,]   0    0    0    0    0    0    0    0    0    0     0   1/2  \n\nBetag &lt;- iXLX %*% t(X) %*% y\nBetag\n\n        [,1]\n [1,]  0.000\n [2,]  0.000\n [3,]  0.000\n [4,]  0.000\n [5,]  0.000\n [6,]  0.000\n [7,] 38.905\n [8,] 35.575\n [9,] 36.510\n[10,] 38.985\n[11,] 35.550\n[12,] 35.870\n\n\nA tabela abaixo traz as duas estimativas de \\(\\boldsymbol{\\beta}\\) calculadas anteriormente.\n\n\n\n\n\nbeta_mp\nbeta_g\n\n\n\n$\\hat{\\mu}$\n18.4496\n0.000\n\n\n$\\hat{\\alpha_1}$\n9.2979\n0.000\n\n\n$\\hat{\\alpha_2}$\n9.1517\n0.000\n\n\n$\\hat{\\beta_1}$\n7.5137\n0.000\n\n\n$\\hat{\\beta_2}$\n5.2587\n0.000\n\n\n$\\hat{\\beta_3}$\n5.6771\n0.000\n\n\n$\\hat{\\gamma_{11}}$\n3.6437\n38.905\n\n\n$\\hat{\\gamma_{12}}$\n2.5687\n35.575\n\n\n$\\hat{\\gamma_{21}}$\n3.0854\n36.510\n\n\n$\\hat{\\gamma_{22}}$\n3.8700\n38.985\n\n\n$\\hat{\\gamma_{31}}$\n2.6900\n35.550\n\n\n$\\hat{\\gamma_{32}}$\n2.5917\n35.870\n\n\n\n\n\n\n\n\nproc iml;\nreset fuzz;\ny = {39.02,38.79,35.74,35.41,37.02,36.00,38.96,39.01,35.58,35.52,35.70,36.04};\nX = {1 1 0 1 0 0 1 0 0 0 0 0,\n     1 1 0 1 0 0 1 0 0 0 0 0,\n     1 1 0 0 1 0 0 1 0 0 0 0,\n     1 1 0 0 1 0 0 1 0 0 0 0,\n     1 1 0 0 0 1 0 0 1 0 0 0,\n     1 1 0 0 0 1 0 0 1 0 0 0,\n     1 0 1 1 0 0 0 0 0 1 0 0,\n     1 0 1 1 0 0 0 0 0 1 0 0,\n     1 0 1 0 1 0 0 0 0 0 1 0,\n     1 0 1 0 1 0 0 0 0 0 1 0,\n     1 0 1 0 0 1 0 0 0 0 0 1,\n     1 0 1 0 0 1 0 0 0 0 0 1};\nprint X[format=4.0] y[format=12.2];\n\nrank_X = round(trace(ginv(X)*X));   * Calcula o posto da matriz X;\nnpar = ncol(X);\na=2;    * Número de níveis do fator A;\nb=3;    * Número de níveis do fator B;\nn=2;    * Número de repetições;\nabn = a*b*n;    * Número total de observações;\nX0 = X[,1];\nXA = X[,2:3];\nXB = X[,4:6];\nXAB = X[,7:12];\nprint X[format=8.0],,XA[format=8.0],,XB[format=8.0],,XAB[format=8.0];\n* ----------------------------------------------------------------------------;\n\n* Estima Beta usando inversa de Moore-Penrose;\nBetaMP = ginv(t(X)*X)*t(X)*y;       * Beta usando invG (Moore Penrose));\nXLX = t(X)*X;\niXLX = (1/2)*{0 0, 0 1}@I(6);       \nBetaG = iXLX*t(X)*y;                * Beta usando invG mais simples (Searle);\n\nprint BetaMP BetaG;\n\n* ----------------------------------------------------------------------------;"
  },
  {
    "objectID": "exemplo5.3.html#teste-de-hipótese",
    "href": "exemplo5.3.html#teste-de-hipótese",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.3 Teste de hipótese",
    "text": "13.3 Teste de hipótese\n\n\nR\nSAS\n\n\n\n\nIn &lt;- diag(abn)\nJn &lt;- matrix(1, nrow = abn, ncol = abn)\n\n\n# Total\nTot &lt;- In - (1 / abn) * Jn\n\nSQTotal &lt;- t(y) %*% Tot %*% y\nSQTotal\n\n         [,1]\n[1,] 26.97869\n\ngl_total &lt;- round(sum(diag(Tot %*% ginv(Tot))))\ngl_total\n\n[1] 11\n\n\n\n# Resíduo\nPR &lt;- In - X %*% ginv(t(X) %*% X) %*% t(X)\n\nSQRes &lt;- t(y) %*% PR %*% y\nSQRes\n\n        [,1]\n[1,] 0.66195\n\ngl_res &lt;- round(sum(diag(PR %*% ginv(PR))))\ngl_res\n\n[1] 6\n\nQMRes &lt;- SQRes / gl_res\nQMRes\n\n         [,1]\n[1,] 0.110325\n\n\n\n# Interação AxB\nX1 &lt;- cbind(X0, XA, XB)\nX1\n\n      X0          \n [1,]  1 1 0 1 0 0\n [2,]  1 1 0 1 0 0\n [3,]  1 1 0 0 1 0\n [4,]  1 1 0 0 1 0\n [5,]  1 1 0 0 0 1\n [6,]  1 1 0 0 0 1\n [7,]  1 0 1 1 0 0\n [8,]  1 0 1 1 0 0\n [9,]  1 0 1 0 1 0\n[10,]  1 0 1 0 1 0\n[11,]  1 0 1 0 0 1\n[12,]  1 0 1 0 0 1\n\nPAB &lt;- X %*% ginv(t(X) %*% X) %*% t(X) - X1 %*% ginv(t(X1) %*% X1) %*% t(X1)\nPAB\n\n             [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n [1,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [2,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [3,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [4,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [5,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [6,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [7,] -0.16666667 -0.16666667  0.08333333  0.08333333  0.08333333  0.08333333\n [8,] -0.16666667 -0.16666667  0.08333333  0.08333333  0.08333333  0.08333333\n [9,]  0.08333333  0.08333333 -0.16666667 -0.16666667  0.08333333  0.08333333\n[10,]  0.08333333  0.08333333 -0.16666667 -0.16666667  0.08333333  0.08333333\n[11,]  0.08333333  0.08333333  0.08333333  0.08333333 -0.16666667 -0.16666667\n[12,]  0.08333333  0.08333333  0.08333333  0.08333333 -0.16666667 -0.16666667\n             [,7]        [,8]        [,9]       [,10]       [,11]       [,12]\n [1,] -0.16666667 -0.16666667  0.08333333  0.08333333  0.08333333  0.08333333\n [2,] -0.16666667 -0.16666667  0.08333333  0.08333333  0.08333333  0.08333333\n [3,]  0.08333333  0.08333333 -0.16666667 -0.16666667  0.08333333  0.08333333\n [4,]  0.08333333  0.08333333 -0.16666667 -0.16666667  0.08333333  0.08333333\n [5,]  0.08333333  0.08333333  0.08333333  0.08333333 -0.16666667 -0.16666667\n [6,]  0.08333333  0.08333333  0.08333333  0.08333333 -0.16666667 -0.16666667\n [7,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [8,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [9,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[10,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[11,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n[12,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n\nSQAB &lt;- t(y) %*% PAB %*% y\nSQAB\n\n        [,1]\n[1,] 0.30255\n\nglAB &lt;- round(sum(diag(ginv(PAB) %*% PAB)))\nglAB\n\n[1] 2\n\nQMAB &lt;- SQAB / glAB\nQMAB\n\n         [,1]\n[1,] 0.151275\n\nFAB &lt;- QMAB / QMRes\nFAB\n\n         [,1]\n[1,] 1.371176\n\nftabAB &lt;- qf(0.95, glAB, gl_res)\nftabAB\n\n[1] 5.143253\n\np_valorAB &lt;- 1 - pf(FAB, glAB, gl_res)\np_valorAB\n\n          [,1]\n[1,] 0.3232726\n\n\n\n# Fator A\nPA &lt;- XA %*% ginv(t(XA) %*% XA) %*% t(XA) - Jn/abn\nPA\n\n             [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n [1,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [2,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [3,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [4,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [5,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [6,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [7,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [8,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [9,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n[10,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n[11,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n[12,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n             [,7]        [,8]        [,9]       [,10]       [,11]       [,12]\n [1,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [2,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [3,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [4,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [5,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [6,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [7,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [8,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n [9,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n[10,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n[11,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n[12,]  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333  0.08333333\n\nSQA &lt;- t(y) %*% PA %*% y\nSQA\n\n         [,1]\n[1,] 0.114075\n\nglA &lt;- round(sum(diag(ginv(PA) %*% PA)))\nglA\n\n[1] 1\n\nQMA &lt;- SQA / glA\nQMA\n\n         [,1]\n[1,] 0.114075\n\nFA &lt;- QMA / QMRes\nFA\n\n        [,1]\n[1,] 1.03399\n\nftabA &lt;- qf(0.95, glA, gl_res)\nftabA\n\n[1] 5.987378\n\np_valorA &lt;- 1 - pf(FA, glA, gl_res)\np_valorA\n\n          [,1]\n[1,] 0.3484596\n\n\n\n# Fator B\nPB &lt;- XB %*% ginv(t(XB) %*% XB) %*% t(XB) - Jn/abn\nPB\n\n             [,1]        [,2]        [,3]        [,4]        [,5]        [,6]\n [1,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [2,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [3,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [4,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [5,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [6,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [7,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [8,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [9,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[10,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[11,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n[12,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n             [,7]        [,8]        [,9]       [,10]       [,11]       [,12]\n [1,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [2,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [3,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [4,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n [5,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [6,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n [7,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [8,]  0.16666667  0.16666667 -0.08333333 -0.08333333 -0.08333333 -0.08333333\n [9,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[10,] -0.08333333 -0.08333333  0.16666667  0.16666667 -0.08333333 -0.08333333\n[11,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n[12,] -0.08333333 -0.08333333 -0.08333333 -0.08333333  0.16666667  0.16666667\n\nSQB &lt;- t(y) %*% PB %*% y\nSQB\n\n         [,1]\n[1,] 25.90012\n\nglB &lt;- round(sum(diag(ginv(PB) %*% PB)))\nglB\n\n[1] 2\n\nQMB &lt;- SQB / glB\nQMB\n\n         [,1]\n[1,] 12.95006\n\nFB &lt;- QMB / QMRes\nFB\n\n        [,1]\n[1,] 117.381\n\nftabB &lt;- qf(0.95, glB, gl_res)\nftabB\n\n[1] 5.143253\n\np_valorB &lt;- 1 - pf(FB, glB, gl_res)\np_valorB\n\n              [,1]\n[1,] 0.00001547711\n\n\nA seguir, os resultado estão descritos no quadro de ANOVA:\n\n\n\nANOVA: Fatorial 2x3 (caso balanceado) - Dados sobre queijos\n\nFV\ngl\nSQ\nQM\nFcal\nFtab\np-valor (5%)\n\n\n\nMétodo\n1\n0.114\n0.114\n1.034\n5.987\n0.3485\n\n\nTipo\n2\n25.900\n12.950\n117.381\n5.143\n0.0000\n\n\nInteração\n2\n0.303\n0.151\n1.371\n5.143\n0.3233\n\n\nResíduo\n6\n0.662\n0.110\n\n\n\n\n\nTotal\n11\n26.979\n\n\n\n\n\n\n\n\n\nPode-se notar que a interação Método x Tipo de queijo não foi significativa ao nível de 5% de significãncia. Apenas o Tipo de queijo apresentou significância estatística, com p-valor menor que 5%.\n\n\n\n* Cálculo da soma de quadrados total - SQTotal;\nP = I(abn) - J(abn,abn,1)/abn;\nSQTotal = t(y)*(P)*y;\nglTotal = round(trace(ginv(P)*P));\n\n* Cálculo da soma de quadrados de resíduos - SQRes;\nPR = I(abn) - X*ginv(t(X)*X)*t(X);\nSQRes = t(y)*(PR)*y;\nglRes = round(trace(ginv(PR)*PR));\nQMRes = SQRes/glRes;\n\n* Cálculo SQAxB - forma quadrática;\nX1 = X0||XA||XB;\nPAB = X*ginv(t(X)*X)*t(X) - X1*ginv(t(X1)*X1)*t(X1);\nSQAB = t(y)*PAB*y;                  * Calcula SQ(AB);\nglAB = round(trace(ginv(PAB)*PAB)); * Calcula gl da interação AxB;\nQMAB = SQAB/glAB;\nFAB = QMAB/QMRes;\np_valorAB = 1-cdf('F',FAB,glAB,glRes);\n\n* Cálculo da soma de quadrados do fator A - SQ(A);\nPA = XA*ginv(t(XA)*XA)*t(XA) - J(abn,abn,1)/abn;\nSQA = t(y)*PA*y;\nglA = round(trace(ginv(PA)*PA));\nQMA = SQA/glA;\nFA = QMA/QMRes;\np_valorA = 1-cdf('F',FA,glA,glRes);\n\n* Cálculo da soma de quadrados do fator B - SQ(B);\nPB = XB*ginv(t(XB)*XB)*t(XB) - J(abn,abn,1)/abn;\nSQB = t(y)*PB*y;\nglB = round(trace(ginv(PB)*PB));\nQMB = SQB/glB;\nFB = QMB/QMRes;\np_valorB = 1-cdf('F',FB,glB,glRes);\n\n* Imprime o quadro de análise de variância – ANOVA (pág. 252);\nprint 'QUADRO DE ANOVA: Exemplo 14.4.2: Fatorial 2x3 (caso balanceado)';\nprint 'Método    ' glA[format=8.0]     SQA[format=12.4]   QMA[format=12.4]  FA[format=12.4]  p_valorA[format=12.4];\nprint 'Tipo      ' glB[format=8.0]     SQB[format=12.4]   QMB[format=12.4]  FB[format=12.4]  p_valorB[format=12.4];\nprint 'Interação ' glAB[format=8.0]    SQAB[format=12.4]  QMAB[format=12.4] FAB[format=12.4] p_valorAB[format=12.4];\nprint 'Resíduo   ' glRes[format=8.0]   SQRes[format=12.4] QMRes[format=12.4];\nprint 'Total     ' glTotal[format=8.0] SQTotal[format=12.4] ;"
  },
  {
    "objectID": "exemplo5.3.html#somas-de-quadrados-usando-hipótese-linear-geral",
    "href": "exemplo5.3.html#somas-de-quadrados-usando-hipótese-linear-geral",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.4 Somas de quadrados usando Hipótese Linear Geral",
    "text": "13.4 Somas de quadrados usando Hipótese Linear Geral\nTambém podemos calcular as somas de quadrados utilizando a abordagem da hipótese linear geral com as matrizes CA, CB e CAxB.\n\\[\n\\begin{align}\nCA &= (\\alpha_1 - \\alpha_2) + \\frac{1}3 (\\gamma_{11} + \\gamma_{12} + \\gamma_{13}) - \\frac{1}3 (\\gamma_{21} + \\gamma_{22} +\\gamma_{23}) \\\\\nCB &= \\\\\nCA \\times B &=\n\\end{align}\n\\]\n\n\nR\nSAS\n\n\n\n\nCA &lt;- \n  (1/3) * matrix(\n    c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1, byrow = TRUE\n  )\nCA |&gt; fractions()\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    0    1   -1    0    0    0  1/3  1/3  1/3 -1/3  -1/3  -1/3 \n\nCB &lt;- \n  (1/2) * matrix(\n    c(0, 0, 0, 2, -2, 0, 1, -1, 0, 1, -1, 0,\n      0, 0, 0, 2, 0, -2, 1, 0, -1, 1, 0, -1), nrow = 2, byrow = TRUE\n  )\nCB |&gt; fractions()\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    0    0    0    1   -1    0  1/2 -1/2    0  1/2  -1/2     0 \n[2,]    0    0    0    1    0   -1  1/2    0 -1/2  1/2     0  -1/2 \n\nCAxB &lt;- matrix(\n  c(0, 0, 0, 0, 0, 0, 1, -1, 0, -1, 1, 0,\n    0, 0, 0, 0, 0, 0, 1, 0, -1, -1, 0, 1), nrow = 2, byrow = TRUE\n)\nCAxB\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    0    0    0    0    0    0    1   -1    0    -1     1     0\n[2,]    0    0    0    0    0    0    1    0   -1    -1     0     1\n\nSQ_CA &lt;- t(CA %*% Beta) %*% solve(CA %*% ginv(t(X) %*% X) %*% t(CA)) %*% (CA %*% Beta)\nSQ_CA\n\n         [,1]\n[1,] 0.114075\n\nSQ_CB &lt;- t(CB %*% Beta) %*% solve(CB %*% ginv(t(X) %*% X) %*% t(CB)) %*% (CB %*% Beta)\nSQ_CB\n\n         [,1]\n[1,] 25.90012\n\nSQ_CAxB &lt;- t(CAxB %*% Beta) %*% solve(CAxB %*% ginv(t(X) %*% X) %*% t(CAxB)) %*% (CAxB %*% Beta)\nSQ_CAxB\n\n        [,1]\n[1,] 0.30255\n\n\n\n\n\n* ----------------------------------------------;\n* Cálculo das SQ usando Hipótese Linear Geral;\n* ----------------------------------------------;\n\n*            mi a1 a2 b1 b2 b3 g11 g12 g13 g21 g22 g23;      \nCA   = (1/3)*{0  3 -3  0  0  0   1   1   1  -1  -1  -1};\n\nCB   = (1/2)*{0  0  0  2 -2  0   1  -1   0   1  -1   0,\n              0  0  0  2  0 -2   1   0  -1   1   0  -1};\n\nCAxB =       {0  0  0  0  0  0   1  -1   0  -1   1   0,\n              0  0  0  0  0  0   1   0  -1  -1   0   1}; \n\nSQ_CA = t(CA*Beta)*inv(CA*ginv(t(X)*X)*t(CA))*CA*Beta;\nSQ_CB = t(CB*Beta)*inv(CB*ginv(t(X)*X)*t(CB))*CB*Beta;\nSQ_CAxB = t(CAxB*Beta)*inv(CAxB*ginv(t(X)*X)*t(CAxB))*CAxB*Beta;\n\nprint 'Somas de quadrados usando Hipótese Linear Geral:',,\n    SQ_CA[format=12.4] SQ_CB[format=12.4] SQ_CAxB[format=12.4];"
  },
  {
    "objectID": "exemplo5.3.html#estimabilidade-no-modelo-superparametrizado-sem-restrições",
    "href": "exemplo5.3.html#estimabilidade-no-modelo-superparametrizado-sem-restrições",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.5 Estimabilidade no modelo superparametrizado SEM restrições",
    "text": "13.5 Estimabilidade no modelo superparametrizado SEM restrições\nModelo SEM restrições nos parâmetros.\nVerificaremos se \\(\\beta = \\alpha_1 - \\alpha_2\\) e \\(\\beta = \\alpha_1 - \\alpha_2 + (\\frac{1}3(\\gamma_{11} + \\gamma_{12} + \\gamma_{13}) - (\\frac{1}3)(\\gamma_{21} = \\gamma_{22} + \\gamma_{23})\\) são estimáveis.\n\n\nR\nSAS\n\n\n\n\n\n\\(\\beta = \\alpha_1 - \\alpha_2\\):\n\n\nL1 &lt;- t(matrix(c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 1))\nL1\n\n      [,1]\n [1,]    0\n [2,]    1\n [3,]   -1\n [4,]    0\n [5,]    0\n [6,]    0\n [7,]    0\n [8,]    0\n [9,]    0\n[10,]    0\n[11,]    0\n[12,]    0\n\nver &lt;- t(X) %*% X %*% ginv(t(X) %*% X)\n\nverL1 &lt;- ver %*% L1 |&gt; round(2)\nverL1\n\n       [,1]\n [1,]  0.00\n [2,]  0.75\n [3,] -0.75\n [4,]  0.00\n [5,]  0.00\n [6,]  0.00\n [7,]  0.25\n [8,]  0.25\n [9,]  0.25\n[10,] -0.25\n[11,] -0.25\n[12,] -0.25\n\nL1Beta &lt;- t(L1) %*% Beta\nL1Beta\n\n        [,1]\n[1,] 0.14625\n\n\nVerifica-se que \\(\\beta = \\alpha_1 - \\alpha_2\\) não é estimável no modelo sem restrição nos parâmetros.\n\n\n\\(\\beta = \\alpha_1 - \\alpha_2 + (\\frac{1}3(\\gamma_{11} + \\gamma_{12} + \\gamma_{13}) - (\\frac{1}3)(\\gamma_{21} = \\gamma_{22} + \\gamma_{23})\\):\n\n\nL2 &lt;- (1/3) * t(matrix(c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1))\nL2\n\n            [,1]\n [1,]  0.0000000\n [2,]  1.0000000\n [3,] -1.0000000\n [4,]  0.0000000\n [5,]  0.0000000\n [6,]  0.0000000\n [7,]  0.3333333\n [8,]  0.3333333\n [9,]  0.3333333\n[10,] -0.3333333\n[11,] -0.3333333\n[12,] -0.3333333\n\nver &lt;- t(X) %*% X %*% ginv(t(X) %*% X)\n\nverL2 &lt;- ver %*% L2 |&gt; round(2)\nverL2\n\n       [,1]\n [1,]  0.00\n [2,]  1.00\n [3,] -1.00\n [4,]  0.00\n [5,]  0.00\n [6,]  0.00\n [7,]  0.33\n [8,]  0.33\n [9,]  0.33\n[10,] -0.33\n[11,] -0.33\n[12,] -0.33\n\nL2Beta &lt;- t(L2) %*% Beta\nL2Beta\n\n      [,1]\n[1,] 0.195\n\n\nVerifica-se que \\(\\beta = \\alpha_1 - \\alpha_2 + (\\frac{1}3(\\gamma_{11} + \\gamma_{12} + \\gamma_{13}) - (\\frac{1}3)(\\gamma_{21} = \\gamma_{22} + \\gamma_{23})\\) é estimavel no modelo sem restrição nos parametros.\nMatriz T de condições marginais: T*Beta = 0\n\nt &lt;- matrix(\n  c(0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n    0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1), \n  nrow = 7, ncol = 12, byrow = TRUE)\n\nrank_T &lt;- sum(diag(ginv(t) %*% t))\nrank_T\n\n[1] 6\n\nW &lt;- rbind(X, t)\n\nrank_W &lt;- sum(diag(ginv(W) %*% W))\nrank_W\n\n[1] 12\n\nyr &lt;- c(y, rep(0, 7))\nyr\n\n [1] 39.02 38.79 35.74 35.41 37.02 36.00 38.96 39.01 35.58 35.52 35.70 36.04\n[13]  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n\nBeta_R &lt;- solve(t(W) %*% W) %*% t(W) %*% yr   # Beta sujeito às condições marginais\nBeta_R\n\n            [,1]\n [1,] 36.8991667\n [2,]  0.0975000\n [3,] -0.0975000\n [4,]  2.0458333\n [5,] -1.3366667\n [6,] -0.7091667\n [7,] -0.1375000\n [8,] -0.0850000\n [9,]  0.2225000\n[10,]  0.1375000\n[11,]  0.0850000\n[12,] -0.2225000\n\n### Beta do modelo COM restrições nos parâmetros\n\n\n\n\nL1 = t({0 1 -1 0 0 0 0 0 0 0 0 0});\nver = t(X)*X*ginv(t(X)*X);\nverL1 = ver*L1;\nL1BetaMP = t(L1)*BetaMP;\nprint 'Mostra que L1Beta = a1-a2 NÃO É estimável no modelo', 'SEM restrição nos parâmetros';\nprint L1 verL1 L1BetaMP[format=12.4];\n\nprint 'Mostra que L2Beta = a1-a2 + (1/3(g11+g12+g13)-(1/3)(g21+g22+g23) É estimável no modelo',\n      'SEM restrição nos parâmetros';\nL2 = (1/3)*t({0 3 -3 0 0 0 1 1 1 -1 -1 -1});\nverL2 = ver*L2;\nL2BetaMP = t(L2)*BetaMP;\nprint L2 verL2 L2BetaMP[format=12.4];   \n\n* Matriz T de condições marginais: T*Beta = 0;\nT = {0 1 1 0 0 0 0 0 0 0 0 0,\n     0 0 0 1 1 1 0 0 0 0 0 0,\n     0 0 0 0 0 0 1 1 1 0 0 0,\n     0 0 0 0 0 0 0 0 0 1 1 1,\n     0 0 0 0 0 0 1 0 0 1 0 0,\n     0 0 0 0 0 0 0 1 0 0 1 0,\n     0 0 0 0 0 0 0 0 1 0 0 1};\nrank_T = round(trace(ginv(T)*T));   * Determina o posto da matriz T;\nW = X//T;                           * Junta as matrizes X e T;\nrank_W = round(trace(ginv(W)*W));   * Calcula o posto da matriz W = X//T;\nprint npar rank_X rank_T rank_W;\nyr = y//j(7,1,0);                   * Completa o vetor y com 7 zeros;\nBeta_R = inv(t(W)*W)*t(W)*yr;       * Beta sujeito às condições marginais;\nprint BetaMP[format=12.4] Beta_R[format=12.4];\n\n\n\n\n\n13.5.1 Estimabilidade no modelo superparametrizado COM restrições\nModelo COM restricoes nos parametros\n\n\nR\nSAS\n\n\n\nLiBeta é estimável se Li = verLi\nVerifica que L1Beta = a1-a2 é ESTIMÁVEL no modelo COM RESTRIÇÃO nos parâmetros\n\nL1 &lt;- t(matrix(c(0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 1))\nL1\n\n      [,1]\n [1,]    0\n [2,]    1\n [3,]   -1\n [4,]    0\n [5,]    0\n [6,]    0\n [7,]    0\n [8,]    0\n [9,]    0\n[10,]    0\n[11,]    0\n[12,]    0\n\nver &lt;- t(W) %*% W %*% solve(t(W) %*% W)\n\nverL1 &lt;- ver %*% L1 |&gt; round()\nverL1\n\n      [,1]\n [1,]    0\n [2,]    1\n [3,]   -1\n [4,]    0\n [5,]    0\n [6,]    0\n [7,]    0\n [8,]    0\n [9,]    0\n[10,]    0\n[11,]    0\n[12,]    0\n\nL1Beta_r &lt;- t(L1) %*% Beta_R\nL1Beta_r\n\n      [,1]\n[1,] 0.195\n\n### Mostra que L1Beta = a1-a2 É estimável no modelo', 'COM restrição nos parâmetros\n\nVerifica que L2Beta = a1-a2 a1-a2 + (1/3(g11+g12+g13) - (1/3)(g21+g22+g23) é ESTIMÁVEL no modelo’, ’COM RESTRIÇÃO nos parâmetros\n\nL22 &lt;- (1/3) * t(matrix(c(0, 3, -3, 0, 0, 0, 1, 1, 1, -1, -1, -1), nrow = 1))\nL22 |&gt; round(3)\n\n        [,1]\n [1,]  0.000\n [2,]  1.000\n [3,] -1.000\n [4,]  0.000\n [5,]  0.000\n [6,]  0.000\n [7,]  0.333\n [8,]  0.333\n [9,]  0.333\n[10,] -0.333\n[11,] -0.333\n[12,] -0.333\n\nverL22 &lt;- ver %*% L22 |&gt; round(3)\nverL22\n\n        [,1]\n [1,]  0.000\n [2,]  1.000\n [3,] -1.000\n [4,]  0.000\n [5,]  0.000\n [6,]  0.000\n [7,]  0.333\n [8,]  0.333\n [9,]  0.333\n[10,] -0.333\n[11,] -0.333\n[12,] -0.333\n\nL2Beta_R &lt;- t(L22) %*% Beta_R\nL2Beta_R\n\n      [,1]\n[1,] 0.195\n\n### Mostra que L2Beta = a1-a2 +(1/3(g11+g12+g13)-(1/3)(g21=g22+g23) É estimável no modelo', 'COM restrição nos parâmetros\n\n\n\n\nprint '------------------------------------------------------------',\n      ' ESTIMABILIDADE NO MODELO SUPERPARAMETRIZADO COM RESTRIÇÕES ',\n      '------------------------------------------------------------',;\nL1 = t({0 1 -1 0 0 0 0 0 0 0 0 0});\nver = t(W)*W*inv(t(W)*W);\nverL1 = ver*L1;\nL1Beta_r = t(L1)*Beta_R;\nprint 'Mostra que L1Beta = a1-a2 É estimável no modelo', 'COM restrição nos parâmetros';\nprint L1 verL1 L1Beta_R[format=12.4];\n\nprint 'Mostra que L2Beta = a1-a2 +(1/3(g11+g12+g13)-(1/3)(g21=g22+g23) É estimável no modelo',\n      'COM restrição nos parâmetros';\nL2 = (1/3)*t({0 3 -3 0  0  0  1  1  1 -1 -1 -1});\nverL2 = ver*L2;\nL2Beta_R = t(L2)*Beta_R;\nprint L2 verL2 L2Beta_R[format=12.4];"
  },
  {
    "objectID": "exemplo5.3.html#hipótese-linear-geral",
    "href": "exemplo5.3.html#hipótese-linear-geral",
    "title": "\n13  ANOVA balanceada com dois fatores\n",
    "section": "\n13.6 6. Hipótese linear geral",
    "text": "13.6 6. Hipótese linear geral\n\n\nR\nSAS\n\n\n\n\n### ESTIMABILIDADE NO MODELO SUPERPARAMETRIZADO SEM RESTRIÇÃO\n\nCA &lt;- (1 / 3) * c(0,  3, -3,  0,  0,  0,  1,  1,  1, -1, -1, -1)\nCA |&gt; fractions()\n\n [1]    0    1   -1    0    0    0  1/3  1/3  1/3 -1/3 -1/3 -1/3\n\nCABeta &lt;- CA %*% Beta_R\nCABeta\n\n      [,1]\n[1,] 0.195\n\n\n\n# No modelo COM restrição nos parâmetros:\n\nCA\n\n [1]  0.0000000  1.0000000 -1.0000000  0.0000000  0.0000000  0.0000000\n [7]  0.3333333  0.3333333  0.3333333 -0.3333333 -0.3333333 -0.3333333\n\nCABeta_R &lt;- CA %*% Beta_R\nCABeta_R\n\n      [,1]\n[1,] 0.195\n\n\n\nSQ_A &lt;- t(CABeta) %*% solve(t(CA) %*% ginv(t(X) %*% X) %*% CA) %*% CABeta\nSQ_A\n\n         [,1]\n[1,] 0.114075\n\nSQ_A_R = t(CABeta_R) %*% solve(t(CA) %*% solve(t(W) %*% W) %*% CA) %*% CABeta_R\nSQ_A_R\n\n         [,1]\n[1,] 0.114075\n\n\n\n\n\n* Cálculo de SQA usando hipótese linear geral;\n\n* (1) No modelo SEM restrição;\n* CA = {0  1 -1  0  0  0  0  0  0  0  0  0};\nCA = (1/3)*{0  3 -3  0  0  0  1  1  1 -1 -1 -1};\nCABeta = CA*Beta_R;\nprint '-----------------------------------------------------------',\n      ' ESTIMABILIDADE NO MODELO SUPERPARAMETRIZADO SEM RESTRIÇÃO ',\n      '-----------------------------------------------------------',\n      CA[format=6.2],,CABeta[format=12.4],,;\n\nSQ_A = t(CABeta)*inv(CA*ginv(t(X)*X)*t(CA))*CABeta;\n\n* (1) No modelo COM restrição;\nCABeta_R = CA*Beta_R;\nprint 'No modelo COM restrição nos parâmetros:',,CA[format=6.2],,CABeta_R[format=12.4],,;\n\nSQ_A_R = t(CABeta_r)*inv(CA*inv(t(W)*W)*t(CA))*CABeta_r;\nprint SQ_A[format=12.4] SQ_A_R[format=12.4];\n\nquit;"
  }
]